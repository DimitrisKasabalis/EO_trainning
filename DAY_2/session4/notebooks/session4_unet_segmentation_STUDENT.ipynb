{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa4ef7d0",
   "metadata": {},
   "source": [
    "# Session 4 Part C: U-Net for Semantic Segmentation\n",
    "## Pixel-Level Land Cover Classification\n",
    "\n",
    "**Duration:** 60 minutes | **Difficulty:** Advanced  \n",
    "**Task:** Forest boundary delineation in Palawan\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Objectives\n",
    "\n",
    "1. âœ… Understand semantic segmentation vs classification\n",
    "2. âœ… Implement U-Net encoder-decoder architecture\n",
    "3. âœ… Train on pixel-level masks\n",
    "4. âœ… Evaluate with IoU metric\n",
    "5. âœ… Visualize segmentation results\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ³ What is Semantic Segmentation?\n",
    "\n",
    "**Classification:** One label per image  \n",
    "**Segmentation:** One label per pixel\n",
    "\n",
    "**Applications:** Forest boundaries, field delineation, building footprints\n",
    "\n",
    "---\n",
    "\n",
    "Let's build a segmentation model! ðŸš€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0702d40a",
   "metadata": {},
   "source": [
    "# Step 1: Setup & Dataset (10 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2078bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cb9fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_forest_data(n=500, size=256):\n",
    "    images, masks = [], []\n",
    "    for _ in range(n):\n",
    "        img = np.random.rand(size, size, 3) * 0.3\n",
    "        mask = np.zeros((size, size), dtype=np.uint8)\n",
    "        for _ in range(np.random.randint(2, 6)):\n",
    "            cx, cy = np.random.randint(50, size-50, 2)\n",
    "            r = np.random.randint(30, 80)\n",
    "            Y, X = np.ogrid[:size, :size]\n",
    "            forest = (np.sqrt((X-cx)**2 + (Y-cy)**2) + np.random.randn(size,size)*10) < r\n",
    "            mask[forest] = 1\n",
    "            img[forest, 1] += 0.4\n",
    "        images.append(np.clip(img, 0, 1))\n",
    "        masks.append(mask)\n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "images, masks = generate_forest_data(500, 256)\n",
    "print(f\"Dataset: {images.shape}, {masks.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e320e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(images, masks, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42)\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train, 2)\n",
    "y_val_cat = tf.keras.utils.to_categorical(y_val, 2)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test, 2)\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ea75d7",
   "metadata": {},
   "source": [
    "# Step 2: Build U-Net (15 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filters):\n",
    "    x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)\n",
    "    return x\n",
    "\n",
    "def encoder_block(x, filters):\n",
    "    conv = conv_block(x, filters)\n",
    "    pool = layers.MaxPooling2D(2)(conv)\n",
    "    return conv, pool\n",
    "\n",
    "def decoder_block(x, skip, filters):\n",
    "    up = layers.Conv2DTranspose(filters, 2, strides=2, padding='same')(x)\n",
    "    concat = layers.Concatenate()([up, skip])\n",
    "    return conv_block(concat, filters)\n",
    "\n",
    "def build_unet(input_shape=(256,256,3), num_classes=2):\n",
    "    inputs = layers.Input(input_shape)\n",
    "    # Encoder\n",
    "    c1, p1 = encoder_block(inputs, 64)\n",
    "    c2, p2 = encoder_block(p1, 128)\n",
    "    c3, p3 = encoder_block(p2, 256)\n",
    "    c4, p4 = encoder_block(p3, 512)\n",
    "    # Bottleneck\n",
    "    b = conv_block(p4, 1024)\n",
    "    # Decoder\n",
    "    d4 = decoder_block(b, c4, 512)\n",
    "    d3 = decoder_block(d4, c3, 256)\n",
    "    d2 = decoder_block(d3, c2, 128)\n",
    "    d1 = decoder_block(d2, c1, 64)\n",
    "    outputs = layers.Conv2D(num_classes, 1, activation='softmax')(d1)\n",
    "    return models.Model(inputs, outputs, name='U-Net')\n",
    "\n",
    "model = build_unet()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "              metrics=['accuracy', keras.metrics.MeanIoU(num_classes=2)])\n",
    "print(\"U-Net ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27fd04e",
   "metadata": {},
   "source": [
    "# Step 3: Training (20 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fc8107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "    ModelCheckpoint('unet_best.h5', monitor='val_mean_io_u', save_best_only=True)\n",
    "]\n",
    "history = model.fit(X_train, y_train_cat, validation_data=(X_val, y_val_cat),\n",
    "                    epochs=50, batch_size=8, callbacks=callbacks)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27fdea5",
   "metadata": {},
   "source": [
    "# Step 4: Evaluation (10 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b82c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_iou = model.evaluate(X_test, y_test_cat)\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Test IoU: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac532c36",
   "metadata": {},
   "source": [
    "# Step 5: Visualization (5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b3fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "pred_masks = np.argmax(predictions, axis=-1)\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "for i in range(3):\n",
    "    idx = i\n",
    "    axes[i, 0].imshow(X_test[idx])\n",
    "    axes[i, 0].set_title('Input')\n",
    "    axes[i, 1].imshow(y_test[idx], cmap='Greens')\n",
    "    axes[i, 1].set_title('Ground Truth')\n",
    "    axes[i, 2].imshow(pred_masks[idx], cmap='Greens')\n",
    "    axes[i, 2].set_title('Prediction')\n",
    "    overlay = X_test[idx].copy()\n",
    "    overlay[pred_masks[idx]==1] = overlay[pred_masks[idx]==1]*0.5 + [0,0.5,0]*0.5\n",
    "    axes[i, 3].imshow(overlay)\n",
    "    axes[i, 3].set_title('Overlay')\n",
    "    for ax in axes[i]: ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb88edf9",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸŽ‰ U-Net Segmentation Complete!\n",
    "\n",
    "## Summary\n",
    "âœ… Built U-Net encoder-decoder architecture  \n",
    "âœ… Trained on forest segmentation  \n",
    "âœ… Achieved {test_iou:.2f} IoU  \n",
    "âœ… Pixel-level classification successful  \n",
    "\n",
    "## Philippine Applications\n",
    "- **Mangrove mapping** - Palawan coastlines\n",
    "- **Rice field delineation** - Central Luzon\n",
    "- **Building footprints** - Metro Manila\n",
    "- **Flood extent** - Pampanga Basin\n",
    "\n",
    "## Key Insights\n",
    "U-Net excels at:\n",
    "- Precise boundary delineation\n",
    "- Spatial detail preservation\n",
    "- Works with limited data (skip connections help)\n",
    "\n",
    "**Next:** Apply to real Palawan Sentinel-2 imagery!\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
