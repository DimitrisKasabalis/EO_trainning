<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-10-09">

<title>Session 2: Core Concepts of AI/ML for Earth Observation – CopPhil EO AI/ML Training - Day 1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../images/favicon.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-c9822816d3895e59fda95a6fa7545fef.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-4a074efccdeff27617fbc72d37c1244e.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-c9822816d3895e59fda95a6fa7545fef.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-863aa4d8cd7d47b2ea71a40f543e327a.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-6c7212b2519dd7bebfa1236c409c2f09.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../site_libs/bootstrap/bootstrap-863aa4d8cd7d47b2ea71a40f543e327a.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


<link rel="stylesheet" href="../styles/custom.css">
</head>

<body class="nav-sidebar floating nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">CopPhil EO AI/ML Training - Day 1</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-sessions" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Sessions</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-sessions">    
        <li>
    <a class="dropdown-item" href="../sessions/session1.html">
 <span class="dropdown-text">Session 1: Copernicus &amp; Philippine EO</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../sessions/session2.html">
 <span class="dropdown-text">Session 2: AI/ML Fundamentals</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../sessions/session3.html">
 <span class="dropdown-text">Session 3: Python for Geospatial</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../sessions/session4.html">
 <span class="dropdown-text">Session 4: Google Earth Engine</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Resources</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="../resources/setup.html">
 <span class="dropdown-text">Setup Guide</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../resources/philippine-eo.html">
 <span class="dropdown-text">Philippine EO Links</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../resources/cheatsheets.html">
 <span class="dropdown-text">Cheat Sheets</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../resources/faq.html">
 <span class="dropdown-text">FAQ</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../resources/glossary.html">
 <span class="dropdown-text">Glossary</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/copphil-training"> <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../resources/downloads.html"> <i class="bi bi-download" role="img">
</i> 
<span class="menu-text">Materials</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../sessions/session1.html">Training Sessions</a></li><li class="breadcrumb-item"><a href="../sessions/session2.html">Session 2: Core Concepts of AI/ML for Earth Observation</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="../index.html" class="sidebar-logo-link">
      </a>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Getting Started</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome to Day 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../resources/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup Guide</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Training Sessions</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sessions/session1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 1: Copernicus Sentinel Data Deep Dive &amp; Philippine EO Ecosystem</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sessions/session2.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Session 2: Core Concepts of AI/ML for Earth Observation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sessions/session3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 3: Hands-on Python for Geospatial Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sessions/session4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 4: Introduction to Google Earth Engine</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../resources/philippine-eo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Philippine EO Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../resources/cheatsheets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Cheat Sheets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../resources/downloads.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Downloads</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../resources/faq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Frequently Asked Questions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../resources/glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Glossary</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On This Page</h2>
   
  <ul>
  <li><a href="#session-overview" id="toc-session-overview" class="nav-link active" data-scroll-target="#session-overview">Session Overview</a>
  <ul class="collapse">
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link" data-scroll-target="#learning-objectives">Learning Objectives</a></li>
  </ul></li>
  <li><a href="#part-1-what-is-aiml" id="toc-part-1-what-is-aiml" class="nav-link" data-scroll-target="#part-1-what-is-aiml">Part 1: What is AI/ML?</a>
  <ul class="collapse">
  <li><a href="#defining-the-terms" id="toc-defining-the-terms" class="nav-link" data-scroll-target="#defining-the-terms">Defining the Terms</a></li>
  <li><a href="#why-ml-for-earth-observation" id="toc-why-ml-for-earth-observation" class="nav-link" data-scroll-target="#why-ml-for-earth-observation">Why ML for Earth Observation?</a></li>
  </ul></li>
  <li><a href="#part-2-the-aiml-workflow-for-earth-observation" id="toc-part-2-the-aiml-workflow-for-earth-observation" class="nav-link" data-scroll-target="#part-2-the-aiml-workflow-for-earth-observation">Part 2: The AI/ML Workflow for Earth Observation</a>
  <ul class="collapse">
  <li><a href="#step-1-problem-definition" id="toc-step-1-problem-definition" class="nav-link" data-scroll-target="#step-1-problem-definition">Step 1: Problem Definition</a></li>
  <li><a href="#step-2-data-acquisition" id="toc-step-2-data-acquisition" class="nav-link" data-scroll-target="#step-2-data-acquisition">Step 2: Data Acquisition</a></li>
  <li><a href="#step-3-data-pre-processing" id="toc-step-3-data-pre-processing" class="nav-link" data-scroll-target="#step-3-data-pre-processing">Step 3: Data Pre-processing</a></li>
  <li><a href="#step-4-feature-engineering" id="toc-step-4-feature-engineering" class="nav-link" data-scroll-target="#step-4-feature-engineering">Step 4: Feature Engineering</a></li>
  <li><a href="#step-5-model-selection-and-training" id="toc-step-5-model-selection-and-training" class="nav-link" data-scroll-target="#step-5-model-selection-and-training">Step 5: Model Selection and Training</a></li>
  <li><a href="#step-6-validation-and-evaluation" id="toc-step-6-validation-and-evaluation" class="nav-link" data-scroll-target="#step-6-validation-and-evaluation">Step 6: Validation and Evaluation</a></li>
  <li><a href="#step-7-deployment-and-operationalization" id="toc-step-7-deployment-and-operationalization" class="nav-link" data-scroll-target="#step-7-deployment-and-operationalization">Step 7: Deployment and Operationalization</a></li>
  </ul></li>
  <li><a href="#part-3-types-of-machine-learning" id="toc-part-3-types-of-machine-learning" class="nav-link" data-scroll-target="#part-3-types-of-machine-learning">Part 3: Types of Machine Learning</a>
  <ul class="collapse">
  <li><a href="#supervised-learning" id="toc-supervised-learning" class="nav-link" data-scroll-target="#supervised-learning">Supervised Learning</a></li>
  <li><a href="#unsupervised-learning" id="toc-unsupervised-learning" class="nav-link" data-scroll-target="#unsupervised-learning">Unsupervised Learning</a></li>
  </ul></li>
  <li><a href="#part-4-introduction-to-deep-learning" id="toc-part-4-introduction-to-deep-learning" class="nav-link" data-scroll-target="#part-4-introduction-to-deep-learning">Part 4: Introduction to Deep Learning</a>
  <ul class="collapse">
  <li><a href="#what-is-deep-learning" id="toc-what-is-deep-learning" class="nav-link" data-scroll-target="#what-is-deep-learning">What is Deep Learning?</a></li>
  <li><a href="#neural-network-fundamentals" id="toc-neural-network-fundamentals" class="nav-link" data-scroll-target="#neural-network-fundamentals">Neural Network Fundamentals</a></li>
  <li><a href="#deep-learning-for-earth-observation" id="toc-deep-learning-for-earth-observation" class="nav-link" data-scroll-target="#deep-learning-for-earth-observation">Deep Learning for Earth Observation</a></li>
  </ul></li>
  <li><a href="#part-5-data-centric-ai-in-earth-observation" id="toc-part-5-data-centric-ai-in-earth-observation" class="nav-link" data-scroll-target="#part-5-data-centric-ai-in-earth-observation">Part 5: Data-Centric AI in Earth Observation</a>
  <ul class="collapse">
  <li><a href="#the-paradigm-shift-2025" id="toc-the-paradigm-shift-2025" class="nav-link" data-scroll-target="#the-paradigm-shift-2025">The Paradigm Shift (2025)</a></li>
  <li><a href="#pillar-1-data-quality" id="toc-pillar-1-data-quality" class="nav-link" data-scroll-target="#pillar-1-data-quality">Pillar 1: Data Quality</a></li>
  <li><a href="#pillar-2-data-quantity" id="toc-pillar-2-data-quantity" class="nav-link" data-scroll-target="#pillar-2-data-quantity">Pillar 2: Data Quantity</a></li>
  <li><a href="#pillar-3-data-diversity" id="toc-pillar-3-data-diversity" class="nav-link" data-scroll-target="#pillar-3-data-diversity">Pillar 3: Data Diversity</a></li>
  <li><a href="#pillar-4-annotation-strategy" id="toc-pillar-4-annotation-strategy" class="nav-link" data-scroll-target="#pillar-4-annotation-strategy">Pillar 4: Annotation Strategy</a></li>
  <li><a href="#examples-data-centric-success-stories" id="toc-examples-data-centric-success-stories" class="nav-link" data-scroll-target="#examples-data-centric-success-stories">2025 Examples: Data-Centric Success Stories</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  <li><a href="#discussion-questions" id="toc-discussion-questions" class="nav-link" data-scroll-target="#discussion-questions">Discussion Questions</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading">Further Reading</a>
  <ul class="collapse">
  <li><a href="#foundational-concepts" id="toc-foundational-concepts" class="nav-link" data-scroll-target="#foundational-concepts">Foundational Concepts</a></li>
  <li><a href="#neural-networks" id="toc-neural-networks" class="nav-link" data-scroll-target="#neural-networks">Neural Networks</a></li>
  <li><a href="#eo-specific-ml" id="toc-eo-specific-ml" class="nav-link" data-scroll-target="#eo-specific-ml">EO-Specific ML</a></li>
  <li><a href="#philippine-ai-initiatives" id="toc-philippine-ai-initiatives" class="nav-link" data-scroll-target="#philippine-ai-initiatives">Philippine AI Initiatives</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../sessions/session1.html">Training Sessions</a></li><li class="breadcrumb-item"><a href="../sessions/session2.html">Session 2: Core Concepts of AI/ML for Earth Observation</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Session 2: Core Concepts of AI/ML for Earth Observation</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">Understanding the fundamentals of machine learning for satellite data analysis</p>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Date</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 9, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="session-info">
<p><strong>Duration:</strong> 2 hours | <strong>Format:</strong> Lecture + Conceptual Exercises | <strong>Platform:</strong> Presentation</p>
</div>
<section id="session-overview" class="level2">
<h2 class="anchored" data-anchor-id="session-overview">Session Overview</h2>
<p>This session provides a comprehensive introduction to Artificial Intelligence and Machine Learning concepts specifically tailored for Earth Observation applications. You’ll learn the complete AI/ML workflow, understand different learning paradigms, explore neural network fundamentals, and discover why data quality matters more than model complexity in 2025’s data-centric AI paradigm.</p>
<section id="learning-objectives" class="level3 learning-objectives">
<h3 class="anchored" data-anchor-id="learning-objectives">Learning Objectives</h3>
<p>By the end of this session, you will be able to:</p>
<ul>
<li><strong>Define</strong> AI and ML in the context of Earth Observation</li>
<li><strong>Describe</strong> the complete AI/ML workflow from problem definition to deployment</li>
<li><strong>Distinguish</strong> between supervised and unsupervised learning with EO examples</li>
<li><strong>Explain</strong> classification vs.&nbsp;regression tasks in satellite data analysis</li>
<li><strong>Understand</strong> neural network architecture fundamentals</li>
<li><strong>Identify</strong> key components: neurons, layers, activation functions, loss functions, optimizers</li>
<li><strong>Articulate</strong> the data-centric AI paradigm and its importance for EO</li>
<li><strong>Apply</strong> best practices for data quality, quantity, diversity, and annotation</li>
</ul>
</section>
<hr>
</section>
<section id="part-1-what-is-aiml" class="level2">
<h2 class="anchored" data-anchor-id="part-1-what-is-aiml">Part 1: What is AI/ML?</h2>
<section id="defining-the-terms" class="level3">
<h3 class="anchored" data-anchor-id="defining-the-terms">Defining the Terms</h3>
<p><strong>Artificial Intelligence (AI):</strong></p>
<ul>
<li>Broad field focused on creating intelligent machines</li>
<li>Systems that can perceive, reason, learn, and act</li>
<li>Includes everything from rule-based systems to machine learning</li>
</ul>
<p><strong>Machine Learning (ML):</strong></p>
<ul>
<li>Subset of AI focused on learning from data</li>
<li>Algorithms that improve performance through experience</li>
<li><strong>Key distinction:</strong> No explicit programming of rules</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>The ML Difference
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Traditional Programming:</strong></p>
<pre><code>Rules + Data → Output</code></pre>
<p><strong>Machine Learning:</strong></p>
<pre><code>Data + Desired Output → Rules (Model)</code></pre>
<p>In EO: Instead of coding “if NIR &gt; 0.6 and Red &lt; 0.3, then forest”, ML learns the pattern from labeled examples.</p>
</div>
</div>
</section>
<section id="why-ml-for-earth-observation" class="level3">
<h3 class="anchored" data-anchor-id="why-ml-for-earth-observation">Why ML for Earth Observation?</h3>
<p><strong>Challenges that ML addresses:</strong></p>
<ol type="1">
<li><strong>Scale:</strong> Petabytes of satellite data - impossible to manually analyze</li>
<li><strong>Complexity:</strong> Multispectral, temporal, spatial patterns humans can’t easily detect</li>
<li><strong>Consistency:</strong> Automated processing ensures reproducible results</li>
<li><strong>Speed:</strong> Real-time disaster mapping requires immediate analysis</li>
</ol>
<p><strong>Traditional vs.&nbsp;ML approaches:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 40%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Task</th>
<th>Traditional</th>
<th>ML Approach</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Water detection</strong></td>
<td>Manual NDWI threshold</td>
<td>Learn optimal threshold + texture from examples</td>
</tr>
<tr class="even">
<td><strong>Land cover</strong></td>
<td>Rule-based classification</td>
<td>Random Forest or CNN with training samples</td>
</tr>
<tr class="odd">
<td><strong>Flood mapping</strong></td>
<td>Expert visual interpretation</td>
<td>U-Net segmentation trained on labeled floods</td>
</tr>
<tr class="even">
<td><strong>Crop monitoring</strong></td>
<td>Fixed vegetation index thresholds</td>
<td>LSTM time series model learning phenology</td>
</tr>
</tbody>
</table>
<hr>
</section>
</section>
<section id="part-2-the-aiml-workflow-for-earth-observation" class="level2">
<h2 class="anchored" data-anchor-id="part-2-the-aiml-workflow-for-earth-observation">Part 2: The AI/ML Workflow for Earth Observation</h2>
<p>Understanding the complete workflow is essential for successful EO projects. Each step matters.</p>
<section id="step-1-problem-definition" class="level3">
<h3 class="anchored" data-anchor-id="step-1-problem-definition">Step 1: Problem Definition</h3>
<p><strong>Define clearly what you want to achieve:</strong></p>
<ul>
<li>What question are you answering? (e.g., “Where are mangroves declining?”)</li>
<li>What output do you need? (map, time series, alert system?)</li>
<li>What accuracy is acceptable?</li>
<li>What constraints exist? (time, computational resources, data availability)</li>
</ul>
<div class="philippine-context">
<p><strong>Philippine Example:</strong></p>
<p><strong>Problem:</strong> Map rice paddies in Central Luzon to estimate harvest timing for food security</p>
<p><strong>Clear definition:</strong> - Binary classification: rice vs.&nbsp;non-rice - 20m spatial resolution acceptable (Sentinel-2 bands) - Temporal: wet and dry season separately - Accuracy target: &gt;85% for operational use</p>
</div>
</section>
<section id="step-2-data-acquisition" class="level3">
<h3 class="anchored" data-anchor-id="step-2-data-acquisition">Step 2: Data Acquisition</h3>
<p><strong>Gather all necessary data:</strong></p>
<ul>
<li><strong>Satellite imagery:</strong> Sentinel-1/2, Landsat, commercial VHR</li>
<li><strong>Ground truth:</strong> Field surveys, high-res imagery interpretation, existing maps</li>
<li><strong>Ancillary data:</strong> DEM, climate, administrative boundaries</li>
</ul>
<p><strong>Data sources for Philippines:</strong></p>
<ul>
<li>Copernicus Data Space Ecosystem (Sentinel-1/2)</li>
<li>PhilSA SIYASAT (NovaSAR-1)</li>
<li>NAMRIA Geoportal (land cover basemaps)</li>
<li>PAGASA (climate data)</li>
</ul>
</section>
<section id="step-3-data-pre-processing" class="level3">
<h3 class="anchored" data-anchor-id="step-3-data-pre-processing">Step 3: Data Pre-processing</h3>
<p><strong>Critical step - “Garbage in, garbage out”</strong></p>
<p><strong>For satellite imagery:</strong></p>
<ul>
<li><strong>Atmospheric correction:</strong> Convert to surface reflectance (use Level-2A)</li>
<li><strong>Cloud masking:</strong> Remove or mask cloudy pixels</li>
<li><strong>Geometric correction:</strong> Ensure proper alignment</li>
<li><strong>Radiometric calibration:</strong> Consistent values across scenes</li>
<li><strong>Temporal compositing:</strong> Reduce clouds via median/mean composites</li>
</ul>
<p><strong>For training labels:</strong></p>
<ul>
<li><strong>Quality control:</strong> Verify label accuracy</li>
<li><strong>Coordinate alignment:</strong> Ensure labels match imagery timing and location</li>
<li><strong>Class balancing:</strong> Ensure adequate samples per class</li>
<li><strong>Format standardization:</strong> Convert to ML-ready format</li>
</ul>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Warning</span>Pre-processing Pitfalls
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Common errors that degrade model performance:</strong></p>
<ul>
<li>Using Top-of-Atmosphere instead of surface reflectance</li>
<li>Temporal mismatch: 2020 imagery with 2018 labels</li>
<li>Incomplete cloud masking leaving cloud shadows</li>
<li>Mixed pixels at boundaries (especially for validation)</li>
<li>Inconsistent band ordering across scenes</li>
</ul>
</div>
</div>
</section>
<section id="step-4-feature-engineering" class="level3">
<h3 class="anchored" data-anchor-id="step-4-feature-engineering">Step 4: Feature Engineering</h3>
<p><strong>Deriving informative variables from raw data</strong></p>
<p><strong>For traditional ML (Random Forest, SVM):</strong></p>
<ul>
<li><strong>Spectral indices:</strong> NDVI, NDWI, NDBI, EVI, SAVI</li>
<li><strong>Textural features:</strong> GLCM metrics (contrast, entropy)</li>
<li><strong>Temporal features:</strong> Mean, std dev, phenology metrics</li>
<li><strong>Topographic features:</strong> Elevation, slope, aspect (from DEM)</li>
<li><strong>Contextual features:</strong> Distance to roads, water bodies</li>
</ul>
<p><strong>Example: Forest classification features</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Spectral indices</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>NDVI <span class="op">=</span> (NIR <span class="op">-</span> Red) <span class="op">/</span> (NIR <span class="op">+</span> Red)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>NDWI <span class="op">=</span> (Green <span class="op">-</span> NIR) <span class="op">/</span> (Green <span class="op">+</span> NIR)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Texture (from GLCM)</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>Contrast <span class="op">=</span> ...  <span class="co"># measure of local variation</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>Homogeneity <span class="op">=</span> ...  <span class="co"># measure of uniformity</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Topographic</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>Elevation, Slope</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Result: Input feature vector per pixel</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> [Red, Green, Blue, NIR, SWIR1, SWIR2, NDVI, NDWI, Contrast, Elevation, Slope]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>For deep learning (CNNs):</strong></p>
<ul>
<li>Less manual feature engineering needed</li>
<li>Networks automatically learn features from raw pixels</li>
<li>Still benefit from good input data (cloud-free, calibrated)</li>
</ul>
</section>
<section id="step-5-model-selection-and-training" class="level3">
<h3 class="anchored" data-anchor-id="step-5-model-selection-and-training">Step 5: Model Selection and Training</h3>
<p><strong>Choose appropriate algorithm:</strong></p>
<p><strong>Consider:</strong></p>
<ul>
<li>Task type (classification, regression, segmentation)</li>
<li>Data size (deep learning needs more data)</li>
<li>Interpretability requirements</li>
<li>Computational resources</li>
<li>Deployment constraints</li>
</ul>
<p><strong>Common EO algorithms:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 15%">
<col style="width: 25%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>Algorithm</th>
<th>Type</th>
<th>Best For</th>
<th>Data Needs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Random Forest</strong></td>
<td>Ensemble</td>
<td>Classification, feature importance</td>
<td>Medium</td>
</tr>
<tr class="even">
<td><strong>SVM</strong></td>
<td>Kernel</td>
<td>Binary classification, small data</td>
<td>Small-Medium</td>
</tr>
<tr class="odd">
<td><strong>CNN</strong></td>
<td>Deep Learning</td>
<td>Image classification, automatic features</td>
<td>Large</td>
</tr>
<tr class="even">
<td><strong>U-Net</strong></td>
<td>Deep Learning</td>
<td>Semantic segmentation (pixel-wise)</td>
<td>Large</td>
</tr>
<tr class="odd">
<td><strong>LSTM</strong></td>
<td>Deep Learning</td>
<td>Time series prediction</td>
<td>Large</td>
</tr>
</tbody>
</table>
<p><strong>Training process:</strong></p>
<ol type="1">
<li>Split data: training (70%), validation (15%), testing (15%)</li>
<li>Feed training data to algorithm</li>
<li>Algorithm adjusts parameters to minimize error</li>
<li>Monitor performance on validation set</li>
<li>Iterate: adjust hyperparameters if needed</li>
</ol>
</section>
<section id="step-6-validation-and-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="step-6-validation-and-evaluation">Step 6: Validation and Evaluation</h3>
<p><strong>Rigorous testing on independent data</strong></p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Never Test on Training Data!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Testing on data the model has seen gives falsely optimistic results. Always use held-out test data.</p>
</div>
</div>
<p><strong>Classification metrics:</strong></p>
<ul>
<li><strong>Overall Accuracy:</strong> Percentage of correctly classified pixels</li>
<li><strong>Confusion Matrix:</strong> Shows which classes are confused</li>
<li><strong>Producer’s Accuracy:</strong> How many ground truth samples were correctly classified</li>
<li><strong>User’s Accuracy:</strong> How many predicted samples are actually correct</li>
<li><strong>Kappa Coefficient:</strong> Agreement accounting for chance</li>
<li><strong>F1-Score:</strong> Harmonic mean of precision and recall</li>
</ul>
<p><strong>Regression metrics:</strong></p>
<ul>
<li><strong>RMSE (Root Mean Squared Error):</strong> Average prediction error</li>
<li><strong>MAE (Mean Absolute Error):</strong> Average absolute deviation</li>
<li><strong>R² (Coefficient of Determination):</strong> Proportion of variance explained</li>
</ul>
<p><strong>Philippine Example: Flood mapping evaluation</strong></p>
<pre><code>Confusion Matrix:
                Predicted
              | Flood | No Flood |
Actual Flood  |  450  |   50     |  Producer's Acc: 90%
Actual No Flood|  30   |  1470    |  Producer's Acc: 98%

User's Accuracy: 93.8%   96.7%
Overall Accuracy: 96%</code></pre>
</section>
<section id="step-7-deployment-and-operationalization" class="level3">
<h3 class="anchored" data-anchor-id="step-7-deployment-and-operationalization">Step 7: Deployment and Operationalization</h3>
<p><strong>Making the model operational:</strong></p>
<p><strong>Deployment strategies:</strong></p>
<ol type="1">
<li><strong>Batch processing:</strong> Apply model to large archives</li>
<li><strong>Near real-time:</strong> Process new satellite acquisitions automatically</li>
<li><strong>On-demand:</strong> User-triggered analysis</li>
<li><strong>Edge processing:</strong> On-board satellite AI (ESA Φsat-2)</li>
</ol>
<p><strong>Operational considerations:</strong></p>
<ul>
<li><strong>Scalability:</strong> Can it handle regional/national scale?</li>
<li><strong>Automation:</strong> Minimize manual intervention</li>
<li><strong>Monitoring:</strong> Track performance over time</li>
<li><strong>Retraining:</strong> Update model as conditions change</li>
<li><strong>Integration:</strong> Connect to decision support systems</li>
</ul>
<p><strong>Philippine context:</strong></p>
<ul>
<li>DOST-ASTI AIPI platform for model deployment</li>
<li>DIMER repository for model sharing</li>
<li>Integration with LGU disaster response protocols</li>
<li>Delivery via PhilSA Digital Space Campus</li>
</ul>
<hr>
</section>
</section>
<section id="part-3-types-of-machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="part-3-types-of-machine-learning">Part 3: Types of Machine Learning</h2>
<section id="supervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="supervised-learning">Supervised Learning</h3>
<p><strong>Learning from labeled data</strong></p>
<p>The algorithm is given: - <strong>Input:</strong> Satellite image or features - <strong>Output:</strong> Known label (class or value) - <strong>Goal:</strong> Learn mapping from input to output</p>
<section id="classification-tasks" class="level4">
<h4 class="anchored" data-anchor-id="classification-tasks">Classification Tasks</h4>
<p><strong>Predicting categorical labels</strong></p>
<p><strong>EO Examples:</strong></p>
<ol type="1">
<li><strong>Land Cover Classification</strong>
<ul>
<li>Input: Sentinel-2 pixel values</li>
<li>Output: Forest, Water, Urban, Agriculture, Bare soil</li>
<li>Algorithm: Random Forest, CNN</li>
</ul></li>
<li><strong>Cloud Detection</strong>
<ul>
<li>Input: Multi-band imagery</li>
<li>Output: Cloud vs.&nbsp;Clear</li>
<li>Algorithm: Threshold or ML classifier</li>
</ul></li>
<li><strong>Crop Type Mapping</strong>
<ul>
<li>Input: Multi-temporal NDVI</li>
<li>Output: Rice, Corn, Sugarcane, Coconut</li>
<li>Algorithm: Random Forest or LSTM</li>
</ul></li>
</ol>
<div class="philippine-context">
<p><strong>Philippine Case Study: Mangrove Mapping</strong></p>
<p><strong>Task:</strong> Classify pixels as mangrove or non-mangrove in coastal areas</p>
<p><strong>Data:</strong> - Sentinel-2 multi-temporal imagery (dry and wet season) - Field-validated mangrove polygons - NAMRIA coastal land cover baseline</p>
<p><strong>Approach:</strong> - Extract spectral values and indices (NDVI, NDWI) - Train Random Forest classifier - Validate against independent field data - Deploy via DOST-ASTI AIPI</p>
<p><strong>Result:</strong> 92% accuracy mangrove extent map for Palawan coastline</p>
</div>
</section>
<section id="regression-tasks" class="level4">
<h4 class="anchored" data-anchor-id="regression-tasks">Regression Tasks</h4>
<p><strong>Predicting continuous values</strong></p>
<p><strong>EO Examples:</strong></p>
<ol type="1">
<li><strong>Biomass Estimation</strong>
<ul>
<li>Input: Sentinel-1 SAR backscatter, Sentinel-2 vegetation indices</li>
<li>Output: Forest biomass (tons per hectare)</li>
<li>Algorithm: Random Forest Regression</li>
</ul></li>
<li><strong>Soil Moisture Prediction</strong>
<ul>
<li>Input: Sentinel-1 VV/VH polarization, temperature</li>
<li>Output: Volumetric soil moisture (%)</li>
<li>Algorithm: Neural network regression</li>
</ul></li>
<li><strong>Crop Yield Forecasting</strong>
<ul>
<li>Input: NDVI time series, rainfall, temperature</li>
<li>Output: Expected yield (tons per hectare)</li>
<li>Algorithm: LSTM regression</li>
</ul></li>
</ol>
<p><strong>Key difference from classification:</strong> - Output is a number on a continuous scale - Loss functions measure distance from true value (MSE, RMSE) - Evaluation uses regression metrics (R², RMSE)</p>
</section>
</section>
<section id="unsupervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="unsupervised-learning">Unsupervised Learning</h3>
<p><strong>Finding patterns in unlabeled data</strong></p>
<p>The algorithm receives: - <strong>Input:</strong> Satellite imagery or features - <strong>No labels provided</strong> - <strong>Goal:</strong> Discover inherent structure or groupings</p>
<section id="clustering" class="level4">
<h4 class="anchored" data-anchor-id="clustering">Clustering</h4>
<p><strong>Grouping similar pixels/regions together</strong></p>
<p><strong>Common algorithm: k-means</strong></p>
<ol type="1">
<li>Specify number of clusters (k)</li>
<li>Algorithm iteratively groups pixels with similar spectral characteristics</li>
<li>Result: Image segmented into k clusters</li>
<li><strong>Human interpretation needed:</strong> “Cluster 1 looks like water, Cluster 2 like forest…”</li>
</ol>
<p><strong>EO Applications:</strong></p>
<ul>
<li><strong>Exploratory analysis:</strong> “How many distinct spectral classes in this region?”</li>
<li><strong>Change detection:</strong> Cluster before/after images to find anomalies</li>
<li><strong>Image segmentation:</strong> Group similar pixels for object-based analysis</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>When to Use Unsupervised Learning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Advantages:</strong> - No need for expensive labeled data - Can discover unexpected patterns - Good for initial data exploration</p>
<p><strong>Limitations:</strong> - Results need interpretation - No guarantee clusters match desired classes - Often less accurate than supervised methods for specific tasks - Difficult to evaluate objectively</p>
</div>
</div>
<p><strong>Comparison Example:</strong></p>
<p><strong>Supervised (Land Cover Classification):</strong> - Provide 1000 labeled samples: forest, water, urban - Train Random Forest - Result: Every pixel assigned forest/water/urban - Evaluation: 90% accuracy against test labels</p>
<p><strong>Unsupervised (k-means Clustering):</strong> - No labels provided - Run k-means with k=3 - Result: Three clusters emerge - Interpretation: Cluster A=water, B=vegetation, C=mixed urban/bare - Evaluation: Subjective or requires labels anyway</p>
<hr>
</section>
</section>
</section>
<section id="part-4-introduction-to-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="part-4-introduction-to-deep-learning">Part 4: Introduction to Deep Learning</h2>
<section id="what-is-deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="what-is-deep-learning">What is Deep Learning?</h3>
<p><strong>Deep Learning = Neural Networks with Many Layers</strong></p>
<ul>
<li>Subset of machine learning</li>
<li>Inspired by biological neurons</li>
<li>Multiple processing layers extract progressively abstract features</li>
<li>Dominant approach for image analysis since ~2012</li>
</ul>
<p><strong>Why “deep”?</strong> - Refers to depth: many hidden layers - Modern networks: 10s to 100s of layers - Enables learning complex, hierarchical representations</p>
</section>
<section id="neural-network-fundamentals" class="level3">
<h3 class="anchored" data-anchor-id="neural-network-fundamentals">Neural Network Fundamentals</h3>
<section id="the-artificial-neuron" class="level4">
<h4 class="anchored" data-anchor-id="the-artificial-neuron">The Artificial Neuron</h4>
<p><strong>Building block of neural networks:</strong></p>
<pre><code>Inputs (x1, x2, x3) → [Weighted Sum + Bias] → Activation Function → Output</code></pre>
<p><strong>Mathematical operation:</strong></p>
<ol type="1">
<li><strong>Weighted sum:</strong> <code>z = w1*x1 + w2*x2 + w3*x3 + b</code></li>
<li><strong>Activation function:</strong> <code>output = activation(z)</code></li>
</ol>
<p><strong>Example: Detecting bright pixels</strong></p>
<pre><code>Inputs: [Red=0.8, Green=0.7, NIR=0.9]
Weights: [w1=1.0, w2=1.0, w3=1.0]
Bias: b = -2.0

z = 1.0*0.8 + 1.0*0.7 + 1.0*0.9 - 2.0 = 0.4
output = ReLU(0.4) = 0.4  (indicates moderately bright)</code></pre>
</section>
<section id="network-architecture" class="level4">
<h4 class="anchored" data-anchor-id="network-architecture">Network Architecture</h4>
<p><strong>Layers of neurons:</strong></p>
<ol type="1">
<li><strong>Input Layer:</strong> Receives raw data (e.g., pixel values)</li>
<li><strong>Hidden Layers:</strong> Process and transform data</li>
<li><strong>Output Layer:</strong> Produces final prediction</li>
</ol>
<p><strong>For a simple image classification:</strong></p>
<pre><code>Input Layer (256 neurons = 16x16 image)
   ↓
Hidden Layer 1 (128 neurons with ReLU)
   ↓
Hidden Layer 2 (64 neurons with ReLU)
   ↓
Output Layer (5 neurons = 5 classes, softmax activation)</code></pre>
<p>Each connection has a <strong>weight</strong> - the network learns optimal weights through training.</p>
</section>
<section id="activation-functions" class="level4">
<h4 class="anchored" data-anchor-id="activation-functions">Activation Functions</h4>
<p><strong>Introduce non-linearity - crucial for learning complex patterns</strong></p>
<p><strong>Common activation functions:</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Function</th>
<th>Equation</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>ReLU</strong></td>
<td><code>max(0, x)</code></td>
<td>Hidden layers (most common)</td>
</tr>
<tr class="even">
<td><strong>Sigmoid</strong></td>
<td><code>1 / (1 + e^-x)</code></td>
<td>Binary classification output</td>
</tr>
<tr class="odd">
<td><strong>Softmax</strong></td>
<td><code>e^xi / Σe^xj</code></td>
<td>Multi-class classification output</td>
</tr>
<tr class="even">
<td><strong>Tanh</strong></td>
<td><code>(e^x - e^-x) / (e^x + e^-x)</code></td>
<td>Hidden layers (older)</td>
</tr>
</tbody>
</table>
<p><strong>Why activation functions matter:</strong></p>
<p>Without non-linearity, multiple layers would collapse to a single linear transformation - no benefit from depth!</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>ReLU: The Default Choice
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>ReLU (Rectified Linear Unit)</strong> has become standard for hidden layers because:</p>
<ul>
<li>Simple: <code>f(x) = max(0, x)</code></li>
<li>Computationally efficient</li>
<li>Avoids vanishing gradient problem</li>
<li>Empirically performs very well</li>
</ul>
</div>
</div>
</section>
<section id="loss-functions" class="level4">
<h4 class="anchored" data-anchor-id="loss-functions">Loss Functions</h4>
<p><strong>Measure how wrong the model’s predictions are</strong></p>
<p>The model’s objective: <strong>minimize the loss function</strong></p>
<p><strong>For classification:</strong></p>
<p><strong>Categorical Cross-Entropy:</strong></p>
<pre><code>Loss = -Σ(y_true * log(y_pred))</code></pre>
<ul>
<li>Penalizes confident wrong predictions heavily</li>
<li>Encourages high probability for correct class</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code>True class: Forest (encoded as [1, 0, 0, 0, 0])
Prediction: [0.7, 0.1, 0.1, 0.05, 0.05]  ← Good, 70% on forest
Loss = -1*log(0.7) = 0.36

Prediction: [0.2, 0.3, 0.4, 0.05, 0.05]  ← Bad, only 20% on forest
Loss = -1*log(0.2) = 1.61  (much higher penalty)</code></pre>
<p><strong>For regression:</strong></p>
<p><strong>Mean Squared Error (MSE):</strong></p>
<pre><code>Loss = (1/n) * Σ(y_true - y_pred)²</code></pre>
<p><strong>Example: Biomass prediction:</strong></p>
<pre><code>True: 150 tons/ha
Prediction: 140 tons/ha
Error: 10 tons/ha
Squared Error: 100</code></pre>
</section>
<section id="optimizers" class="level4">
<h4 class="anchored" data-anchor-id="optimizers">Optimizers</h4>
<p><strong>Algorithms that adjust weights to minimize loss</strong></p>
<p><strong>The process:</strong></p>
<ol type="1">
<li>Calculate loss on current batch of data</li>
<li>Compute gradients (via backpropagation): how should each weight change?</li>
<li>Update weights in direction that reduces loss</li>
<li>Repeat thousands/millions of times</li>
</ol>
<p><strong>Common optimizers:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 29%">
<col style="width: 35%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Optimizer</th>
<th>Description</th>
<th>When to Use</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>SGD</strong></td>
<td>Stochastic Gradient Descent</td>
<td>Simple, well-understood</td>
</tr>
<tr class="even">
<td><strong>Adam</strong></td>
<td>Adaptive learning rate</td>
<td>Default choice, usually works well</td>
</tr>
<tr class="odd">
<td><strong>RMSprop</strong></td>
<td>Root Mean Square Propagation</td>
<td>Good for RNNs</td>
</tr>
<tr class="even">
<td><strong>AdaGrad</strong></td>
<td>Adaptive Gradient</td>
<td>When features vary in frequency</td>
</tr>
</tbody>
</table>
<p><strong>Adam is most popular</strong> because: - Adapts learning rate per parameter - Combines benefits of momentum and adaptive learning - Requires minimal tuning - Works well across diverse problems</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Training Terminology
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Epoch:</strong> One complete pass through the entire training dataset</p>
<p><strong>Batch:</strong> Subset of training data processed together before updating weights</p>
<p><strong>Iteration:</strong> One weight update (one batch processed)</p>
<p><strong>Example:</strong> - Training data: 10,000 samples - Batch size: 100 - 1 epoch = 100 iterations (10,000 / 100) - Training for 50 epochs = 5,000 iterations</p>
</div>
</div>
</section>
<section id="the-training-process" class="level4">
<h4 class="anchored" data-anchor-id="the-training-process">The Training Process</h4>
<p><strong>Iterative improvement:</strong></p>
<pre><code>1. Initialize weights randomly
2. For each epoch:
    For each batch:
        a. Forward pass: Compute predictions
        b. Calculate loss
        c. Backward pass: Compute gradients (backpropagation)
        d. Update weights using optimizer
    e. Evaluate on validation set
3. Stop when validation performance plateaus</code></pre>
<p><strong>Monitoring training:</strong></p>
<ul>
<li><strong>Training loss should decrease</strong> - model learning patterns</li>
<li><strong>Validation loss should decrease</strong> - model generalizing</li>
<li><strong>If validation loss increases while training loss decreases:</strong> Overfitting!</li>
</ul>
</section>
</section>
<section id="deep-learning-for-earth-observation" class="level3">
<h3 class="anchored" data-anchor-id="deep-learning-for-earth-observation">Deep Learning for Earth Observation</h3>
<p><strong>Why CNNs excel at EO:</strong></p>
<p>Traditional ML: - Manual feature engineering needed - Limited ability to capture spatial patterns - Each pixel treated somewhat independently</p>
<p>CNNs: - <strong>Automatic feature extraction</strong> from raw pixels - <strong>Spatial awareness</strong> through convolutional filters - <strong>Hierarchical learning:</strong> edges → textures → objects → scenes - <strong>Translation invariance:</strong> Detects patterns anywhere in image</p>
<p><strong>Common EO architectures:</strong></p>
<ol type="1">
<li><strong>CNNs:</strong> Image classification, object detection</li>
<li><strong>U-Net:</strong> Semantic segmentation (flood mapping, building extraction)</li>
<li><strong>ResNet:</strong> Very deep networks for complex classification</li>
<li><strong>LSTMs:</strong> Time series analysis (crop monitoring, drought prediction)</li>
</ol>
<p>We’ll explore these in depth on Days 2-4!</p>
<hr>
</section>
</section>
<section id="part-5-data-centric-ai-in-earth-observation" class="level2">
<h2 class="anchored" data-anchor-id="part-5-data-centric-ai-in-earth-observation">Part 5: Data-Centric AI in Earth Observation</h2>
<section id="the-paradigm-shift-2025" class="level3">
<h3 class="anchored" data-anchor-id="the-paradigm-shift-2025">The Paradigm Shift (2025)</h3>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Data &gt; Models
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Old paradigm (Model-Centric AI):</strong> - Focus on developing better algorithms - Keep data fixed, iterate on model architecture - “Our new model achieves 92% accuracy!”</p>
<p><strong>New paradigm (Data-Centric AI):</strong> - Focus on improving data quality and curation - Keep model fixed (use proven architectures), iterate on data - “Better data improved our model from 85% to 95% accuracy!”</p>
</div>
</div>
<p><strong>Why the shift?</strong></p>
<ol type="1">
<li><strong>Model architectures have matured:</strong> ResNet, U-Net, LSTM are well-established</li>
<li><strong>Biggest gains now come from data:</strong> Most underperforming models suffer from data issues</li>
<li><strong>Real-world deployment:</strong> Data quality determines operational success</li>
</ol>
</section>
<section id="pillar-1-data-quality" class="level3">
<h3 class="anchored" data-anchor-id="pillar-1-data-quality">Pillar 1: Data Quality</h3>
<p><strong>High-quality data is accurate, consistent, and properly processed</strong></p>
<p><strong>For satellite imagery:</strong></p>
<p><strong>Quality issues to address:</strong></p>
<ul>
<li><strong>Cloud contamination:</strong> Use Level-2A with SCL cloud masks</li>
<li><strong>Atmospheric effects:</strong> Always use atmospherically corrected data</li>
<li><strong>Sensor artifacts:</strong> Check for striping, banding, saturation</li>
<li><strong>Geometric accuracy:</strong> Ensure sub-pixel registration</li>
<li><strong>Radiometric consistency:</strong> Calibrate across sensors and times</li>
</ul>
<div class="philippine-context">
<p><strong>Philippine Challenge: Cloud Cover</strong></p>
<p>Philippines has one of highest cloud cover frequencies globally (&gt;60% during monsoon).</p>
<p><strong>Data quality solutions:</strong> - Multi-temporal compositing (median over 3 months) - Combine optical (Sentinel-2) + SAR (Sentinel-1) which penetrates clouds - Use aggressive cloud masking (accept fewer images for higher quality) - Leverage dry season (Dec-May) for optical data</p>
</div>
<p><strong>For training labels:</strong></p>
<p><strong>Quality issues:</strong></p>
<ul>
<li><strong>Positional error:</strong> GPS drift, georeferencing mismatch</li>
<li><strong>Temporal mismatch:</strong> 2018 labels with 2020 imagery</li>
<li><strong>Class ambiguity:</strong> Unclear definitions (shrub vs.&nbsp;sparse forest?)</li>
<li><strong>Mixed pixels:</strong> Polygon boundaries include multiple classes</li>
<li><strong>Labeling inconsistency:</strong> Different interpreters, different criteria</li>
</ul>
<p><strong>Best practices:</strong></p>
<ol type="1">
<li><strong>Clear class definitions:</strong> Document what each class includes/excludes</li>
<li><strong>Consistent methodology:</strong> Same interpreter, same time of year, same imagery</li>
<li><strong>Quality control:</strong> Multiple reviewers, consensus protocols</li>
<li><strong>Temporal alignment:</strong> Labels contemporary with imagery (within months)</li>
<li><strong>Positional accuracy:</strong> Use high-resolution reference imagery</li>
</ol>
</section>
<section id="pillar-2-data-quantity" class="level3">
<h3 class="anchored" data-anchor-id="pillar-2-data-quantity">Pillar 2: Data Quantity</h3>
<p><strong>More data (usually) improves performance</strong></p>
<p><strong>But quantity alone isn’t enough - quality matters more!</strong></p>
<p><strong>How much data do you need?</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Algorithm</th>
<th>Typical Requirements</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Random Forest</td>
<td>100s - 1000s samples per class</td>
</tr>
<tr class="even">
<td>Simple CNN</td>
<td>1000s - 10,000s samples</td>
</tr>
<tr class="odd">
<td>Deep CNN (ResNet)</td>
<td>10,000s - 100,000s samples</td>
</tr>
<tr class="even">
<td>Foundation Models</td>
<td>Millions - billions samples</td>
</tr>
</tbody>
</table>
<p><strong>Strategies when labeled data is limited:</strong></p>
<ol type="1">
<li><strong>Data Augmentation</strong>
<ul>
<li>Rotation, flipping, cropping</li>
<li>Color jittering (adjust brightness, contrast)</li>
<li>Adding noise</li>
<li><strong>Caution:</strong> Ensure augmentations are realistic for EO</li>
</ul></li>
<li><strong>Transfer Learning</strong>
<ul>
<li>Use model pre-trained on large dataset (ImageNet, SatMAE)</li>
<li>Fine-tune on your small dataset</li>
<li>Leverages learned features from similar tasks</li>
</ul></li>
<li><strong>Active Learning</strong>
<ul>
<li>Iteratively: train model → find uncertain predictions → label those → retrain</li>
<li>Efficiently focuses labeling effort where it matters most</li>
</ul></li>
<li><strong>Synthetic Data</strong>
<ul>
<li>Generate training data via simulation</li>
<li>Example: Simulated SAR scenes for flood detection</li>
</ul></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>2025 Research: Data Efficiency
</div>
</div>
<div class="callout-body-container callout-body">
<p>Recent studies show:</p>
<ul>
<li>Some EO tasks reach optimal accuracy with <strong>&lt;20% of temporal instances</strong></li>
<li><strong>Single band</strong> from single sensor can be sufficient for specific tasks</li>
<li><strong>Implication:</strong> Smart data selection &gt; brute force data collection</li>
</ul>
<p><strong>Source:</strong> “Data-Centric Machine Learning for Earth Observation: Necessary and Sufficient Features” (arXiv 2024)</p>
</div>
</div>
</section>
<section id="pillar-3-data-diversity" class="level3">
<h3 class="anchored" data-anchor-id="pillar-3-data-diversity">Pillar 3: Data Diversity</h3>
<p><strong>Representative data covers the full range of scenarios the model will encounter</strong></p>
<p><strong>Dimensions of diversity:</strong></p>
<ol type="1">
<li><strong>Geographic diversity</strong>
<ul>
<li>Different regions (Luzon, Visayas, Mindanao)</li>
<li>Different ecosystems (lowland, highland, coastal)</li>
<li>Different climate zones</li>
</ul></li>
<li><strong>Temporal diversity</strong>
<ul>
<li>Different seasons (wet, dry)</li>
<li>Different years (inter-annual variability)</li>
<li>Different phenological stages (planting, growing, harvest)</li>
</ul></li>
<li><strong>Class diversity</strong>
<ul>
<li>Multiple examples per class</li>
<li>Edge cases and rare types</li>
<li>Transitional zones</li>
</ul></li>
<li><strong>Sensor diversity</strong>
<ul>
<li>Different satellites (Sentinel-2A, 2B, 2C)</li>
<li>Different atmospheric conditions</li>
<li>Different viewing angles</li>
</ul></li>
</ol>
<p><strong>Example: Urban classification</strong></p>
<p><strong>Poor diversity:</strong> All training samples from Metro Manila CBD</p>
<p><strong>Result:</strong> Model fails on: - Small provincial towns (different building density) - Informal settlements (different materials) - Peri-urban areas (mixed land cover)</p>
<p><strong>Good diversity:</strong> Samples from: - Large cities (Manila, Cebu, Davao) - Medium towns (Baguio, Iloilo, Cagayan de Oro) - Small municipalities - Different building materials (concrete, metal roofing, nipa huts) - Different periods (to capture growth)</p>
<p><strong>Result:</strong> Model generalizes well across Philippines</p>
</section>
<section id="pillar-4-annotation-strategy" class="level3">
<h3 class="anchored" data-anchor-id="pillar-4-annotation-strategy">Pillar 4: Annotation Strategy</h3>
<p><strong>How you label data profoundly impacts model performance</strong></p>
<p><strong>Annotation approaches:</strong></p>
<ol type="1">
<li><strong>Point sampling:</strong> Fast, but limited context</li>
<li><strong>Polygon delineation:</strong> More information, more time-consuming</li>
<li><strong>Pixel-level labeling:</strong> Maximum detail, required for segmentation</li>
<li><strong>Image-level labels:</strong> Easiest, suitable for scene classification</li>
</ol>
<p><strong>Best practices:</strong></p>
<p><strong>1. Expert involvement</strong> - Use domain experts for complex classes (forest types, crop stages) - Train labelers thoroughly on class definitions - Regular calibration sessions</p>
<p><strong>2. Quality over quantity</strong> - 500 high-quality labels &gt; 5000 noisy labels - Invest in review and correction - Document difficult cases</p>
<p><strong>3. Class balance</strong> - Ensure adequate representation of minority classes - Stratified sampling by class - Consider class weights in training if imbalanced</p>
<p><strong>4. Consensus protocols</strong> - Multiple labelers per sample - Majority vote or adjudication for disagreements - Measure inter-annotator agreement</p>
<p><strong>5. Iterative refinement</strong> - Use model predictions to find label errors - Retrain after improving labels - Focus effort on low-confidence predictions</p>
<div class="philippine-context">
<p><strong>Philippine Solution: ALaM Project</strong></p>
<p>DOST-ASTI’s <strong>Automated Labeling Machine (ALaM)</strong> addresses annotation bottleneck:</p>
<ul>
<li>Combines automated labeling with crowdsourcing</li>
<li>Human-in-the-loop quality control</li>
<li>Integration with DIMER model repository</li>
<li>Reduces labeling time and cost significantly</li>
</ul>
</div>
</section>
<section id="examples-data-centric-success-stories" class="level3">
<h3 class="anchored" data-anchor-id="examples-data-centric-success-stories">2025 Examples: Data-Centric Success Stories</h3>
<section id="nasa-ibm-geospatial-foundation-model" class="level4">
<h4 class="anchored" data-anchor-id="nasa-ibm-geospatial-foundation-model">NASA-IBM Geospatial Foundation Model</h4>
<p><strong>Open-source model trained on massive HLS dataset (Harmonized Landsat-Sentinel-2)</strong></p>
<p><strong>Data-centric approach:</strong> - Millions of satellite images - Self-supervised pre-training (no labels needed) - Fine-tuned for specific tasks with small labeled datasets</p>
<p><strong>Result:</strong> - State-of-the-art performance on multiple EO tasks - Reduces labeled data requirements by 10-100x - Democratizes access to powerful EO AI</p>
</section>
<section id="esa-φsat-2-on-board-ai" class="level4">
<h4 class="anchored" data-anchor-id="esa-φsat-2-on-board-ai">ESA Φsat-2 On-Board AI</h4>
<p><strong>Launched 2025: 22cm CubeSat with on-board AI processing</strong></p>
<p><strong>Data-centric innovation:</strong> - Processes imagery directly on satellite - Only transmits actionable information (not raw data) - Reduces bandwidth requirements - Enables real-time event detection (fires, ships, clouds)</p>
<p><strong>Implication:</strong> Data quality selection happens in space!</p>
</section>
<section id="earthdaily-constellation" class="level4">
<h4 class="anchored" data-anchor-id="earthdaily-constellation">EarthDaily Constellation</h4>
<p><strong>10-satellite constellation for daily global coverage</strong></p>
<p><strong>Focus on AI-ready data:</strong> - Scientific-grade calibration - Consistent, reliable acquisitions - Optimized spectral bands for ML - Emphasis on data quality for algorithm performance</p>
<hr>
</section>
</section>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Session 2 Summary
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><strong>AI/ML learns patterns from data</strong> rather than explicit programming</li>
<li><strong>The EO workflow</strong> spans problem definition → data → preprocessing → features → training → validation → deployment</li>
<li><strong>Supervised learning</strong> (classification &amp; regression) is dominant for EO because we need specific outputs</li>
<li><strong>Unsupervised learning</strong> (clustering) is useful for exploration but requires interpretation</li>
<li><strong>Neural networks</strong> are composed of layers of neurons using activation functions, optimized via loss minimization</li>
<li><strong>Deep learning</strong> automatically extracts hierarchical features - dominant for image analysis</li>
<li><strong>Data-centric AI (2025 paradigm):</strong> Improving data quality, quantity, diversity, and annotation beats tweaking models</li>
<li><strong>For Philippine EO:</strong> Leverage DOST-ASTI tools (DIMER, AIPI, ALaM) to operationalize data-centric approaches</li>
</ol>
<p><strong>Next steps:</strong> Hands-on Python for geospatial data (Session 3) and Google Earth Engine (Session 4) to put these concepts into practice!</p>
</div>
</div>
<hr>
</section>
<section id="discussion-questions" class="level2">
<h2 class="anchored" data-anchor-id="discussion-questions">Discussion Questions</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Reflect &amp; Discuss
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><p><strong>What EO problem in your work</strong> could benefit from ML? Is it classification or regression?</p></li>
<li><p><strong>What data quality issues</strong> have you encountered with Philippine satellite data?</p></li>
<li><p><strong>How would you ensure diversity</strong> in training data for a national-scale land cover map?</p></li>
<li><p><strong>Which Philippine datasets</strong> (PhilSA, NAMRIA, PAGASA) could complement satellite imagery for your ML project?</p></li>
<li><p><strong>How might DOST-ASTI’s DIMER and AIPI platforms</strong> reduce barriers to deploying ML in your organization?</p></li>
</ol>
</div>
</div>
<hr>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<section id="foundational-concepts" class="level3">
<h3 class="anchored" data-anchor-id="foundational-concepts">Foundational Concepts</h3>
<ul>
<li><a href="https://appliedsciences.nasa.gov/get-involved/training/english/arset-fundamentals-machine-learning-earth-science">NASA ARSET: Fundamentals of Machine Learning for Earth Science</a></li>
<li><a href="https://arxiv.org/abs/2312.05327">Data-Centric AI: Better, Not Just More</a></li>
</ul>
</section>
<section id="neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="neural-networks">Neural Networks</h3>
<ul>
<li><a href="https://www.deeplearningbook.org/">Deep Learning Book (Goodfellow et al.)</a> - Free online</li>
<li><a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning (Nielsen)</a> - Interactive tutorial</li>
</ul>
</section>
<section id="eo-specific-ml" class="level3">
<h3 class="anchored" data-anchor-id="eo-specific-ml">EO-Specific ML</h3>
<ul>
<li><a href="https://eo-college.org/courses/introduction-to-machine-learning-for-earth-observation/">EO College: Introduction to Machine Learning for Earth Observation</a></li>
<li><a href="https://ml4earth.de/">ML4Earth Resources</a></li>
<li><a href="https://www.climatechange.ai/subject_areas/earth_observation_monitoring">Climate Change AI: Earth Observation &amp; Monitoring</a></li>
</ul>
</section>
<section id="philippine-ai-initiatives" class="level3">
<h3 class="anchored" data-anchor-id="philippine-ai-initiatives">Philippine AI Initiatives</h3>
<ul>
<li><a href="https://asti.dost.gov.ph/">DOST-ASTI SkAI-Pinas</a></li>
<li><a href="https://asti.dost.gov.ph/news-articles/asti-leads-ph-ai-revo-with-dimer-model-hub/">DIMER Model Hub</a></li>
</ul>
<hr>
<div class="session-nav">
<div class="session-nav-link" href="session1.qmd">
<div class="session-nav-label">
<p>← Previous</p>
</div>
<div class="session-nav-title">
<p>Session 1: Copernicus &amp; Philippine EO</p>
</div>
</div>
<div class="session-nav-link" href="session3.qmd">
<div class="session-nav-label">
<p>Next Session</p>
</div>
<div class="session-nav-title">
<p>Session 3: Python for Geospatial Data →</p>
</div>
</div>
</div>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/YOUR_GITHUB_USERNAME\.github\.io\/ESAPhil");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="copphil-training/day1" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Session 2: Core Concepts of AI/ML for Earth Observation"</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Understanding the fundamentals of machine learning for satellite data analysis"</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> last-modified</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>::: {.session-info}</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>**Duration:** 2 hours | **Format:** Lecture + Conceptual Exercises | **Platform:** Presentation</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="fu">## Session Overview</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>This session provides a comprehensive introduction to Artificial Intelligence and Machine Learning concepts specifically tailored for Earth Observation applications. You'll learn the complete AI/ML workflow, understand different learning paradigms, explore neural network fundamentals, and discover why data quality matters more than model complexity in 2025's data-centric AI paradigm.</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>::: {.learning-objectives}</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="fu">### Learning Objectives</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>By the end of this session, you will be able to:</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Define** AI and ML in the context of Earth Observation</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Describe** the complete AI/ML workflow from problem definition to deployment</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Distinguish** between supervised and unsupervised learning with EO examples</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Explain** classification vs. regression tasks in satellite data analysis</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Understand** neural network architecture fundamentals</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Identify** key components: neurons, layers, activation functions, loss functions, optimizers</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Articulate** the data-centric AI paradigm and its importance for EO</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Apply** best practices for data quality, quantity, diversity, and annotation</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="fu">## Part 1: What is AI/ML?</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a><span class="fu">### Defining the Terms</span></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>**Artificial Intelligence (AI):**</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Broad field focused on creating intelligent machines</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Systems that can perceive, reason, learn, and act</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Includes everything from rule-based systems to machine learning</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>**Machine Learning (ML):**</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Subset of AI focused on learning from data</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Algorithms that improve performance through experience</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Key distinction:** No explicit programming of rules</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a><span class="fu">## The ML Difference</span></span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>**Traditional Programming:**</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a><span class="in">Rules + Data → Output</span></span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a>**Machine Learning:**</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a><span class="in">Data + Desired Output → Rules (Model)</span></span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>In EO: Instead of coding "if NIR &gt; 0.6 and Red &lt; 0.3, then forest", ML learns the pattern from labeled examples.</span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why ML for Earth Observation?</span></span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a>**Challenges that ML addresses:**</span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Scale:** Petabytes of satellite data - impossible to manually analyze</span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Complexity:** Multispectral, temporal, spatial patterns humans can't easily detect</span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Consistency:** Automated processing ensures reproducible results</span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Speed:** Real-time disaster mapping requires immediate analysis</span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a>**Traditional vs. ML approaches:**</span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Task <span class="pp">|</span> Traditional <span class="pp">|</span> ML Approach <span class="pp">|</span></span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-------------|-------------|</span></span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Water detection** <span class="pp">|</span> Manual NDWI threshold <span class="pp">|</span> Learn optimal threshold + texture from examples <span class="pp">|</span></span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Land cover** <span class="pp">|</span> Rule-based classification <span class="pp">|</span> Random Forest or CNN with training samples <span class="pp">|</span></span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Flood mapping** <span class="pp">|</span> Expert visual interpretation <span class="pp">|</span> U-Net segmentation trained on labeled floods <span class="pp">|</span></span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Crop monitoring** <span class="pp">|</span> Fixed vegetation index thresholds <span class="pp">|</span> LSTM time series model learning phenology <span class="pp">|</span></span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-84"><a href="#cb13-84" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb13-85"><a href="#cb13-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-86"><a href="#cb13-86" aria-hidden="true" tabindex="-1"></a><span class="fu">## Part 2: The AI/ML Workflow for Earth Observation</span></span>
<span id="cb13-87"><a href="#cb13-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-88"><a href="#cb13-88" aria-hidden="true" tabindex="-1"></a>Understanding the complete workflow is essential for successful EO projects. Each step matters.</span>
<span id="cb13-89"><a href="#cb13-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-90"><a href="#cb13-90" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 1: Problem Definition</span></span>
<span id="cb13-91"><a href="#cb13-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-92"><a href="#cb13-92" aria-hidden="true" tabindex="-1"></a>**Define clearly what you want to achieve:**</span>
<span id="cb13-93"><a href="#cb13-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-94"><a href="#cb13-94" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What question are you answering? (e.g., "Where are mangroves declining?")</span>
<span id="cb13-95"><a href="#cb13-95" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What output do you need? (map, time series, alert system?)</span>
<span id="cb13-96"><a href="#cb13-96" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What accuracy is acceptable?</span>
<span id="cb13-97"><a href="#cb13-97" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What constraints exist? (time, computational resources, data availability)</span>
<span id="cb13-98"><a href="#cb13-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-99"><a href="#cb13-99" aria-hidden="true" tabindex="-1"></a>::: {.philippine-context}</span>
<span id="cb13-100"><a href="#cb13-100" aria-hidden="true" tabindex="-1"></a>**Philippine Example:**</span>
<span id="cb13-101"><a href="#cb13-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-102"><a href="#cb13-102" aria-hidden="true" tabindex="-1"></a>**Problem:** Map rice paddies in Central Luzon to estimate harvest timing for food security</span>
<span id="cb13-103"><a href="#cb13-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-104"><a href="#cb13-104" aria-hidden="true" tabindex="-1"></a>**Clear definition:**</span>
<span id="cb13-105"><a href="#cb13-105" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Binary classification: rice vs. non-rice</span>
<span id="cb13-106"><a href="#cb13-106" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>20m spatial resolution acceptable (Sentinel-2 bands)</span>
<span id="cb13-107"><a href="#cb13-107" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Temporal: wet and dry season separately</span>
<span id="cb13-108"><a href="#cb13-108" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Accuracy target: &gt;85% for operational use</span>
<span id="cb13-109"><a href="#cb13-109" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-110"><a href="#cb13-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-111"><a href="#cb13-111" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 2: Data Acquisition</span></span>
<span id="cb13-112"><a href="#cb13-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-113"><a href="#cb13-113" aria-hidden="true" tabindex="-1"></a>**Gather all necessary data:**</span>
<span id="cb13-114"><a href="#cb13-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-115"><a href="#cb13-115" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Satellite imagery:** Sentinel-1/2, Landsat, commercial VHR</span>
<span id="cb13-116"><a href="#cb13-116" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Ground truth:** Field surveys, high-res imagery interpretation, existing maps</span>
<span id="cb13-117"><a href="#cb13-117" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Ancillary data:** DEM, climate, administrative boundaries</span>
<span id="cb13-118"><a href="#cb13-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-119"><a href="#cb13-119" aria-hidden="true" tabindex="-1"></a>**Data sources for Philippines:**</span>
<span id="cb13-120"><a href="#cb13-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-121"><a href="#cb13-121" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Copernicus Data Space Ecosystem (Sentinel-1/2)</span>
<span id="cb13-122"><a href="#cb13-122" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>PhilSA SIYASAT (NovaSAR-1)</span>
<span id="cb13-123"><a href="#cb13-123" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>NAMRIA Geoportal (land cover basemaps)</span>
<span id="cb13-124"><a href="#cb13-124" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>PAGASA (climate data)</span>
<span id="cb13-125"><a href="#cb13-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-126"><a href="#cb13-126" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 3: Data Pre-processing</span></span>
<span id="cb13-127"><a href="#cb13-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-128"><a href="#cb13-128" aria-hidden="true" tabindex="-1"></a>**Critical step - "Garbage in, garbage out"**</span>
<span id="cb13-129"><a href="#cb13-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-130"><a href="#cb13-130" aria-hidden="true" tabindex="-1"></a>**For satellite imagery:**</span>
<span id="cb13-131"><a href="#cb13-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-132"><a href="#cb13-132" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Atmospheric correction:** Convert to surface reflectance (use Level-2A)</span>
<span id="cb13-133"><a href="#cb13-133" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Cloud masking:** Remove or mask cloudy pixels</span>
<span id="cb13-134"><a href="#cb13-134" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Geometric correction:** Ensure proper alignment</span>
<span id="cb13-135"><a href="#cb13-135" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Radiometric calibration:** Consistent values across scenes</span>
<span id="cb13-136"><a href="#cb13-136" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Temporal compositing:** Reduce clouds via median/mean composites</span>
<span id="cb13-137"><a href="#cb13-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-138"><a href="#cb13-138" aria-hidden="true" tabindex="-1"></a>**For training labels:**</span>
<span id="cb13-139"><a href="#cb13-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-140"><a href="#cb13-140" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Quality control:** Verify label accuracy</span>
<span id="cb13-141"><a href="#cb13-141" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Coordinate alignment:** Ensure labels match imagery timing and location</span>
<span id="cb13-142"><a href="#cb13-142" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Class balancing:** Ensure adequate samples per class</span>
<span id="cb13-143"><a href="#cb13-143" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Format standardization:** Convert to ML-ready format</span>
<span id="cb13-144"><a href="#cb13-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-145"><a href="#cb13-145" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb13-146"><a href="#cb13-146" aria-hidden="true" tabindex="-1"></a><span class="fu">## Pre-processing Pitfalls</span></span>
<span id="cb13-147"><a href="#cb13-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-148"><a href="#cb13-148" aria-hidden="true" tabindex="-1"></a>**Common errors that degrade model performance:**</span>
<span id="cb13-149"><a href="#cb13-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-150"><a href="#cb13-150" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Using Top-of-Atmosphere instead of surface reflectance</span>
<span id="cb13-151"><a href="#cb13-151" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Temporal mismatch: 2020 imagery with 2018 labels</span>
<span id="cb13-152"><a href="#cb13-152" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Incomplete cloud masking leaving cloud shadows</span>
<span id="cb13-153"><a href="#cb13-153" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Mixed pixels at boundaries (especially for validation)</span>
<span id="cb13-154"><a href="#cb13-154" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Inconsistent band ordering across scenes</span>
<span id="cb13-155"><a href="#cb13-155" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-156"><a href="#cb13-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-157"><a href="#cb13-157" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 4: Feature Engineering</span></span>
<span id="cb13-158"><a href="#cb13-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-159"><a href="#cb13-159" aria-hidden="true" tabindex="-1"></a>**Deriving informative variables from raw data**</span>
<span id="cb13-160"><a href="#cb13-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-161"><a href="#cb13-161" aria-hidden="true" tabindex="-1"></a>**For traditional ML (Random Forest, SVM):**</span>
<span id="cb13-162"><a href="#cb13-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-163"><a href="#cb13-163" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Spectral indices:** NDVI, NDWI, NDBI, EVI, SAVI</span>
<span id="cb13-164"><a href="#cb13-164" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Textural features:** GLCM metrics (contrast, entropy)</span>
<span id="cb13-165"><a href="#cb13-165" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Temporal features:** Mean, std dev, phenology metrics</span>
<span id="cb13-166"><a href="#cb13-166" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Topographic features:** Elevation, slope, aspect (from DEM)</span>
<span id="cb13-167"><a href="#cb13-167" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Contextual features:** Distance to roads, water bodies</span>
<span id="cb13-168"><a href="#cb13-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-169"><a href="#cb13-169" aria-hidden="true" tabindex="-1"></a>**Example: Forest classification features**</span>
<span id="cb13-170"><a href="#cb13-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-171"><a href="#cb13-171" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb13-172"><a href="#cb13-172" aria-hidden="true" tabindex="-1"></a><span class="co"># Spectral indices</span></span>
<span id="cb13-173"><a href="#cb13-173" aria-hidden="true" tabindex="-1"></a>NDVI <span class="op">=</span> (NIR <span class="op">-</span> Red) <span class="op">/</span> (NIR <span class="op">+</span> Red)</span>
<span id="cb13-174"><a href="#cb13-174" aria-hidden="true" tabindex="-1"></a>NDWI <span class="op">=</span> (Green <span class="op">-</span> NIR) <span class="op">/</span> (Green <span class="op">+</span> NIR)</span>
<span id="cb13-175"><a href="#cb13-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-176"><a href="#cb13-176" aria-hidden="true" tabindex="-1"></a><span class="co"># Texture (from GLCM)</span></span>
<span id="cb13-177"><a href="#cb13-177" aria-hidden="true" tabindex="-1"></a>Contrast <span class="op">=</span> ...  <span class="co"># measure of local variation</span></span>
<span id="cb13-178"><a href="#cb13-178" aria-hidden="true" tabindex="-1"></a>Homogeneity <span class="op">=</span> ...  <span class="co"># measure of uniformity</span></span>
<span id="cb13-179"><a href="#cb13-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-180"><a href="#cb13-180" aria-hidden="true" tabindex="-1"></a><span class="co"># Topographic</span></span>
<span id="cb13-181"><a href="#cb13-181" aria-hidden="true" tabindex="-1"></a>Elevation, Slope</span>
<span id="cb13-182"><a href="#cb13-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-183"><a href="#cb13-183" aria-hidden="true" tabindex="-1"></a><span class="co"># Result: Input feature vector per pixel</span></span>
<span id="cb13-184"><a href="#cb13-184" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> [Red, Green, Blue, NIR, SWIR1, SWIR2, NDVI, NDWI, Contrast, Elevation, Slope]</span>
<span id="cb13-185"><a href="#cb13-185" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-186"><a href="#cb13-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-187"><a href="#cb13-187" aria-hidden="true" tabindex="-1"></a>**For deep learning (CNNs):**</span>
<span id="cb13-188"><a href="#cb13-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-189"><a href="#cb13-189" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Less manual feature engineering needed</span>
<span id="cb13-190"><a href="#cb13-190" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Networks automatically learn features from raw pixels</span>
<span id="cb13-191"><a href="#cb13-191" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Still benefit from good input data (cloud-free, calibrated)</span>
<span id="cb13-192"><a href="#cb13-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-193"><a href="#cb13-193" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 5: Model Selection and Training</span></span>
<span id="cb13-194"><a href="#cb13-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-195"><a href="#cb13-195" aria-hidden="true" tabindex="-1"></a>**Choose appropriate algorithm:**</span>
<span id="cb13-196"><a href="#cb13-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-197"><a href="#cb13-197" aria-hidden="true" tabindex="-1"></a>**Consider:**</span>
<span id="cb13-198"><a href="#cb13-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-199"><a href="#cb13-199" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Task type (classification, regression, segmentation)</span>
<span id="cb13-200"><a href="#cb13-200" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Data size (deep learning needs more data)</span>
<span id="cb13-201"><a href="#cb13-201" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Interpretability requirements</span>
<span id="cb13-202"><a href="#cb13-202" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Computational resources</span>
<span id="cb13-203"><a href="#cb13-203" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Deployment constraints</span>
<span id="cb13-204"><a href="#cb13-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-205"><a href="#cb13-205" aria-hidden="true" tabindex="-1"></a>**Common EO algorithms:**</span>
<span id="cb13-206"><a href="#cb13-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-207"><a href="#cb13-207" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Algorithm <span class="pp">|</span> Type <span class="pp">|</span> Best For <span class="pp">|</span> Data Needs <span class="pp">|</span></span>
<span id="cb13-208"><a href="#cb13-208" aria-hidden="true" tabindex="-1"></a><span class="pp">|-----------|------|----------|------------|</span></span>
<span id="cb13-209"><a href="#cb13-209" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Random Forest** <span class="pp">|</span> Ensemble <span class="pp">|</span> Classification, feature importance <span class="pp">|</span> Medium <span class="pp">|</span></span>
<span id="cb13-210"><a href="#cb13-210" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **SVM** <span class="pp">|</span> Kernel <span class="pp">|</span> Binary classification, small data <span class="pp">|</span> Small-Medium <span class="pp">|</span></span>
<span id="cb13-211"><a href="#cb13-211" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **CNN** <span class="pp">|</span> Deep Learning <span class="pp">|</span> Image classification, automatic features <span class="pp">|</span> Large <span class="pp">|</span></span>
<span id="cb13-212"><a href="#cb13-212" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **U-Net** <span class="pp">|</span> Deep Learning <span class="pp">|</span> Semantic segmentation (pixel-wise) <span class="pp">|</span> Large <span class="pp">|</span></span>
<span id="cb13-213"><a href="#cb13-213" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **LSTM** <span class="pp">|</span> Deep Learning <span class="pp">|</span> Time series prediction <span class="pp">|</span> Large <span class="pp">|</span></span>
<span id="cb13-214"><a href="#cb13-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-215"><a href="#cb13-215" aria-hidden="true" tabindex="-1"></a>**Training process:**</span>
<span id="cb13-216"><a href="#cb13-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-217"><a href="#cb13-217" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Split data: training (70%), validation (15%), testing (15%)</span>
<span id="cb13-218"><a href="#cb13-218" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Feed training data to algorithm</span>
<span id="cb13-219"><a href="#cb13-219" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Algorithm adjusts parameters to minimize error</span>
<span id="cb13-220"><a href="#cb13-220" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Monitor performance on validation set</span>
<span id="cb13-221"><a href="#cb13-221" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Iterate: adjust hyperparameters if needed</span>
<span id="cb13-222"><a href="#cb13-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-223"><a href="#cb13-223" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 6: Validation and Evaluation</span></span>
<span id="cb13-224"><a href="#cb13-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-225"><a href="#cb13-225" aria-hidden="true" tabindex="-1"></a>**Rigorous testing on independent data**</span>
<span id="cb13-226"><a href="#cb13-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-227"><a href="#cb13-227" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb13-228"><a href="#cb13-228" aria-hidden="true" tabindex="-1"></a><span class="fu">## Never Test on Training Data!</span></span>
<span id="cb13-229"><a href="#cb13-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-230"><a href="#cb13-230" aria-hidden="true" tabindex="-1"></a>Testing on data the model has seen gives falsely optimistic results. Always use held-out test data.</span>
<span id="cb13-231"><a href="#cb13-231" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-232"><a href="#cb13-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-233"><a href="#cb13-233" aria-hidden="true" tabindex="-1"></a>**Classification metrics:**</span>
<span id="cb13-234"><a href="#cb13-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-235"><a href="#cb13-235" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Overall Accuracy:** Percentage of correctly classified pixels</span>
<span id="cb13-236"><a href="#cb13-236" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Confusion Matrix:** Shows which classes are confused</span>
<span id="cb13-237"><a href="#cb13-237" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Producer's Accuracy:** How many ground truth samples were correctly classified</span>
<span id="cb13-238"><a href="#cb13-238" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**User's Accuracy:** How many predicted samples are actually correct</span>
<span id="cb13-239"><a href="#cb13-239" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Kappa Coefficient:** Agreement accounting for chance</span>
<span id="cb13-240"><a href="#cb13-240" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**F1-Score:** Harmonic mean of precision and recall</span>
<span id="cb13-241"><a href="#cb13-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-242"><a href="#cb13-242" aria-hidden="true" tabindex="-1"></a>**Regression metrics:**</span>
<span id="cb13-243"><a href="#cb13-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-244"><a href="#cb13-244" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**RMSE (Root Mean Squared Error):** Average prediction error</span>
<span id="cb13-245"><a href="#cb13-245" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**MAE (Mean Absolute Error):** Average absolute deviation</span>
<span id="cb13-246"><a href="#cb13-246" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**R² (Coefficient of Determination):** Proportion of variance explained</span>
<span id="cb13-247"><a href="#cb13-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-248"><a href="#cb13-248" aria-hidden="true" tabindex="-1"></a>**Philippine Example: Flood mapping evaluation**</span>
<span id="cb13-249"><a href="#cb13-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-250"><a href="#cb13-250" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-251"><a href="#cb13-251" aria-hidden="true" tabindex="-1"></a><span class="in">Confusion Matrix:</span></span>
<span id="cb13-252"><a href="#cb13-252" aria-hidden="true" tabindex="-1"></a><span class="in">                Predicted</span></span>
<span id="cb13-253"><a href="#cb13-253" aria-hidden="true" tabindex="-1"></a><span class="in">              | Flood | No Flood |</span></span>
<span id="cb13-254"><a href="#cb13-254" aria-hidden="true" tabindex="-1"></a><span class="in">Actual Flood  |  450  |   50     |  Producer's Acc: 90%</span></span>
<span id="cb13-255"><a href="#cb13-255" aria-hidden="true" tabindex="-1"></a><span class="in">Actual No Flood|  30   |  1470    |  Producer's Acc: 98%</span></span>
<span id="cb13-256"><a href="#cb13-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-257"><a href="#cb13-257" aria-hidden="true" tabindex="-1"></a><span class="in">User's Accuracy: 93.8%   96.7%</span></span>
<span id="cb13-258"><a href="#cb13-258" aria-hidden="true" tabindex="-1"></a><span class="in">Overall Accuracy: 96%</span></span>
<span id="cb13-259"><a href="#cb13-259" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-260"><a href="#cb13-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-261"><a href="#cb13-261" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 7: Deployment and Operationalization</span></span>
<span id="cb13-262"><a href="#cb13-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-263"><a href="#cb13-263" aria-hidden="true" tabindex="-1"></a>**Making the model operational:**</span>
<span id="cb13-264"><a href="#cb13-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-265"><a href="#cb13-265" aria-hidden="true" tabindex="-1"></a>**Deployment strategies:**</span>
<span id="cb13-266"><a href="#cb13-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-267"><a href="#cb13-267" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Batch processing:** Apply model to large archives</span>
<span id="cb13-268"><a href="#cb13-268" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Near real-time:** Process new satellite acquisitions automatically</span>
<span id="cb13-269"><a href="#cb13-269" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**On-demand:** User-triggered analysis</span>
<span id="cb13-270"><a href="#cb13-270" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Edge processing:** On-board satellite AI (ESA Φsat-2)</span>
<span id="cb13-271"><a href="#cb13-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-272"><a href="#cb13-272" aria-hidden="true" tabindex="-1"></a>**Operational considerations:**</span>
<span id="cb13-273"><a href="#cb13-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-274"><a href="#cb13-274" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Scalability:** Can it handle regional/national scale?</span>
<span id="cb13-275"><a href="#cb13-275" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Automation:** Minimize manual intervention</span>
<span id="cb13-276"><a href="#cb13-276" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Monitoring:** Track performance over time</span>
<span id="cb13-277"><a href="#cb13-277" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Retraining:** Update model as conditions change</span>
<span id="cb13-278"><a href="#cb13-278" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Integration:** Connect to decision support systems</span>
<span id="cb13-279"><a href="#cb13-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-280"><a href="#cb13-280" aria-hidden="true" tabindex="-1"></a>**Philippine context:**</span>
<span id="cb13-281"><a href="#cb13-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-282"><a href="#cb13-282" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>DOST-ASTI AIPI platform for model deployment</span>
<span id="cb13-283"><a href="#cb13-283" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>DIMER repository for model sharing</span>
<span id="cb13-284"><a href="#cb13-284" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Integration with LGU disaster response protocols</span>
<span id="cb13-285"><a href="#cb13-285" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Delivery via PhilSA Digital Space Campus</span>
<span id="cb13-286"><a href="#cb13-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-287"><a href="#cb13-287" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb13-288"><a href="#cb13-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-289"><a href="#cb13-289" aria-hidden="true" tabindex="-1"></a><span class="fu">## Part 3: Types of Machine Learning</span></span>
<span id="cb13-290"><a href="#cb13-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-291"><a href="#cb13-291" aria-hidden="true" tabindex="-1"></a><span class="fu">### Supervised Learning</span></span>
<span id="cb13-292"><a href="#cb13-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-293"><a href="#cb13-293" aria-hidden="true" tabindex="-1"></a>**Learning from labeled data**</span>
<span id="cb13-294"><a href="#cb13-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-295"><a href="#cb13-295" aria-hidden="true" tabindex="-1"></a>The algorithm is given:</span>
<span id="cb13-296"><a href="#cb13-296" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Input:** Satellite image or features</span>
<span id="cb13-297"><a href="#cb13-297" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output:** Known label (class or value)</span>
<span id="cb13-298"><a href="#cb13-298" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Goal:** Learn mapping from input to output</span>
<span id="cb13-299"><a href="#cb13-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-300"><a href="#cb13-300" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Classification Tasks</span></span>
<span id="cb13-301"><a href="#cb13-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-302"><a href="#cb13-302" aria-hidden="true" tabindex="-1"></a>**Predicting categorical labels**</span>
<span id="cb13-303"><a href="#cb13-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-304"><a href="#cb13-304" aria-hidden="true" tabindex="-1"></a>**EO Examples:**</span>
<span id="cb13-305"><a href="#cb13-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-306"><a href="#cb13-306" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Land Cover Classification**</span>
<span id="cb13-307"><a href="#cb13-307" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Input: Sentinel-2 pixel values</span>
<span id="cb13-308"><a href="#cb13-308" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Output: Forest, Water, Urban, Agriculture, Bare soil</span>
<span id="cb13-309"><a href="#cb13-309" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Algorithm: Random Forest, CNN</span>
<span id="cb13-310"><a href="#cb13-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-311"><a href="#cb13-311" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Cloud Detection**</span>
<span id="cb13-312"><a href="#cb13-312" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Input: Multi-band imagery</span>
<span id="cb13-313"><a href="#cb13-313" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Output: Cloud vs. Clear</span>
<span id="cb13-314"><a href="#cb13-314" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Algorithm: Threshold or ML classifier</span>
<span id="cb13-315"><a href="#cb13-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-316"><a href="#cb13-316" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Crop Type Mapping**</span>
<span id="cb13-317"><a href="#cb13-317" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Input: Multi-temporal NDVI</span>
<span id="cb13-318"><a href="#cb13-318" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Output: Rice, Corn, Sugarcane, Coconut</span>
<span id="cb13-319"><a href="#cb13-319" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Algorithm: Random Forest or LSTM</span>
<span id="cb13-320"><a href="#cb13-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-321"><a href="#cb13-321" aria-hidden="true" tabindex="-1"></a>::: {.philippine-context}</span>
<span id="cb13-322"><a href="#cb13-322" aria-hidden="true" tabindex="-1"></a>**Philippine Case Study: Mangrove Mapping**</span>
<span id="cb13-323"><a href="#cb13-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-324"><a href="#cb13-324" aria-hidden="true" tabindex="-1"></a>**Task:** Classify pixels as mangrove or non-mangrove in coastal areas</span>
<span id="cb13-325"><a href="#cb13-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-326"><a href="#cb13-326" aria-hidden="true" tabindex="-1"></a>**Data:**</span>
<span id="cb13-327"><a href="#cb13-327" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sentinel-2 multi-temporal imagery (dry and wet season)</span>
<span id="cb13-328"><a href="#cb13-328" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Field-validated mangrove polygons</span>
<span id="cb13-329"><a href="#cb13-329" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>NAMRIA coastal land cover baseline</span>
<span id="cb13-330"><a href="#cb13-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-331"><a href="#cb13-331" aria-hidden="true" tabindex="-1"></a>**Approach:**</span>
<span id="cb13-332"><a href="#cb13-332" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Extract spectral values and indices (NDVI, NDWI)</span>
<span id="cb13-333"><a href="#cb13-333" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Train Random Forest classifier</span>
<span id="cb13-334"><a href="#cb13-334" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Validate against independent field data</span>
<span id="cb13-335"><a href="#cb13-335" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Deploy via DOST-ASTI AIPI</span>
<span id="cb13-336"><a href="#cb13-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-337"><a href="#cb13-337" aria-hidden="true" tabindex="-1"></a>**Result:** 92% accuracy mangrove extent map for Palawan coastline</span>
<span id="cb13-338"><a href="#cb13-338" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-339"><a href="#cb13-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-340"><a href="#cb13-340" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Regression Tasks</span></span>
<span id="cb13-341"><a href="#cb13-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-342"><a href="#cb13-342" aria-hidden="true" tabindex="-1"></a>**Predicting continuous values**</span>
<span id="cb13-343"><a href="#cb13-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-344"><a href="#cb13-344" aria-hidden="true" tabindex="-1"></a>**EO Examples:**</span>
<span id="cb13-345"><a href="#cb13-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-346"><a href="#cb13-346" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Biomass Estimation**</span>
<span id="cb13-347"><a href="#cb13-347" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Input: Sentinel-1 SAR backscatter, Sentinel-2 vegetation indices</span>
<span id="cb13-348"><a href="#cb13-348" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Output: Forest biomass (tons per hectare)</span>
<span id="cb13-349"><a href="#cb13-349" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Algorithm: Random Forest Regression</span>
<span id="cb13-350"><a href="#cb13-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-351"><a href="#cb13-351" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Soil Moisture Prediction**</span>
<span id="cb13-352"><a href="#cb13-352" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Input: Sentinel-1 VV/VH polarization, temperature</span>
<span id="cb13-353"><a href="#cb13-353" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Output: Volumetric soil moisture (%)</span>
<span id="cb13-354"><a href="#cb13-354" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Algorithm: Neural network regression</span>
<span id="cb13-355"><a href="#cb13-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-356"><a href="#cb13-356" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Crop Yield Forecasting**</span>
<span id="cb13-357"><a href="#cb13-357" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Input: NDVI time series, rainfall, temperature</span>
<span id="cb13-358"><a href="#cb13-358" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Output: Expected yield (tons per hectare)</span>
<span id="cb13-359"><a href="#cb13-359" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Algorithm: LSTM regression</span>
<span id="cb13-360"><a href="#cb13-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-361"><a href="#cb13-361" aria-hidden="true" tabindex="-1"></a>**Key difference from classification:**</span>
<span id="cb13-362"><a href="#cb13-362" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Output is a number on a continuous scale</span>
<span id="cb13-363"><a href="#cb13-363" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Loss functions measure distance from true value (MSE, RMSE)</span>
<span id="cb13-364"><a href="#cb13-364" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Evaluation uses regression metrics (R², RMSE)</span>
<span id="cb13-365"><a href="#cb13-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-366"><a href="#cb13-366" aria-hidden="true" tabindex="-1"></a><span class="fu">### Unsupervised Learning</span></span>
<span id="cb13-367"><a href="#cb13-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-368"><a href="#cb13-368" aria-hidden="true" tabindex="-1"></a>**Finding patterns in unlabeled data**</span>
<span id="cb13-369"><a href="#cb13-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-370"><a href="#cb13-370" aria-hidden="true" tabindex="-1"></a>The algorithm receives:</span>
<span id="cb13-371"><a href="#cb13-371" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Input:** Satellite imagery or features</span>
<span id="cb13-372"><a href="#cb13-372" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**No labels provided**</span>
<span id="cb13-373"><a href="#cb13-373" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Goal:** Discover inherent structure or groupings</span>
<span id="cb13-374"><a href="#cb13-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-375"><a href="#cb13-375" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Clustering</span></span>
<span id="cb13-376"><a href="#cb13-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-377"><a href="#cb13-377" aria-hidden="true" tabindex="-1"></a>**Grouping similar pixels/regions together**</span>
<span id="cb13-378"><a href="#cb13-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-379"><a href="#cb13-379" aria-hidden="true" tabindex="-1"></a>**Common algorithm: k-means**</span>
<span id="cb13-380"><a href="#cb13-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-381"><a href="#cb13-381" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Specify number of clusters (k)</span>
<span id="cb13-382"><a href="#cb13-382" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Algorithm iteratively groups pixels with similar spectral characteristics</span>
<span id="cb13-383"><a href="#cb13-383" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Result: Image segmented into k clusters</span>
<span id="cb13-384"><a href="#cb13-384" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Human interpretation needed:** "Cluster 1 looks like water, Cluster 2 like forest..."</span>
<span id="cb13-385"><a href="#cb13-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-386"><a href="#cb13-386" aria-hidden="true" tabindex="-1"></a>**EO Applications:**</span>
<span id="cb13-387"><a href="#cb13-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-388"><a href="#cb13-388" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Exploratory analysis:** "How many distinct spectral classes in this region?"</span>
<span id="cb13-389"><a href="#cb13-389" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Change detection:** Cluster before/after images to find anomalies</span>
<span id="cb13-390"><a href="#cb13-390" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Image segmentation:** Group similar pixels for object-based analysis</span>
<span id="cb13-391"><a href="#cb13-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-392"><a href="#cb13-392" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb13-393"><a href="#cb13-393" aria-hidden="true" tabindex="-1"></a><span class="fu">## When to Use Unsupervised Learning</span></span>
<span id="cb13-394"><a href="#cb13-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-395"><a href="#cb13-395" aria-hidden="true" tabindex="-1"></a>**Advantages:**</span>
<span id="cb13-396"><a href="#cb13-396" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>No need for expensive labeled data</span>
<span id="cb13-397"><a href="#cb13-397" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can discover unexpected patterns</span>
<span id="cb13-398"><a href="#cb13-398" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Good for initial data exploration</span>
<span id="cb13-399"><a href="#cb13-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-400"><a href="#cb13-400" aria-hidden="true" tabindex="-1"></a>**Limitations:**</span>
<span id="cb13-401"><a href="#cb13-401" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Results need interpretation</span>
<span id="cb13-402"><a href="#cb13-402" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>No guarantee clusters match desired classes</span>
<span id="cb13-403"><a href="#cb13-403" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Often less accurate than supervised methods for specific tasks</span>
<span id="cb13-404"><a href="#cb13-404" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Difficult to evaluate objectively</span>
<span id="cb13-405"><a href="#cb13-405" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-406"><a href="#cb13-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-407"><a href="#cb13-407" aria-hidden="true" tabindex="-1"></a>**Comparison Example:**</span>
<span id="cb13-408"><a href="#cb13-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-409"><a href="#cb13-409" aria-hidden="true" tabindex="-1"></a>**Supervised (Land Cover Classification):**</span>
<span id="cb13-410"><a href="#cb13-410" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Provide 1000 labeled samples: forest, water, urban</span>
<span id="cb13-411"><a href="#cb13-411" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Train Random Forest</span>
<span id="cb13-412"><a href="#cb13-412" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Result: Every pixel assigned forest/water/urban</span>
<span id="cb13-413"><a href="#cb13-413" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Evaluation: 90% accuracy against test labels</span>
<span id="cb13-414"><a href="#cb13-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-415"><a href="#cb13-415" aria-hidden="true" tabindex="-1"></a>**Unsupervised (k-means Clustering):**</span>
<span id="cb13-416"><a href="#cb13-416" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>No labels provided</span>
<span id="cb13-417"><a href="#cb13-417" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Run k-means with k=3</span>
<span id="cb13-418"><a href="#cb13-418" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Result: Three clusters emerge</span>
<span id="cb13-419"><a href="#cb13-419" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Interpretation: Cluster A=water, B=vegetation, C=mixed urban/bare</span>
<span id="cb13-420"><a href="#cb13-420" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Evaluation: Subjective or requires labels anyway</span>
<span id="cb13-421"><a href="#cb13-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-422"><a href="#cb13-422" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb13-423"><a href="#cb13-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-424"><a href="#cb13-424" aria-hidden="true" tabindex="-1"></a><span class="fu">## Part 4: Introduction to Deep Learning</span></span>
<span id="cb13-425"><a href="#cb13-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-426"><a href="#cb13-426" aria-hidden="true" tabindex="-1"></a><span class="fu">### What is Deep Learning?</span></span>
<span id="cb13-427"><a href="#cb13-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-428"><a href="#cb13-428" aria-hidden="true" tabindex="-1"></a>**Deep Learning = Neural Networks with Many Layers**</span>
<span id="cb13-429"><a href="#cb13-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-430"><a href="#cb13-430" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Subset of machine learning</span>
<span id="cb13-431"><a href="#cb13-431" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Inspired by biological neurons</span>
<span id="cb13-432"><a href="#cb13-432" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multiple processing layers extract progressively abstract features</span>
<span id="cb13-433"><a href="#cb13-433" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dominant approach for image analysis since ~2012</span>
<span id="cb13-434"><a href="#cb13-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-435"><a href="#cb13-435" aria-hidden="true" tabindex="-1"></a>**Why "deep"?**</span>
<span id="cb13-436"><a href="#cb13-436" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Refers to depth: many hidden layers</span>
<span id="cb13-437"><a href="#cb13-437" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Modern networks: 10s to 100s of layers</span>
<span id="cb13-438"><a href="#cb13-438" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Enables learning complex, hierarchical representations</span>
<span id="cb13-439"><a href="#cb13-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-440"><a href="#cb13-440" aria-hidden="true" tabindex="-1"></a><span class="fu">### Neural Network Fundamentals</span></span>
<span id="cb13-441"><a href="#cb13-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-442"><a href="#cb13-442" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The Artificial Neuron</span></span>
<span id="cb13-443"><a href="#cb13-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-444"><a href="#cb13-444" aria-hidden="true" tabindex="-1"></a>**Building block of neural networks:**</span>
<span id="cb13-445"><a href="#cb13-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-446"><a href="#cb13-446" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-447"><a href="#cb13-447" aria-hidden="true" tabindex="-1"></a><span class="in">Inputs (x1, x2, x3) → [Weighted Sum + Bias] → Activation Function → Output</span></span>
<span id="cb13-448"><a href="#cb13-448" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-449"><a href="#cb13-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-450"><a href="#cb13-450" aria-hidden="true" tabindex="-1"></a>**Mathematical operation:**</span>
<span id="cb13-451"><a href="#cb13-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-452"><a href="#cb13-452" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Weighted sum:** <span class="in">`z = w1*x1 + w2*x2 + w3*x3 + b`</span></span>
<span id="cb13-453"><a href="#cb13-453" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Activation function:** <span class="in">`output = activation(z)`</span></span>
<span id="cb13-454"><a href="#cb13-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-455"><a href="#cb13-455" aria-hidden="true" tabindex="-1"></a>**Example: Detecting bright pixels**</span>
<span id="cb13-456"><a href="#cb13-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-457"><a href="#cb13-457" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-458"><a href="#cb13-458" aria-hidden="true" tabindex="-1"></a><span class="in">Inputs: [Red=0.8, Green=0.7, NIR=0.9]</span></span>
<span id="cb13-459"><a href="#cb13-459" aria-hidden="true" tabindex="-1"></a><span class="in">Weights: [w1=1.0, w2=1.0, w3=1.0]</span></span>
<span id="cb13-460"><a href="#cb13-460" aria-hidden="true" tabindex="-1"></a><span class="in">Bias: b = -2.0</span></span>
<span id="cb13-461"><a href="#cb13-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-462"><a href="#cb13-462" aria-hidden="true" tabindex="-1"></a><span class="in">z = 1.0*0.8 + 1.0*0.7 + 1.0*0.9 - 2.0 = 0.4</span></span>
<span id="cb13-463"><a href="#cb13-463" aria-hidden="true" tabindex="-1"></a><span class="in">output = ReLU(0.4) = 0.4  (indicates moderately bright)</span></span>
<span id="cb13-464"><a href="#cb13-464" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-465"><a href="#cb13-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-466"><a href="#cb13-466" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Network Architecture</span></span>
<span id="cb13-467"><a href="#cb13-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-468"><a href="#cb13-468" aria-hidden="true" tabindex="-1"></a>**Layers of neurons:**</span>
<span id="cb13-469"><a href="#cb13-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-470"><a href="#cb13-470" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Input Layer:** Receives raw data (e.g., pixel values)</span>
<span id="cb13-471"><a href="#cb13-471" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Hidden Layers:** Process and transform data</span>
<span id="cb13-472"><a href="#cb13-472" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Output Layer:** Produces final prediction</span>
<span id="cb13-473"><a href="#cb13-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-474"><a href="#cb13-474" aria-hidden="true" tabindex="-1"></a>**For a simple image classification:**</span>
<span id="cb13-475"><a href="#cb13-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-476"><a href="#cb13-476" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-477"><a href="#cb13-477" aria-hidden="true" tabindex="-1"></a><span class="in">Input Layer (256 neurons = 16x16 image)</span></span>
<span id="cb13-478"><a href="#cb13-478" aria-hidden="true" tabindex="-1"></a><span class="in">   ↓</span></span>
<span id="cb13-479"><a href="#cb13-479" aria-hidden="true" tabindex="-1"></a><span class="in">Hidden Layer 1 (128 neurons with ReLU)</span></span>
<span id="cb13-480"><a href="#cb13-480" aria-hidden="true" tabindex="-1"></a><span class="in">   ↓</span></span>
<span id="cb13-481"><a href="#cb13-481" aria-hidden="true" tabindex="-1"></a><span class="in">Hidden Layer 2 (64 neurons with ReLU)</span></span>
<span id="cb13-482"><a href="#cb13-482" aria-hidden="true" tabindex="-1"></a><span class="in">   ↓</span></span>
<span id="cb13-483"><a href="#cb13-483" aria-hidden="true" tabindex="-1"></a><span class="in">Output Layer (5 neurons = 5 classes, softmax activation)</span></span>
<span id="cb13-484"><a href="#cb13-484" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-485"><a href="#cb13-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-486"><a href="#cb13-486" aria-hidden="true" tabindex="-1"></a>Each connection has a **weight** - the network learns optimal weights through training.</span>
<span id="cb13-487"><a href="#cb13-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-488"><a href="#cb13-488" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Activation Functions</span></span>
<span id="cb13-489"><a href="#cb13-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-490"><a href="#cb13-490" aria-hidden="true" tabindex="-1"></a>**Introduce non-linearity - crucial for learning complex patterns**</span>
<span id="cb13-491"><a href="#cb13-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-492"><a href="#cb13-492" aria-hidden="true" tabindex="-1"></a>**Common activation functions:**</span>
<span id="cb13-493"><a href="#cb13-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-494"><a href="#cb13-494" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Function <span class="pp">|</span> Equation <span class="pp">|</span> Use Case <span class="pp">|</span></span>
<span id="cb13-495"><a href="#cb13-495" aria-hidden="true" tabindex="-1"></a><span class="pp">|----------|----------|----------|</span></span>
<span id="cb13-496"><a href="#cb13-496" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **ReLU** <span class="pp">|</span> <span class="in">`max(0, x)`</span> <span class="pp">|</span> Hidden layers (most common) <span class="pp">|</span></span>
<span id="cb13-497"><a href="#cb13-497" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Sigmoid** <span class="pp">|</span> <span class="in">`1 / (1 + e^-x)`</span> <span class="pp">|</span> Binary classification output <span class="pp">|</span></span>
<span id="cb13-498"><a href="#cb13-498" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Softmax** <span class="pp">|</span> <span class="in">`e^xi / Σe^xj`</span> <span class="pp">|</span> Multi-class classification output <span class="pp">|</span></span>
<span id="cb13-499"><a href="#cb13-499" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Tanh** <span class="pp">|</span> <span class="in">`(e^x - e^-x) / (e^x + e^-x)`</span> <span class="pp">|</span> Hidden layers (older) <span class="pp">|</span></span>
<span id="cb13-500"><a href="#cb13-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-501"><a href="#cb13-501" aria-hidden="true" tabindex="-1"></a>**Why activation functions matter:**</span>
<span id="cb13-502"><a href="#cb13-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-503"><a href="#cb13-503" aria-hidden="true" tabindex="-1"></a>Without non-linearity, multiple layers would collapse to a single linear transformation - no benefit from depth!</span>
<span id="cb13-504"><a href="#cb13-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-505"><a href="#cb13-505" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb13-506"><a href="#cb13-506" aria-hidden="true" tabindex="-1"></a><span class="fu">## ReLU: The Default Choice</span></span>
<span id="cb13-507"><a href="#cb13-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-508"><a href="#cb13-508" aria-hidden="true" tabindex="-1"></a>**ReLU (Rectified Linear Unit)** has become standard for hidden layers because:</span>
<span id="cb13-509"><a href="#cb13-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-510"><a href="#cb13-510" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Simple: <span class="in">`f(x) = max(0, x)`</span></span>
<span id="cb13-511"><a href="#cb13-511" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Computationally efficient</span>
<span id="cb13-512"><a href="#cb13-512" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Avoids vanishing gradient problem</span>
<span id="cb13-513"><a href="#cb13-513" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Empirically performs very well</span>
<span id="cb13-514"><a href="#cb13-514" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-515"><a href="#cb13-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-516"><a href="#cb13-516" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Loss Functions</span></span>
<span id="cb13-517"><a href="#cb13-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-518"><a href="#cb13-518" aria-hidden="true" tabindex="-1"></a>**Measure how wrong the model's predictions are**</span>
<span id="cb13-519"><a href="#cb13-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-520"><a href="#cb13-520" aria-hidden="true" tabindex="-1"></a>The model's objective: **minimize the loss function**</span>
<span id="cb13-521"><a href="#cb13-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-522"><a href="#cb13-522" aria-hidden="true" tabindex="-1"></a>**For classification:**</span>
<span id="cb13-523"><a href="#cb13-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-524"><a href="#cb13-524" aria-hidden="true" tabindex="-1"></a>**Categorical Cross-Entropy:**</span>
<span id="cb13-525"><a href="#cb13-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-526"><a href="#cb13-526" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-527"><a href="#cb13-527" aria-hidden="true" tabindex="-1"></a><span class="in">Loss = -Σ(y_true * log(y_pred))</span></span>
<span id="cb13-528"><a href="#cb13-528" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-529"><a href="#cb13-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-530"><a href="#cb13-530" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Penalizes confident wrong predictions heavily</span>
<span id="cb13-531"><a href="#cb13-531" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Encourages high probability for correct class</span>
<span id="cb13-532"><a href="#cb13-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-533"><a href="#cb13-533" aria-hidden="true" tabindex="-1"></a>**Example:**</span>
<span id="cb13-534"><a href="#cb13-534" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-535"><a href="#cb13-535" aria-hidden="true" tabindex="-1"></a><span class="in">True class: Forest (encoded as [1, 0, 0, 0, 0])</span></span>
<span id="cb13-536"><a href="#cb13-536" aria-hidden="true" tabindex="-1"></a><span class="in">Prediction: [0.7, 0.1, 0.1, 0.05, 0.05]  ← Good, 70% on forest</span></span>
<span id="cb13-537"><a href="#cb13-537" aria-hidden="true" tabindex="-1"></a><span class="in">Loss = -1*log(0.7) = 0.36</span></span>
<span id="cb13-538"><a href="#cb13-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-539"><a href="#cb13-539" aria-hidden="true" tabindex="-1"></a><span class="in">Prediction: [0.2, 0.3, 0.4, 0.05, 0.05]  ← Bad, only 20% on forest</span></span>
<span id="cb13-540"><a href="#cb13-540" aria-hidden="true" tabindex="-1"></a><span class="in">Loss = -1*log(0.2) = 1.61  (much higher penalty)</span></span>
<span id="cb13-541"><a href="#cb13-541" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-542"><a href="#cb13-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-543"><a href="#cb13-543" aria-hidden="true" tabindex="-1"></a>**For regression:**</span>
<span id="cb13-544"><a href="#cb13-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-545"><a href="#cb13-545" aria-hidden="true" tabindex="-1"></a>**Mean Squared Error (MSE):**</span>
<span id="cb13-546"><a href="#cb13-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-547"><a href="#cb13-547" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-548"><a href="#cb13-548" aria-hidden="true" tabindex="-1"></a><span class="in">Loss = (1/n) * Σ(y_true - y_pred)²</span></span>
<span id="cb13-549"><a href="#cb13-549" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-550"><a href="#cb13-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-551"><a href="#cb13-551" aria-hidden="true" tabindex="-1"></a>**Example: Biomass prediction:**</span>
<span id="cb13-552"><a href="#cb13-552" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-553"><a href="#cb13-553" aria-hidden="true" tabindex="-1"></a><span class="in">True: 150 tons/ha</span></span>
<span id="cb13-554"><a href="#cb13-554" aria-hidden="true" tabindex="-1"></a><span class="in">Prediction: 140 tons/ha</span></span>
<span id="cb13-555"><a href="#cb13-555" aria-hidden="true" tabindex="-1"></a><span class="in">Error: 10 tons/ha</span></span>
<span id="cb13-556"><a href="#cb13-556" aria-hidden="true" tabindex="-1"></a><span class="in">Squared Error: 100</span></span>
<span id="cb13-557"><a href="#cb13-557" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-558"><a href="#cb13-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-559"><a href="#cb13-559" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Optimizers</span></span>
<span id="cb13-560"><a href="#cb13-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-561"><a href="#cb13-561" aria-hidden="true" tabindex="-1"></a>**Algorithms that adjust weights to minimize loss**</span>
<span id="cb13-562"><a href="#cb13-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-563"><a href="#cb13-563" aria-hidden="true" tabindex="-1"></a>**The process:**</span>
<span id="cb13-564"><a href="#cb13-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-565"><a href="#cb13-565" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Calculate loss on current batch of data</span>
<span id="cb13-566"><a href="#cb13-566" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Compute gradients (via backpropagation): how should each weight change?</span>
<span id="cb13-567"><a href="#cb13-567" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Update weights in direction that reduces loss</span>
<span id="cb13-568"><a href="#cb13-568" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Repeat thousands/millions of times</span>
<span id="cb13-569"><a href="#cb13-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-570"><a href="#cb13-570" aria-hidden="true" tabindex="-1"></a>**Common optimizers:**</span>
<span id="cb13-571"><a href="#cb13-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-572"><a href="#cb13-572" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Optimizer <span class="pp">|</span> Description <span class="pp">|</span> When to Use <span class="pp">|</span></span>
<span id="cb13-573"><a href="#cb13-573" aria-hidden="true" tabindex="-1"></a><span class="pp">|-----------|-------------|-------------|</span></span>
<span id="cb13-574"><a href="#cb13-574" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **SGD** <span class="pp">|</span> Stochastic Gradient Descent <span class="pp">|</span> Simple, well-understood <span class="pp">|</span></span>
<span id="cb13-575"><a href="#cb13-575" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Adam** <span class="pp">|</span> Adaptive learning rate <span class="pp">|</span> Default choice, usually works well <span class="pp">|</span></span>
<span id="cb13-576"><a href="#cb13-576" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **RMSprop** <span class="pp">|</span> Root Mean Square Propagation <span class="pp">|</span> Good for RNNs <span class="pp">|</span></span>
<span id="cb13-577"><a href="#cb13-577" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **AdaGrad** <span class="pp">|</span> Adaptive Gradient <span class="pp">|</span> When features vary in frequency <span class="pp">|</span></span>
<span id="cb13-578"><a href="#cb13-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-579"><a href="#cb13-579" aria-hidden="true" tabindex="-1"></a>**Adam is most popular** because:</span>
<span id="cb13-580"><a href="#cb13-580" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Adapts learning rate per parameter</span>
<span id="cb13-581"><a href="#cb13-581" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Combines benefits of momentum and adaptive learning</span>
<span id="cb13-582"><a href="#cb13-582" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Requires minimal tuning</span>
<span id="cb13-583"><a href="#cb13-583" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Works well across diverse problems</span>
<span id="cb13-584"><a href="#cb13-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-585"><a href="#cb13-585" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb13-586"><a href="#cb13-586" aria-hidden="true" tabindex="-1"></a><span class="fu">## Training Terminology</span></span>
<span id="cb13-587"><a href="#cb13-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-588"><a href="#cb13-588" aria-hidden="true" tabindex="-1"></a>**Epoch:** One complete pass through the entire training dataset</span>
<span id="cb13-589"><a href="#cb13-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-590"><a href="#cb13-590" aria-hidden="true" tabindex="-1"></a>**Batch:** Subset of training data processed together before updating weights</span>
<span id="cb13-591"><a href="#cb13-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-592"><a href="#cb13-592" aria-hidden="true" tabindex="-1"></a>**Iteration:** One weight update (one batch processed)</span>
<span id="cb13-593"><a href="#cb13-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-594"><a href="#cb13-594" aria-hidden="true" tabindex="-1"></a>**Example:**</span>
<span id="cb13-595"><a href="#cb13-595" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Training data: 10,000 samples</span>
<span id="cb13-596"><a href="#cb13-596" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Batch size: 100</span>
<span id="cb13-597"><a href="#cb13-597" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>1 epoch = 100 iterations (10,000 / 100)</span>
<span id="cb13-598"><a href="#cb13-598" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Training for 50 epochs = 5,000 iterations</span>
<span id="cb13-599"><a href="#cb13-599" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-600"><a href="#cb13-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-601"><a href="#cb13-601" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The Training Process</span></span>
<span id="cb13-602"><a href="#cb13-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-603"><a href="#cb13-603" aria-hidden="true" tabindex="-1"></a>**Iterative improvement:**</span>
<span id="cb13-604"><a href="#cb13-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-605"><a href="#cb13-605" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-606"><a href="#cb13-606" aria-hidden="true" tabindex="-1"></a><span class="in">1. Initialize weights randomly</span></span>
<span id="cb13-607"><a href="#cb13-607" aria-hidden="true" tabindex="-1"></a><span class="in">2. For each epoch:</span></span>
<span id="cb13-608"><a href="#cb13-608" aria-hidden="true" tabindex="-1"></a><span class="in">    For each batch:</span></span>
<span id="cb13-609"><a href="#cb13-609" aria-hidden="true" tabindex="-1"></a><span class="in">        a. Forward pass: Compute predictions</span></span>
<span id="cb13-610"><a href="#cb13-610" aria-hidden="true" tabindex="-1"></a><span class="in">        b. Calculate loss</span></span>
<span id="cb13-611"><a href="#cb13-611" aria-hidden="true" tabindex="-1"></a><span class="in">        c. Backward pass: Compute gradients (backpropagation)</span></span>
<span id="cb13-612"><a href="#cb13-612" aria-hidden="true" tabindex="-1"></a><span class="in">        d. Update weights using optimizer</span></span>
<span id="cb13-613"><a href="#cb13-613" aria-hidden="true" tabindex="-1"></a><span class="in">    e. Evaluate on validation set</span></span>
<span id="cb13-614"><a href="#cb13-614" aria-hidden="true" tabindex="-1"></a><span class="in">3. Stop when validation performance plateaus</span></span>
<span id="cb13-615"><a href="#cb13-615" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-616"><a href="#cb13-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-617"><a href="#cb13-617" aria-hidden="true" tabindex="-1"></a>**Monitoring training:**</span>
<span id="cb13-618"><a href="#cb13-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-619"><a href="#cb13-619" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Training loss should decrease** - model learning patterns</span>
<span id="cb13-620"><a href="#cb13-620" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Validation loss should decrease** - model generalizing</span>
<span id="cb13-621"><a href="#cb13-621" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**If validation loss increases while training loss decreases:** Overfitting!</span>
<span id="cb13-622"><a href="#cb13-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-623"><a href="#cb13-623" aria-hidden="true" tabindex="-1"></a><span class="fu">### Deep Learning for Earth Observation</span></span>
<span id="cb13-624"><a href="#cb13-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-625"><a href="#cb13-625" aria-hidden="true" tabindex="-1"></a>**Why CNNs excel at EO:**</span>
<span id="cb13-626"><a href="#cb13-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-627"><a href="#cb13-627" aria-hidden="true" tabindex="-1"></a>Traditional ML:</span>
<span id="cb13-628"><a href="#cb13-628" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Manual feature engineering needed</span>
<span id="cb13-629"><a href="#cb13-629" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Limited ability to capture spatial patterns</span>
<span id="cb13-630"><a href="#cb13-630" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each pixel treated somewhat independently</span>
<span id="cb13-631"><a href="#cb13-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-632"><a href="#cb13-632" aria-hidden="true" tabindex="-1"></a>CNNs:</span>
<span id="cb13-633"><a href="#cb13-633" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Automatic feature extraction** from raw pixels</span>
<span id="cb13-634"><a href="#cb13-634" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Spatial awareness** through convolutional filters</span>
<span id="cb13-635"><a href="#cb13-635" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hierarchical learning:** edges → textures → objects → scenes</span>
<span id="cb13-636"><a href="#cb13-636" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Translation invariance:** Detects patterns anywhere in image</span>
<span id="cb13-637"><a href="#cb13-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-638"><a href="#cb13-638" aria-hidden="true" tabindex="-1"></a>**Common EO architectures:**</span>
<span id="cb13-639"><a href="#cb13-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-640"><a href="#cb13-640" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**CNNs:** Image classification, object detection</span>
<span id="cb13-641"><a href="#cb13-641" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**U-Net:** Semantic segmentation (flood mapping, building extraction)</span>
<span id="cb13-642"><a href="#cb13-642" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**ResNet:** Very deep networks for complex classification</span>
<span id="cb13-643"><a href="#cb13-643" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**LSTMs:** Time series analysis (crop monitoring, drought prediction)</span>
<span id="cb13-644"><a href="#cb13-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-645"><a href="#cb13-645" aria-hidden="true" tabindex="-1"></a>We'll explore these in depth on Days 2-4!</span>
<span id="cb13-646"><a href="#cb13-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-647"><a href="#cb13-647" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb13-648"><a href="#cb13-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-649"><a href="#cb13-649" aria-hidden="true" tabindex="-1"></a><span class="fu">## Part 5: Data-Centric AI in Earth Observation</span></span>
<span id="cb13-650"><a href="#cb13-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-651"><a href="#cb13-651" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Paradigm Shift (2025)</span></span>
<span id="cb13-652"><a href="#cb13-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-653"><a href="#cb13-653" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb13-654"><a href="#cb13-654" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data &gt; Models</span></span>
<span id="cb13-655"><a href="#cb13-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-656"><a href="#cb13-656" aria-hidden="true" tabindex="-1"></a>**Old paradigm (Model-Centric AI):**</span>
<span id="cb13-657"><a href="#cb13-657" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Focus on developing better algorithms</span>
<span id="cb13-658"><a href="#cb13-658" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Keep data fixed, iterate on model architecture</span>
<span id="cb13-659"><a href="#cb13-659" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"Our new model achieves 92% accuracy!"</span>
<span id="cb13-660"><a href="#cb13-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-661"><a href="#cb13-661" aria-hidden="true" tabindex="-1"></a>**New paradigm (Data-Centric AI):**</span>
<span id="cb13-662"><a href="#cb13-662" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Focus on improving data quality and curation</span>
<span id="cb13-663"><a href="#cb13-663" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Keep model fixed (use proven architectures), iterate on data</span>
<span id="cb13-664"><a href="#cb13-664" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"Better data improved our model from 85% to 95% accuracy!"</span>
<span id="cb13-665"><a href="#cb13-665" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-666"><a href="#cb13-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-667"><a href="#cb13-667" aria-hidden="true" tabindex="-1"></a>**Why the shift?**</span>
<span id="cb13-668"><a href="#cb13-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-669"><a href="#cb13-669" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Model architectures have matured:** ResNet, U-Net, LSTM are well-established</span>
<span id="cb13-670"><a href="#cb13-670" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Biggest gains now come from data:** Most underperforming models suffer from data issues</span>
<span id="cb13-671"><a href="#cb13-671" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Real-world deployment:** Data quality determines operational success</span>
<span id="cb13-672"><a href="#cb13-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-673"><a href="#cb13-673" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pillar 1: Data Quality</span></span>
<span id="cb13-674"><a href="#cb13-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-675"><a href="#cb13-675" aria-hidden="true" tabindex="-1"></a>**High-quality data is accurate, consistent, and properly processed**</span>
<span id="cb13-676"><a href="#cb13-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-677"><a href="#cb13-677" aria-hidden="true" tabindex="-1"></a>**For satellite imagery:**</span>
<span id="cb13-678"><a href="#cb13-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-679"><a href="#cb13-679" aria-hidden="true" tabindex="-1"></a>**Quality issues to address:**</span>
<span id="cb13-680"><a href="#cb13-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-681"><a href="#cb13-681" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Cloud contamination:** Use Level-2A with SCL cloud masks</span>
<span id="cb13-682"><a href="#cb13-682" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Atmospheric effects:** Always use atmospherically corrected data</span>
<span id="cb13-683"><a href="#cb13-683" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sensor artifacts:** Check for striping, banding, saturation</span>
<span id="cb13-684"><a href="#cb13-684" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Geometric accuracy:** Ensure sub-pixel registration</span>
<span id="cb13-685"><a href="#cb13-685" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Radiometric consistency:** Calibrate across sensors and times</span>
<span id="cb13-686"><a href="#cb13-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-687"><a href="#cb13-687" aria-hidden="true" tabindex="-1"></a>::: {.philippine-context}</span>
<span id="cb13-688"><a href="#cb13-688" aria-hidden="true" tabindex="-1"></a>**Philippine Challenge: Cloud Cover**</span>
<span id="cb13-689"><a href="#cb13-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-690"><a href="#cb13-690" aria-hidden="true" tabindex="-1"></a>Philippines has one of highest cloud cover frequencies globally (&gt;60% during monsoon).</span>
<span id="cb13-691"><a href="#cb13-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-692"><a href="#cb13-692" aria-hidden="true" tabindex="-1"></a>**Data quality solutions:**</span>
<span id="cb13-693"><a href="#cb13-693" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-temporal compositing (median over 3 months)</span>
<span id="cb13-694"><a href="#cb13-694" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Combine optical (Sentinel-2) + SAR (Sentinel-1) which penetrates clouds</span>
<span id="cb13-695"><a href="#cb13-695" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use aggressive cloud masking (accept fewer images for higher quality)</span>
<span id="cb13-696"><a href="#cb13-696" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Leverage dry season (Dec-May) for optical data</span>
<span id="cb13-697"><a href="#cb13-697" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-698"><a href="#cb13-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-699"><a href="#cb13-699" aria-hidden="true" tabindex="-1"></a>**For training labels:**</span>
<span id="cb13-700"><a href="#cb13-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-701"><a href="#cb13-701" aria-hidden="true" tabindex="-1"></a>**Quality issues:**</span>
<span id="cb13-702"><a href="#cb13-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-703"><a href="#cb13-703" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Positional error:** GPS drift, georeferencing mismatch</span>
<span id="cb13-704"><a href="#cb13-704" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Temporal mismatch:** 2018 labels with 2020 imagery</span>
<span id="cb13-705"><a href="#cb13-705" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Class ambiguity:** Unclear definitions (shrub vs. sparse forest?)</span>
<span id="cb13-706"><a href="#cb13-706" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Mixed pixels:** Polygon boundaries include multiple classes</span>
<span id="cb13-707"><a href="#cb13-707" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Labeling inconsistency:** Different interpreters, different criteria</span>
<span id="cb13-708"><a href="#cb13-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-709"><a href="#cb13-709" aria-hidden="true" tabindex="-1"></a>**Best practices:**</span>
<span id="cb13-710"><a href="#cb13-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-711"><a href="#cb13-711" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Clear class definitions:** Document what each class includes/excludes</span>
<span id="cb13-712"><a href="#cb13-712" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Consistent methodology:** Same interpreter, same time of year, same imagery</span>
<span id="cb13-713"><a href="#cb13-713" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Quality control:** Multiple reviewers, consensus protocols</span>
<span id="cb13-714"><a href="#cb13-714" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Temporal alignment:** Labels contemporary with imagery (within months)</span>
<span id="cb13-715"><a href="#cb13-715" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Positional accuracy:** Use high-resolution reference imagery</span>
<span id="cb13-716"><a href="#cb13-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-717"><a href="#cb13-717" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pillar 2: Data Quantity</span></span>
<span id="cb13-718"><a href="#cb13-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-719"><a href="#cb13-719" aria-hidden="true" tabindex="-1"></a>**More data (usually) improves performance**</span>
<span id="cb13-720"><a href="#cb13-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-721"><a href="#cb13-721" aria-hidden="true" tabindex="-1"></a>**But quantity alone isn't enough - quality matters more!**</span>
<span id="cb13-722"><a href="#cb13-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-723"><a href="#cb13-723" aria-hidden="true" tabindex="-1"></a>**How much data do you need?**</span>
<span id="cb13-724"><a href="#cb13-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-725"><a href="#cb13-725" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Algorithm <span class="pp">|</span> Typical Requirements <span class="pp">|</span></span>
<span id="cb13-726"><a href="#cb13-726" aria-hidden="true" tabindex="-1"></a><span class="pp">|-----------|---------------------|</span></span>
<span id="cb13-727"><a href="#cb13-727" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Random Forest <span class="pp">|</span> 100s - 1000s samples per class <span class="pp">|</span></span>
<span id="cb13-728"><a href="#cb13-728" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Simple CNN <span class="pp">|</span> 1000s - 10,000s samples <span class="pp">|</span></span>
<span id="cb13-729"><a href="#cb13-729" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Deep CNN (ResNet) <span class="pp">|</span> 10,000s - 100,000s samples <span class="pp">|</span></span>
<span id="cb13-730"><a href="#cb13-730" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Foundation Models <span class="pp">|</span> Millions - billions samples <span class="pp">|</span></span>
<span id="cb13-731"><a href="#cb13-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-732"><a href="#cb13-732" aria-hidden="true" tabindex="-1"></a>**Strategies when labeled data is limited:**</span>
<span id="cb13-733"><a href="#cb13-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-734"><a href="#cb13-734" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Data Augmentation**</span>
<span id="cb13-735"><a href="#cb13-735" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Rotation, flipping, cropping</span>
<span id="cb13-736"><a href="#cb13-736" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Color jittering (adjust brightness, contrast)</span>
<span id="cb13-737"><a href="#cb13-737" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Adding noise</span>
<span id="cb13-738"><a href="#cb13-738" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Caution:** Ensure augmentations are realistic for EO</span>
<span id="cb13-739"><a href="#cb13-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-740"><a href="#cb13-740" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Transfer Learning**</span>
<span id="cb13-741"><a href="#cb13-741" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Use model pre-trained on large dataset (ImageNet, SatMAE)</span>
<span id="cb13-742"><a href="#cb13-742" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Fine-tune on your small dataset</span>
<span id="cb13-743"><a href="#cb13-743" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Leverages learned features from similar tasks</span>
<span id="cb13-744"><a href="#cb13-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-745"><a href="#cb13-745" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Active Learning**</span>
<span id="cb13-746"><a href="#cb13-746" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Iteratively: train model → find uncertain predictions → label those → retrain</span>
<span id="cb13-747"><a href="#cb13-747" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Efficiently focuses labeling effort where it matters most</span>
<span id="cb13-748"><a href="#cb13-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-749"><a href="#cb13-749" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Synthetic Data**</span>
<span id="cb13-750"><a href="#cb13-750" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Generate training data via simulation</span>
<span id="cb13-751"><a href="#cb13-751" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Example: Simulated SAR scenes for flood detection</span>
<span id="cb13-752"><a href="#cb13-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-753"><a href="#cb13-753" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb13-754"><a href="#cb13-754" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2025 Research: Data Efficiency</span></span>
<span id="cb13-755"><a href="#cb13-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-756"><a href="#cb13-756" aria-hidden="true" tabindex="-1"></a>Recent studies show:</span>
<span id="cb13-757"><a href="#cb13-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-758"><a href="#cb13-758" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Some EO tasks reach optimal accuracy with **&lt;20% of temporal instances**</span>
<span id="cb13-759"><a href="#cb13-759" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Single band** from single sensor can be sufficient for specific tasks</span>
<span id="cb13-760"><a href="#cb13-760" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Implication:** Smart data selection &gt; brute force data collection</span>
<span id="cb13-761"><a href="#cb13-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-762"><a href="#cb13-762" aria-hidden="true" tabindex="-1"></a>**Source:** "Data-Centric Machine Learning for Earth Observation: Necessary and Sufficient Features" (arXiv 2024)</span>
<span id="cb13-763"><a href="#cb13-763" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-764"><a href="#cb13-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-765"><a href="#cb13-765" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pillar 3: Data Diversity</span></span>
<span id="cb13-766"><a href="#cb13-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-767"><a href="#cb13-767" aria-hidden="true" tabindex="-1"></a>**Representative data covers the full range of scenarios the model will encounter**</span>
<span id="cb13-768"><a href="#cb13-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-769"><a href="#cb13-769" aria-hidden="true" tabindex="-1"></a>**Dimensions of diversity:**</span>
<span id="cb13-770"><a href="#cb13-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-771"><a href="#cb13-771" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Geographic diversity**</span>
<span id="cb13-772"><a href="#cb13-772" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different regions (Luzon, Visayas, Mindanao)</span>
<span id="cb13-773"><a href="#cb13-773" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different ecosystems (lowland, highland, coastal)</span>
<span id="cb13-774"><a href="#cb13-774" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different climate zones</span>
<span id="cb13-775"><a href="#cb13-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-776"><a href="#cb13-776" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Temporal diversity**</span>
<span id="cb13-777"><a href="#cb13-777" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different seasons (wet, dry)</span>
<span id="cb13-778"><a href="#cb13-778" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different years (inter-annual variability)</span>
<span id="cb13-779"><a href="#cb13-779" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different phenological stages (planting, growing, harvest)</span>
<span id="cb13-780"><a href="#cb13-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-781"><a href="#cb13-781" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Class diversity**</span>
<span id="cb13-782"><a href="#cb13-782" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Multiple examples per class</span>
<span id="cb13-783"><a href="#cb13-783" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Edge cases and rare types</span>
<span id="cb13-784"><a href="#cb13-784" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Transitional zones</span>
<span id="cb13-785"><a href="#cb13-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-786"><a href="#cb13-786" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Sensor diversity**</span>
<span id="cb13-787"><a href="#cb13-787" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different satellites (Sentinel-2A, 2B, 2C)</span>
<span id="cb13-788"><a href="#cb13-788" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different atmospheric conditions</span>
<span id="cb13-789"><a href="#cb13-789" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different viewing angles</span>
<span id="cb13-790"><a href="#cb13-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-791"><a href="#cb13-791" aria-hidden="true" tabindex="-1"></a>**Example: Urban classification**</span>
<span id="cb13-792"><a href="#cb13-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-793"><a href="#cb13-793" aria-hidden="true" tabindex="-1"></a>**Poor diversity:** All training samples from Metro Manila CBD</span>
<span id="cb13-794"><a href="#cb13-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-795"><a href="#cb13-795" aria-hidden="true" tabindex="-1"></a>**Result:** Model fails on:</span>
<span id="cb13-796"><a href="#cb13-796" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Small provincial towns (different building density)</span>
<span id="cb13-797"><a href="#cb13-797" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Informal settlements (different materials)</span>
<span id="cb13-798"><a href="#cb13-798" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Peri-urban areas (mixed land cover)</span>
<span id="cb13-799"><a href="#cb13-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-800"><a href="#cb13-800" aria-hidden="true" tabindex="-1"></a>**Good diversity:** Samples from:</span>
<span id="cb13-801"><a href="#cb13-801" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Large cities (Manila, Cebu, Davao)</span>
<span id="cb13-802"><a href="#cb13-802" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Medium towns (Baguio, Iloilo, Cagayan de Oro)</span>
<span id="cb13-803"><a href="#cb13-803" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Small municipalities</span>
<span id="cb13-804"><a href="#cb13-804" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Different building materials (concrete, metal roofing, nipa huts)</span>
<span id="cb13-805"><a href="#cb13-805" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Different periods (to capture growth)</span>
<span id="cb13-806"><a href="#cb13-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-807"><a href="#cb13-807" aria-hidden="true" tabindex="-1"></a>**Result:** Model generalizes well across Philippines</span>
<span id="cb13-808"><a href="#cb13-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-809"><a href="#cb13-809" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pillar 4: Annotation Strategy</span></span>
<span id="cb13-810"><a href="#cb13-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-811"><a href="#cb13-811" aria-hidden="true" tabindex="-1"></a>**How you label data profoundly impacts model performance**</span>
<span id="cb13-812"><a href="#cb13-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-813"><a href="#cb13-813" aria-hidden="true" tabindex="-1"></a>**Annotation approaches:**</span>
<span id="cb13-814"><a href="#cb13-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-815"><a href="#cb13-815" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Point sampling:** Fast, but limited context</span>
<span id="cb13-816"><a href="#cb13-816" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Polygon delineation:** More information, more time-consuming</span>
<span id="cb13-817"><a href="#cb13-817" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Pixel-level labeling:** Maximum detail, required for segmentation</span>
<span id="cb13-818"><a href="#cb13-818" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Image-level labels:** Easiest, suitable for scene classification</span>
<span id="cb13-819"><a href="#cb13-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-820"><a href="#cb13-820" aria-hidden="true" tabindex="-1"></a>**Best practices:**</span>
<span id="cb13-821"><a href="#cb13-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-822"><a href="#cb13-822" aria-hidden="true" tabindex="-1"></a>**1. Expert involvement**</span>
<span id="cb13-823"><a href="#cb13-823" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use domain experts for complex classes (forest types, crop stages)</span>
<span id="cb13-824"><a href="#cb13-824" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Train labelers thoroughly on class definitions</span>
<span id="cb13-825"><a href="#cb13-825" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Regular calibration sessions</span>
<span id="cb13-826"><a href="#cb13-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-827"><a href="#cb13-827" aria-hidden="true" tabindex="-1"></a>**2. Quality over quantity**</span>
<span id="cb13-828"><a href="#cb13-828" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>500 high-quality labels &gt; 5000 noisy labels</span>
<span id="cb13-829"><a href="#cb13-829" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Invest in review and correction</span>
<span id="cb13-830"><a href="#cb13-830" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Document difficult cases</span>
<span id="cb13-831"><a href="#cb13-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-832"><a href="#cb13-832" aria-hidden="true" tabindex="-1"></a>**3. Class balance**</span>
<span id="cb13-833"><a href="#cb13-833" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Ensure adequate representation of minority classes</span>
<span id="cb13-834"><a href="#cb13-834" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stratified sampling by class</span>
<span id="cb13-835"><a href="#cb13-835" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Consider class weights in training if imbalanced</span>
<span id="cb13-836"><a href="#cb13-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-837"><a href="#cb13-837" aria-hidden="true" tabindex="-1"></a>**4. Consensus protocols**</span>
<span id="cb13-838"><a href="#cb13-838" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multiple labelers per sample</span>
<span id="cb13-839"><a href="#cb13-839" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Majority vote or adjudication for disagreements</span>
<span id="cb13-840"><a href="#cb13-840" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Measure inter-annotator agreement</span>
<span id="cb13-841"><a href="#cb13-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-842"><a href="#cb13-842" aria-hidden="true" tabindex="-1"></a>**5. Iterative refinement**</span>
<span id="cb13-843"><a href="#cb13-843" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use model predictions to find label errors</span>
<span id="cb13-844"><a href="#cb13-844" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Retrain after improving labels</span>
<span id="cb13-845"><a href="#cb13-845" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Focus effort on low-confidence predictions</span>
<span id="cb13-846"><a href="#cb13-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-847"><a href="#cb13-847" aria-hidden="true" tabindex="-1"></a>::: {.philippine-context}</span>
<span id="cb13-848"><a href="#cb13-848" aria-hidden="true" tabindex="-1"></a>**Philippine Solution: ALaM Project**</span>
<span id="cb13-849"><a href="#cb13-849" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-850"><a href="#cb13-850" aria-hidden="true" tabindex="-1"></a>DOST-ASTI's **Automated Labeling Machine (ALaM)** addresses annotation bottleneck:</span>
<span id="cb13-851"><a href="#cb13-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-852"><a href="#cb13-852" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Combines automated labeling with crowdsourcing</span>
<span id="cb13-853"><a href="#cb13-853" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Human-in-the-loop quality control</span>
<span id="cb13-854"><a href="#cb13-854" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Integration with DIMER model repository</span>
<span id="cb13-855"><a href="#cb13-855" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reduces labeling time and cost significantly</span>
<span id="cb13-856"><a href="#cb13-856" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-857"><a href="#cb13-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-858"><a href="#cb13-858" aria-hidden="true" tabindex="-1"></a><span class="fu">### 2025 Examples: Data-Centric Success Stories</span></span>
<span id="cb13-859"><a href="#cb13-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-860"><a href="#cb13-860" aria-hidden="true" tabindex="-1"></a><span class="fu">#### NASA-IBM Geospatial Foundation Model</span></span>
<span id="cb13-861"><a href="#cb13-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-862"><a href="#cb13-862" aria-hidden="true" tabindex="-1"></a>**Open-source model trained on massive HLS dataset (Harmonized Landsat-Sentinel-2)**</span>
<span id="cb13-863"><a href="#cb13-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-864"><a href="#cb13-864" aria-hidden="true" tabindex="-1"></a>**Data-centric approach:**</span>
<span id="cb13-865"><a href="#cb13-865" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Millions of satellite images</span>
<span id="cb13-866"><a href="#cb13-866" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Self-supervised pre-training (no labels needed)</span>
<span id="cb13-867"><a href="#cb13-867" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fine-tuned for specific tasks with small labeled datasets</span>
<span id="cb13-868"><a href="#cb13-868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-869"><a href="#cb13-869" aria-hidden="true" tabindex="-1"></a>**Result:**</span>
<span id="cb13-870"><a href="#cb13-870" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>State-of-the-art performance on multiple EO tasks</span>
<span id="cb13-871"><a href="#cb13-871" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reduces labeled data requirements by 10-100x</span>
<span id="cb13-872"><a href="#cb13-872" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Democratizes access to powerful EO AI</span>
<span id="cb13-873"><a href="#cb13-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-874"><a href="#cb13-874" aria-hidden="true" tabindex="-1"></a><span class="fu">#### ESA Φsat-2 On-Board AI</span></span>
<span id="cb13-875"><a href="#cb13-875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-876"><a href="#cb13-876" aria-hidden="true" tabindex="-1"></a>**Launched 2025: 22cm CubeSat with on-board AI processing**</span>
<span id="cb13-877"><a href="#cb13-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-878"><a href="#cb13-878" aria-hidden="true" tabindex="-1"></a>**Data-centric innovation:**</span>
<span id="cb13-879"><a href="#cb13-879" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Processes imagery directly on satellite</span>
<span id="cb13-880"><a href="#cb13-880" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Only transmits actionable information (not raw data)</span>
<span id="cb13-881"><a href="#cb13-881" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reduces bandwidth requirements</span>
<span id="cb13-882"><a href="#cb13-882" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Enables real-time event detection (fires, ships, clouds)</span>
<span id="cb13-883"><a href="#cb13-883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-884"><a href="#cb13-884" aria-hidden="true" tabindex="-1"></a>**Implication:** Data quality selection happens in space!</span>
<span id="cb13-885"><a href="#cb13-885" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-886"><a href="#cb13-886" aria-hidden="true" tabindex="-1"></a><span class="fu">#### EarthDaily Constellation</span></span>
<span id="cb13-887"><a href="#cb13-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-888"><a href="#cb13-888" aria-hidden="true" tabindex="-1"></a>**10-satellite constellation for daily global coverage**</span>
<span id="cb13-889"><a href="#cb13-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-890"><a href="#cb13-890" aria-hidden="true" tabindex="-1"></a>**Focus on AI-ready data:**</span>
<span id="cb13-891"><a href="#cb13-891" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Scientific-grade calibration</span>
<span id="cb13-892"><a href="#cb13-892" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Consistent, reliable acquisitions</span>
<span id="cb13-893"><a href="#cb13-893" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Optimized spectral bands for ML</span>
<span id="cb13-894"><a href="#cb13-894" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Emphasis on data quality for algorithm performance</span>
<span id="cb13-895"><a href="#cb13-895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-896"><a href="#cb13-896" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb13-897"><a href="#cb13-897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-898"><a href="#cb13-898" aria-hidden="true" tabindex="-1"></a><span class="fu">## Key Takeaways</span></span>
<span id="cb13-899"><a href="#cb13-899" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-900"><a href="#cb13-900" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb13-901"><a href="#cb13-901" aria-hidden="true" tabindex="-1"></a><span class="fu">## Session 2 Summary</span></span>
<span id="cb13-902"><a href="#cb13-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-903"><a href="#cb13-903" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**AI/ML learns patterns from data** rather than explicit programming</span>
<span id="cb13-904"><a href="#cb13-904" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**The EO workflow** spans problem definition → data → preprocessing → features → training → validation → deployment</span>
<span id="cb13-905"><a href="#cb13-905" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Supervised learning** (classification &amp; regression) is dominant for EO because we need specific outputs</span>
<span id="cb13-906"><a href="#cb13-906" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Unsupervised learning** (clustering) is useful for exploration but requires interpretation</span>
<span id="cb13-907"><a href="#cb13-907" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Neural networks** are composed of layers of neurons using activation functions, optimized via loss minimization</span>
<span id="cb13-908"><a href="#cb13-908" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Deep learning** automatically extracts hierarchical features - dominant for image analysis</span>
<span id="cb13-909"><a href="#cb13-909" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>**Data-centric AI (2025 paradigm):** Improving data quality, quantity, diversity, and annotation beats tweaking models</span>
<span id="cb13-910"><a href="#cb13-910" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span>**For Philippine EO:** Leverage DOST-ASTI tools (DIMER, AIPI, ALaM) to operationalize data-centric approaches</span>
<span id="cb13-911"><a href="#cb13-911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-912"><a href="#cb13-912" aria-hidden="true" tabindex="-1"></a>**Next steps:** Hands-on Python for geospatial data (Session 3) and Google Earth Engine (Session 4) to put these concepts into practice!</span>
<span id="cb13-913"><a href="#cb13-913" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-914"><a href="#cb13-914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-915"><a href="#cb13-915" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb13-916"><a href="#cb13-916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-917"><a href="#cb13-917" aria-hidden="true" tabindex="-1"></a><span class="fu">## Discussion Questions</span></span>
<span id="cb13-918"><a href="#cb13-918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-919"><a href="#cb13-919" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb13-920"><a href="#cb13-920" aria-hidden="true" tabindex="-1"></a><span class="fu">## Reflect &amp; Discuss</span></span>
<span id="cb13-921"><a href="#cb13-921" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-922"><a href="#cb13-922" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**What EO problem in your work** could benefit from ML? Is it classification or regression?</span>
<span id="cb13-923"><a href="#cb13-923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-924"><a href="#cb13-924" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**What data quality issues** have you encountered with Philippine satellite data?</span>
<span id="cb13-925"><a href="#cb13-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-926"><a href="#cb13-926" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**How would you ensure diversity** in training data for a national-scale land cover map?</span>
<span id="cb13-927"><a href="#cb13-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-928"><a href="#cb13-928" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Which Philippine datasets** (PhilSA, NAMRIA, PAGASA) could complement satellite imagery for your ML project?</span>
<span id="cb13-929"><a href="#cb13-929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-930"><a href="#cb13-930" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**How might DOST-ASTI's DIMER and AIPI platforms** reduce barriers to deploying ML in your organization?</span>
<span id="cb13-931"><a href="#cb13-931" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-932"><a href="#cb13-932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-933"><a href="#cb13-933" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb13-934"><a href="#cb13-934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-935"><a href="#cb13-935" aria-hidden="true" tabindex="-1"></a><span class="fu">## Further Reading</span></span>
<span id="cb13-936"><a href="#cb13-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-937"><a href="#cb13-937" aria-hidden="true" tabindex="-1"></a><span class="fu">### Foundational Concepts</span></span>
<span id="cb13-938"><a href="#cb13-938" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">NASA ARSET: Fundamentals of Machine Learning for Earth Science</span><span class="co">](https://appliedsciences.nasa.gov/get-involved/training/english/arset-fundamentals-machine-learning-earth-science)</span></span>
<span id="cb13-939"><a href="#cb13-939" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Data-Centric AI: Better, Not Just More</span><span class="co">](https://arxiv.org/abs/2312.05327)</span></span>
<span id="cb13-940"><a href="#cb13-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-941"><a href="#cb13-941" aria-hidden="true" tabindex="-1"></a><span class="fu">### Neural Networks</span></span>
<span id="cb13-942"><a href="#cb13-942" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Deep Learning Book (Goodfellow et al.)</span><span class="co">](https://www.deeplearningbook.org/)</span> - Free online</span>
<span id="cb13-943"><a href="#cb13-943" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Neural Networks and Deep Learning (Nielsen)</span><span class="co">](http://neuralnetworksanddeeplearning.com/)</span> - Interactive tutorial</span>
<span id="cb13-944"><a href="#cb13-944" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-945"><a href="#cb13-945" aria-hidden="true" tabindex="-1"></a><span class="fu">### EO-Specific ML</span></span>
<span id="cb13-946"><a href="#cb13-946" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">EO College: Introduction to Machine Learning for Earth Observation</span><span class="co">](https://eo-college.org/courses/introduction-to-machine-learning-for-earth-observation/)</span></span>
<span id="cb13-947"><a href="#cb13-947" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">ML4Earth Resources</span><span class="co">](https://ml4earth.de/)</span></span>
<span id="cb13-948"><a href="#cb13-948" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Climate Change AI: Earth Observation &amp; Monitoring</span><span class="co">](https://www.climatechange.ai/subject_areas/earth_observation_monitoring)</span></span>
<span id="cb13-949"><a href="#cb13-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-950"><a href="#cb13-950" aria-hidden="true" tabindex="-1"></a><span class="fu">### Philippine AI Initiatives</span></span>
<span id="cb13-951"><a href="#cb13-951" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">DOST-ASTI SkAI-Pinas</span><span class="co">](https://asti.dost.gov.ph/)</span></span>
<span id="cb13-952"><a href="#cb13-952" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">DIMER Model Hub</span><span class="co">](https://asti.dost.gov.ph/news-articles/asti-leads-ph-ai-revo-with-dimer-model-hub/)</span></span>
<span id="cb13-953"><a href="#cb13-953" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-954"><a href="#cb13-954" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb13-955"><a href="#cb13-955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-956"><a href="#cb13-956" aria-hidden="true" tabindex="-1"></a>::: {.session-nav}</span>
<span id="cb13-957"><a href="#cb13-957" aria-hidden="true" tabindex="-1"></a>::: {.session-nav-link href="session1.qmd"}</span>
<span id="cb13-958"><a href="#cb13-958" aria-hidden="true" tabindex="-1"></a>::: {.session-nav-label}</span>
<span id="cb13-959"><a href="#cb13-959" aria-hidden="true" tabindex="-1"></a>← Previous</span>
<span id="cb13-960"><a href="#cb13-960" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-961"><a href="#cb13-961" aria-hidden="true" tabindex="-1"></a>::: {.session-nav-title}</span>
<span id="cb13-962"><a href="#cb13-962" aria-hidden="true" tabindex="-1"></a>Session 1: Copernicus &amp; Philippine EO</span>
<span id="cb13-963"><a href="#cb13-963" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-964"><a href="#cb13-964" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-965"><a href="#cb13-965" aria-hidden="true" tabindex="-1"></a>::: {.session-nav-link href="session3.qmd"}</span>
<span id="cb13-966"><a href="#cb13-966" aria-hidden="true" tabindex="-1"></a>::: {.session-nav-label}</span>
<span id="cb13-967"><a href="#cb13-967" aria-hidden="true" tabindex="-1"></a>Next Session</span>
<span id="cb13-968"><a href="#cb13-968" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-969"><a href="#cb13-969" aria-hidden="true" tabindex="-1"></a>::: {.session-nav-title}</span>
<span id="cb13-970"><a href="#cb13-970" aria-hidden="true" tabindex="-1"></a>Session 3: Python for Geospatial Data →</span>
<span id="cb13-971"><a href="#cb13-971" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-972"><a href="#cb13-972" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-973"><a href="#cb13-973" aria-hidden="true" tabindex="-1"></a>:::</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>CopPhil EO AI/ML Training Programme</p>
</div>   
    <div class="nav-footer-center">
<p>Funded by the European Union - Global Gateway Initiative</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:info@philsa.gov.ph">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://philsa.gov.ph">
      <i class="bi bi-globe" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>