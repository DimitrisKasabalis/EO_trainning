[
  {
    "objectID": "notebooks/Day1_Session3_Python_Geospatial_Data.html",
    "href": "notebooks/Day1_Session3_Python_Geospatial_Data.html",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "",
    "text": "Open In Colab\n\n\nEU-Philippines Copernicus Capacity Support Programme"
  },
  {
    "objectID": "notebooks/Day1_Session3_Python_Geospatial_Data.html#copphil-4-day-advanced-training-on-aiml-for-earth-observation",
    "href": "notebooks/Day1_Session3_Python_Geospatial_Data.html#copphil-4-day-advanced-training-on-aiml-for-earth-observation",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "",
    "text": "Open In Colab\n\n\nEU-Philippines Copernicus Capacity Support Programme"
  },
  {
    "objectID": "notebooks/Day1_Session3_Python_Geospatial_Data.html#learning-objectives",
    "href": "notebooks/Day1_Session3_Python_Geospatial_Data.html#learning-objectives",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this session, you will be able to:\n\nSet up a Python geospatial environment in Google Colab\nLoad, inspect, and visualize vector data using GeoPandas\nLoad, inspect, and visualize raster data using Rasterio\nPerform basic geospatial operations (filtering, clipping, cropping)\nCalculate vegetation indices (NDVI, NDWI) from Sentinel-2 imagery\nCombine vector and raster data for integrated analysis\nApply these skills to Philippine EO applications (DRR, CCA, NRM)"
  },
  {
    "objectID": "notebooks/Day1_Session3_Python_Geospatial_Data.html#why-this-session-matters",
    "href": "notebooks/Day1_Session3_Python_Geospatial_Data.html#why-this-session-matters",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Why This Session Matters",
    "text": "Why This Session Matters\nPython geospatial skills are the foundation of ALL AI/ML workflows in Earth Observation.\nYou cannot: - Train a model without loading training data ✗ - Preprocess satellite images without raster operations ✗ - Validate results without vector boundaries ✗ - Deploy solutions without understanding data formats ✗\nThis session gives you the superpowers to: - Handle Sentinel-2 imagery like a pro ✓ - Work with Philippine administrative boundaries ✓ - Prepare analysis-ready datasets ✓ - Build production-ready EO applications ✓"
  },
  {
    "objectID": "notebooks/Day1_Session3_Python_Geospatial_Data.html#prerequisites",
    "href": "notebooks/Day1_Session3_Python_Geospatial_Data.html#prerequisites",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nBasic Python knowledge (variables, loops, functions)\nGoogle account for Colab access\nCompletion of Sessions 1-2 (Copernicus overview, AI/ML concepts)"
  },
  {
    "objectID": "notebooks/Day1_Session3_Python_Geospatial_Data.html#session-structure",
    "href": "notebooks/Day1_Session3_Python_Geospatial_Data.html#session-structure",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Session Structure",
    "text": "Session Structure\nPart 1: Environment Setup (10 min) Part 2: Python Basics Recap (10 min) Part 3: GeoPandas for Vector Data (40 min) Part 4: Rasterio for Raster Data (50 min) Part 5: Combined Operations (30 min)\nTotal: ~2 hours with exercises"
  },
  {
    "objectID": "notebooks/Day1_Session3_Python_Geospatial_Data.html#part-1-environment-setup",
    "href": "notebooks/Day1_Session3_Python_Geospatial_Data.html#part-1-environment-setup",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Part 1: Environment Setup",
    "text": "Part 1: Environment Setup\n\n1.1 Mount Google Drive\nWe’ll use Google Drive to: - Access sample datasets - Save outputs and results - Share data between sessions\n\n# Mount Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Create working directory\nimport os\nwork_dir = '/content/drive/MyDrive/CopPhil_Training'\nos.makedirs(work_dir, exist_ok=True)\nos.makedirs(f'{work_dir}/outputs', exist_ok=True)\n\nprint(f\"✓ Google Drive mounted successfully!\")\nprint(f\"✓ Working directory: {work_dir}\")\n\n\n\n1.2 Install Required Packages\nCore geospatial libraries: - geopandas - Vector data (shapefiles, GeoJSON) - rasterio - Raster data (GeoTIFF, satellite imagery) - shapely - Geometric operations - pyproj - Coordinate reference systems\nInstallation time: 1-2 minutes\n\n# Install geospatial libraries (suppress output for cleaner notebook)\n!pip install geopandas rasterio shapely pyproj matplotlib contextily -q\n\nprint(\"✓ All packages installed successfully!\")\n\n\n\n1.3 Import Libraries and Verify Installation\n\n# Core scientific libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\n\n# Geospatial libraries\nimport geopandas as gpd\nimport rasterio\nfrom rasterio.plot import show\nfrom rasterio.mask import mask\nfrom rasterio.warp import calculate_default_transform, reproject, Resampling\nfrom shapely.geometry import Point, Polygon, box\nimport warnings\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# Set visualization defaults for professional-looking plots\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['figure.dpi'] = 100\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 11\nplt.rcParams['axes.titlesize'] = 13\nplt.rcParams['xtick.labelsize'] = 9\nplt.rcParams['ytick.labelsize'] = 9\nplt.rcParams['legend.fontsize'] = 10\n\n# Print versions\nprint(\"✓ All libraries imported successfully!\\n\")\nprint(\"Library Versions:\")\nprint(f\"  • NumPy: {np.__version__}\")\nprint(f\"  • Pandas: {pd.__version__}\")\nprint(f\"  • GeoPandas: {gpd.__version__}\")\nprint(f\"  • Rasterio: {rasterio.__version__}\")\nprint(f\"  • Matplotlib: {plt.matplotlib.__version__}\")\nprint(\"\\n\" + \"=\"*60)"
  },
  {
    "objectID": "notebooks/Day1_Session3_Python_Geospatial_Data.html#part-2-python-basics-quick-recap",
    "href": "notebooks/Day1_Session3_Python_Geospatial_Data.html#part-2-python-basics-quick-recap",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Part 2: Python Basics Quick Recap",
    "text": "Part 2: Python Basics Quick Recap\nBefore diving into geospatial operations, let’s review Python fundamentals you’ll encounter throughout this notebook.\nIf you’re comfortable with Python, feel free to skim this section.\n\n2.1 Data Types and Structures\n\n# Strings - text data\nprovince_name = \"Palawan\"\nregion = \"MIMAROPA\"\n\n# Numbers - integers and floats\npopulation = 1200000  # integer\narea_km2 = 14649.73   # float (decimal)\n\n# Lists - ordered collections (can be modified)\nphilippine_islands = [\"Luzon\", \"Visayas\", \"Mindanao\"]\nband_numbers = [2, 3, 4, 8]  # Sentinel-2 bands\n\n# Dictionaries - key-value pairs\nprovince_data = {\n    \"name\": \"Palawan\",\n    \"capital\": \"Puerto Princesa\",\n    \"population\": 1200000,\n    \"area_km2\": 14649.73,\n    \"coordinates\": [118.73, 9.85]\n}\n\n# Accessing data\nprint(f\"Province: {province_name}\")\nprint(f\"First island: {philippine_islands[0]}\")\nprint(f\"Capital: {province_data['capital']}\")\nprint(f\"Population density: {population / area_km2:.1f} people/km²\")\n\n\n\n2.2 Control Structures - Loops and Conditionals\n\n# For loops - iterate over collections\nprint(\"Philippine Island Groups:\")\nfor island in philippine_islands:\n    print(f\"  • {island}\")\n\n# If-elif-else - conditional execution\nndvi_value = 0.65\n\nif ndvi_value &lt; 0:\n    vegetation_class = \"Water/Bare soil\"\nelif ndvi_value &lt; 0.2:\n    vegetation_class = \"Sparse vegetation\"\nelif ndvi_value &lt; 0.5:\n    vegetation_class = \"Moderate vegetation\"\nelse:\n    vegetation_class = \"Dense vegetation\"\n\nprint(f\"\\nNDVI = {ndvi_value} → {vegetation_class}\")\n\n# List comprehension - compact way to create lists\nband_names = [f\"Band_{b}\" for b in band_numbers]\nprint(f\"\\nBand names: {band_names}\")\n\n\n\n2.3 Functions - Reusable Code Blocks\n\ndef calculate_ndvi(nir, red):\n    \"\"\"\n    Calculate Normalized Difference Vegetation Index.\n    \n    NDVI = (NIR - Red) / (NIR + Red)\n    \n    Parameters:\n    -----------\n    nir : array-like\n        Near-infrared band values\n    red : array-like\n        Red band values\n    \n    Returns:\n    --------\n    ndvi : array-like\n        NDVI values (-1 to 1)\n    \"\"\"\n    # Convert to float to avoid integer division\n    nir = nir.astype(float)\n    red = red.astype(float)\n    \n    # Calculate NDVI, handling division by zero\n    denominator = nir + red\n    ndvi = np.where(denominator != 0, (nir - red) / denominator, 0)\n    \n    return ndvi\n\n# Test the function\nnir_test = np.array([5000, 3000, 1000])\nred_test = np.array([1500, 1200, 900])\nresult = calculate_ndvi(nir_test, red_test)\n\nprint(\"NDVI Calculation Test:\")\nfor i in range(len(result)):\n    print(f\"  NIR={nir_test[i]}, Red={red_test[i]} → NDVI={result[i]:.3f}\")"
  },
  {
    "objectID": "notebooks/Day1_Session3_Python_Geospatial_Data.html#part-3-geopandas-for-vector-data",
    "href": "notebooks/Day1_Session3_Python_Geospatial_Data.html#part-3-geopandas-for-vector-data",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Part 3: GeoPandas for Vector Data",
    "text": "Part 3: GeoPandas for Vector Data\nGeoPandas extends pandas for geospatial vector data (points, lines, polygons).\n\nWhy GeoPandas?\n\n✓ Read/write multiple formats (Shapefile, GeoJSON, KML, etc.)\n✓ Spatial operations (intersection, buffer, union)\n✓ Coordinate reference system (CRS) transformations\n✓ Easy visualization\n✓ Integration with pandas (filtering, grouping, etc.)\n\n\n\n3.1 Creating Sample Philippine Administrative Data\nNote: In production, you would load actual shapefiles from sources like: - NAMRIA Geoportal: https://www.geoportal.gov.ph/ - HDX Philippines: https://data.humdata.org/group/phl - PhilSA: https://philsa.gov.ph/\n\n# Create sample Philippine province polygons for demonstration\n# In practice, load from: gdf = gpd.read_file('philippines_provinces.shp')\n\nprovinces_data = [\n    # (name, region, island_group, population, minx, miny, maxx, maxy)\n    (\"Palawan\", \"MIMAROPA\", \"Luzon\", 1200000, 117.5, 8.5, 119.5, 11.5),\n    (\"Metro Manila\", \"NCR\", \"Luzon\", 13000000, 120.8, 14.4, 121.2, 14.8),\n    (\"Cebu\", \"Central Visayas\", \"Visayas\", 5200000, 123.2, 9.5, 124.0, 11.3),\n    (\"Davao del Sur\", \"Davao Region\", \"Mindanao\", 700000, 125.0, 6.3, 125.7, 7.2),\n    (\"Iloilo\", \"Western Visayas\", \"Visayas\", 2150000, 122.3, 10.4, 123.2, 11.6),\n    (\"Cagayan\", \"Cagayan Valley\", \"Luzon\", 1280000, 121.2, 17.4, 122.3, 18.6),\n]\n\n# Create geometries and build GeoDataFrame\ngeometries = []\nattributes = []\n\nfor name, region, island, pop, minx, miny, maxx, maxy in provinces_data:\n    # Create bounding box polygon\n    geom = box(minx, miny, maxx, maxy)\n    geometries.append(geom)\n    \n    # Calculate area and density\n    area = geom.area * 111 * 111  # Rough conversion to km² at Philippines latitude\n    density = pop / area\n    \n    attributes.append({\n        'Province': name,\n        'Region': region,\n        'Island_Group': island,\n        'Population': pop,\n        'Area_km2': area,\n        'Density': density\n    })\n\n# Create GeoDataFrame\nphilippines_gdf = gpd.GeoDataFrame(\n    attributes,\n    geometry=geometries,\n    crs='EPSG:4326'  # WGS84 geographic coordinates\n)\n\nprint(\"✓ Philippine provinces GeoDataFrame created!\")\nprint(f\"  Provinces: {len(philippines_gdf)}\")\nprint(f\"  CRS: {philippines_gdf.crs.name}\")\n\n\n\n3.2 Inspecting the GeoDataFrame\n\n# Display first few rows\nprint(\"First 3 provinces:\")\ndisplay(philippines_gdf.head(3))\n\n# Check data types\nprint(\"\\nColumn data types:\")\nprint(philippines_gdf.dtypes)\n\n# Summary statistics\nprint(\"\\nSummary statistics:\")\ndisplay(philippines_gdf[['Population', 'Area_km2', 'Density']].describe())\n\n\n# Coordinate Reference System (CRS) information\nprint(\"CRS Details:\")\nprint(f\"  Name: {philippines_gdf.crs.name}\")\nprint(f\"  EPSG Code: {philippines_gdf.crs.to_epsg()}\")\nprint(f\"  Type: {philippines_gdf.crs.type_name}\")\nprint(f\"  Units: {philippines_gdf.crs.axis_info[0].unit_name}\")\n\n# Bounds (extent)\nbounds = philippines_gdf.total_bounds\nprint(f\"\\nGeographic Extent:\")\nprint(f\"  Min Longitude: {bounds[0]:.2f}°\")\nprint(f\"  Min Latitude:  {bounds[1]:.2f}°\")\nprint(f\"  Max Longitude: {bounds[2]:.2f}°\")\nprint(f\"  Max Latitude:  {bounds[3]:.2f}°\")\n\n\n\n3.3 Filtering and Querying Vector Data\n\n# Filter by attribute: Select provinces in Mindanao\nmindanao = philippines_gdf[philippines_gdf['Island_Group'] == 'Mindanao']\nprint(\"Mindanao Provinces:\")\nprint(mindanao[['Province', 'Population', 'Area_km2']])\n\n# Filter by condition: High-density provinces\nhigh_density = philippines_gdf[philippines_gdf['Density'] &gt; 1000]\nprint(\"\\nHigh Density Provinces (&gt;1000 people/km²):\")\nprint(high_density[['Province', 'Density']].sort_values('Density', ascending=False))\n\n# Multiple conditions: Large AND populous\nmajor_provinces = philippines_gdf[\n    (philippines_gdf['Population'] &gt; 1000000) & \n    (philippines_gdf['Area_km2'] &gt; 5000)\n]\nprint(\"\\nMajor Provinces (&gt;1M pop AND &gt;5000 km²):\")\nprint(major_provinces[['Province', 'Population', 'Area_km2']])\n\n\n\n3.4 Spatial Operations\n\n# Calculate centroids\nphilippines_gdf['centroid'] = philippines_gdf.geometry.centroid\n\n# Calculate area in km² (more accurate than bounding box)\n# For accurate area, project to equal-area CRS\nphilippines_utm = philippines_gdf.to_crs('EPSG:32651')  # UTM Zone 51N\nphilippines_gdf['Area_km2_precise'] = philippines_utm.geometry.area / 1e6\n\nprint(\"Centroid coordinates:\")\nfor idx, row in philippines_gdf.iterrows():\n    print(f\"  {row['Province']:&lt;20} ({row['centroid'].x:.3f}, {row['centroid'].y:.3f})\")\n\n# Create buffer around Metro Manila (50km)\nmanila = philippines_gdf[philippines_gdf['Province'] == 'Metro Manila']\nmanila_utm = manila.to_crs('EPSG:32651')  # Project to UTM for accurate buffering\nmanila_buffer = manila_utm.buffer(50000)  # 50km buffer in meters\nmanila_buffer = manila_buffer.to_crs('EPSG:4326')  # Back to geographic\n\nprint(f\"\\n✓ Created 50km buffer around Metro Manila\")\nprint(f\"  Original area: {manila.geometry.area.values[0] * 111 * 111:.0f} km²\")\nprint(f\"  Buffer area: {manila_buffer.area.values[0] * 111 * 111:.0f} km²\")\n\n\n\n3.5 Visualizing Vector Data\n\n# Simple plot - all provinces\nfig, ax = plt.subplots(figsize=(12, 10))\n\nphilippines_gdf.plot(\n    ax=ax,\n    color='lightblue',\n    edgecolor='darkblue',\n    linewidth=1.5,\n    alpha=0.6\n)\n\n# Add province labels\nfor idx, row in philippines_gdf.iterrows():\n    ax.annotate(\n        text=row['Province'],\n        xy=(row['centroid'].x, row['centroid'].y),\n        ha='center',\n        fontsize=8,\n        fontweight='bold'\n    )\n\nax.set_title('Sample Philippine Provinces', fontsize=14, fontweight='bold', pad=20)\nax.set_xlabel('Longitude (°E)', fontsize=11)\nax.set_ylabel('Latitude (°N)', fontsize=11)\nax.grid(True, linestyle='--', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n# Choropleth map - color by population\nfig, ax = plt.subplots(figsize=(12, 10))\n\nphilippines_gdf.plot(\n    ax=ax,\n    column='Population',\n    cmap='YlOrRd',\n    edgecolor='black',\n    linewidth=1,\n    legend=True,\n    legend_kwds={\n        'label': 'Population',\n        'orientation': 'horizontal',\n        'shrink': 0.8\n    }\n)\n\nax.set_title('Philippine Provinces by Population', fontsize=14, fontweight='bold', pad=20)\nax.set_xlabel('Longitude (°E)', fontsize=11)\nax.set_ylabel('Latitude (°N)', fontsize=11)\nax.grid(True, linestyle='--', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n# Categorical map - color by island group\nfig, ax = plt.subplots(figsize=(12, 10))\n\n# Define custom colors for each island group\nisland_colors = {'Luzon': '#2ecc71', 'Visayas': '#3498db', 'Mindanao': '#e74c3c'}\nphilippines_gdf['color'] = philippines_gdf['Island_Group'].map(island_colors)\n\nphilippines_gdf.plot(\n    ax=ax,\n    color=philippines_gdf['color'],\n    edgecolor='black',\n    linewidth=1,\n    alpha=0.7\n)\n\n# Create custom legend\nlegend_elements = [\n    Patch(facecolor='#2ecc71', label='Luzon'),\n    Patch(facecolor='#3498db', label='Visayas'),\n    Patch(facecolor='#e74c3c', label='Mindanao')\n]\nax.legend(handles=legend_elements, loc='upper right', title='Island Group')\n\nax.set_title('Philippine Provinces by Island Group', fontsize=14, fontweight='bold', pad=20)\nax.set_xlabel('Longitude (°E)', fontsize=11)\nax.set_ylabel('Latitude (°N)', fontsize=11)\nax.grid(True, linestyle='--', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n📝 Exercise 1: Select and Plot Your Home Province\nTask: 1. Select a province from the GeoDataFrame 2. Calculate its population density 3. Create a focused map showing only that province 4. Add informative labels\nHint: Use boolean filtering: gdf[gdf['Province'] == 'YourProvince']\n\n# YOUR CODE HERE\n# Example solution (uncomment and modify):\n\n# my_province = philippines_gdf[philippines_gdf['Province'] == 'Palawan']\n# density = my_province['Population'].values[0] / my_province['Area_km2'].values[0]\n\n# fig, ax = plt.subplots(figsize=(10, 8))\n# my_province.plot(ax=ax, color='green', edgecolor='black', linewidth=2, alpha=0.6)\n# ax.set_title(f\"{my_province['Province'].values[0]} Province\\nDensity: {density:.1f} people/km²\",\n#              fontsize=14, fontweight='bold')\n# plt.show()\n\n\n\nClick to see solution\n\n# Select Palawan\nmy_province = philippines_gdf[philippines_gdf['Province'] == 'Palawan']\n\n# Calculate density\npop = my_province['Population'].values[0]\narea = my_province['Area_km2'].values[0]\ndensity = pop / area\n\n# Create visualization\nfig, ax = plt.subplots(figsize=(10, 8))\nmy_province.plot(\n    ax=ax,\n    color='forestgreen',\n    edgecolor='darkgreen',\n    linewidth=2,\n    alpha=0.6\n)\n\n# Add info text\ninfo_text = f\"Population: {pop:,}\\nArea: {area:.0f} km²\\nDensity: {density:.1f} people/km²\"\nax.text(0.02, 0.98, info_text,\n        transform=ax.transAxes,\n        fontsize=10,\n        verticalalignment='top',\n        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nax.set_title(f\"{my_province['Province'].values[0]} Province\",\n             fontsize=14, fontweight='bold', pad=20)\nax.set_xlabel('Longitude (°E)')\nax.set_ylabel('Latitude (°N)')\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/Day1_Session3_Python_Geospatial_Data.html#part-4-rasterio-for-raster-data",
    "href": "notebooks/Day1_Session3_Python_Geospatial_Data.html#part-4-rasterio-for-raster-data",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Part 4: Rasterio for Raster Data",
    "text": "Part 4: Rasterio for Raster Data\nRasterio is the go-to library for working with raster/gridded data like satellite imagery.\n\nWhy Rasterio?\n\n✓ Read/write GeoTIFF and other raster formats\n✓ NumPy integration for fast array operations\n✓ Handles multi-band imagery (Sentinel-2 has 13 bands!)\n✓ Georeferencing and coordinate transformations\n✓ Masking, clipping, resampling, reprojection\n\n\n\n4.1 Creating Synthetic Sentinel-2 Data\nFor this demo, we’ll create realistic synthetic Sentinel-2 imagery for Palawan.\nIn production, you would:\nwith rasterio.open('sentinel2_L2A_palawan.tif') as src:\n    data = src.read()\n\nfrom rasterio.transform import from_bounds\nfrom rasterio.crs import CRS\n\n# Create synthetic Sentinel-2 data for Palawan\n# Palawan approximate bounds\npalawan_bounds = (117.5, 8.5, 119.5, 11.5)  # (minx, miny, maxx, maxy)\nwidth, height = 400, 600  # Image dimensions (pixels)\n\n# Calculate affine transform (pixel coords → geographic coords)\ntransform = from_bounds(*palawan_bounds, width, height)\n\n# Create realistic synthetic bands\nnp.random.seed(42)  # For reproducibility\n\n# Sentinel-2 L2A typical reflectance values (0-10000 scale)\n# Simulate different land covers: forest, water, urban\n\n# Create spatial patterns\ny, x = np.ogrid[0:height, 0:width]\nx_norm = x / width\ny_norm = y / height\n\n# Base pattern (simulates vegetation gradient)\nvegetation_pattern = 0.5 + 0.3 * np.sin(x_norm * 4 * np.pi) * np.cos(y_norm * 3 * np.pi)\n\n# Water mask (lower left corner)\nwater_mask = ((x_norm &lt; 0.3) & (y_norm &gt; 0.7)).astype(float)\n\n# Urban pattern (scattered bright spots)\nurban_pattern = np.random.random((height, width)) &gt; 0.98\n\n# Band 2 (Blue, 490nm) - 10m resolution\nband_blue = np.random.randint(500, 1500, size=(height, width), dtype=np.uint16)\nband_blue = (band_blue * (1 - water_mask * 0.6) + water_mask * 800).astype(np.uint16)\n\n# Band 3 (Green, 560nm) - 10m resolution  \nband_green = np.random.randint(800, 2000, size=(height, width), dtype=np.uint16)\nband_green = (band_green * (1 - water_mask * 0.5) + water_mask * 1000).astype(np.uint16)\n\n# Band 4 (Red, 665nm) - 10m resolution\nband_red = np.random.randint(400, 1800, size=(height, width), dtype=np.uint16)\nband_red = (band_red * vegetation_pattern * (1 - water_mask * 0.8)).astype(np.uint16)\nband_red[urban_pattern] = np.random.randint(2000, 3000, size=np.sum(urban_pattern))\n\n# Band 8 (NIR, 842nm) - 10m resolution\n# NIR is HIGH over vegetation, LOW over water\nband_nir = np.random.randint(2500, 5500, size=(height, width), dtype=np.uint16)\nband_nir = (band_nir * vegetation_pattern * (1 - water_mask * 0.9)).astype(np.uint16)\nband_nir[water_mask &gt; 0.5] = np.random.randint(200, 600, size=np.sum(water_mask &gt; 0.5))\nband_nir[urban_pattern] = np.random.randint(1500, 2500, size=np.sum(urban_pattern))\n\nprint(\"✓ Synthetic Sentinel-2 bands created!\")\nprint(f\"  Dimensions: {width} x {height} pixels\")\nprint(f\"  Bands: Blue (B2), Green (B3), Red (B4), NIR (B8)\")\nprint(f\"  Resolution: ~5km x 5km area at 10m/pixel\")\nprint(f\"  Simulated features: Vegetation, Water, Urban areas\")\n\n\n\n4.2 Writing Raster to File\n\n# Save as GeoTIFF\nraster_path = '/tmp/palawan_sentinel2_sample.tif'\n\nwith rasterio.open(\n    raster_path,\n    'w',\n    driver='GTiff',\n    height=height,\n    width=width,\n    count=4,  # 4 bands\n    dtype=np.uint16,\n    crs=CRS.from_epsg(4326),\n    transform=transform,\n    compress='lzw'  # Compression for smaller file size\n) as dst:\n    # Write each band\n    dst.write(band_blue, 1)\n    dst.write(band_green, 2)\n    dst.write(band_red, 3)\n    dst.write(band_nir, 4)\n    \n    # Set band descriptions\n    dst.set_band_description(1, 'Blue (B2)')\n    dst.set_band_description(2, 'Green (B3)')\n    dst.set_band_description(3, 'Red (B4)')\n    dst.set_band_description(4, 'NIR (B8)')\n\nprint(f\"✓ Raster saved: {raster_path}\")\nprint(f\"  File size: {os.path.getsize(raster_path) / 1024:.1f} KB\")\n\n\n\n4.3 Opening and Inspecting Raster Metadata\n\n# Open raster file (context manager ensures proper closure)\nsrc = rasterio.open(raster_path)\n\nprint(\"=\"*60)\nprint(\"RASTER METADATA\")\nprint(\"=\"*60)\n\nprint(f\"\\nFile Information:\")\nprint(f\"  Driver: {src.driver}\")\nprint(f\"  Format: {src.driver} (GeoTIFF)\")\nprint(f\"  Compression: {src.profile.get('compress', 'None')}\")\n\nprint(f\"\\nDimensions:\")\nprint(f\"  Width: {src.width} pixels\")\nprint(f\"  Height: {src.height} pixels\")\nprint(f\"  Bands: {src.count}\")\n\nprint(f\"\\nData Type:\")\nprint(f\"  dtype: {src.dtypes[0]}\")\nprint(f\"  bits: {np.iinfo(src.dtypes[0]).bits}\")\nprint(f\"  Value range: {np.iinfo(src.dtypes[0]).min} to {np.iinfo(src.dtypes[0]).max}\")\n\nprint(f\"\\nCoordinate Reference System (CRS):\")\nprint(f\"  CRS: {src.crs.to_string()}\")\nprint(f\"  EPSG: {src.crs.to_epsg()}\")\nprint(f\"  Type: {src.crs.type_name}\")\n\nprint(f\"\\nGeographic Extent (Bounds):\")\nprint(f\"  Left (minx):   {src.bounds.left:.4f}°\")\nprint(f\"  Bottom (miny): {src.bounds.bottom:.4f}°\")\nprint(f\"  Right (maxx):  {src.bounds.right:.4f}°\")\nprint(f\"  Top (maxy):    {src.bounds.top:.4f}°\")\n\nprint(f\"\\nSpatial Resolution:\")\nres_x = (src.bounds.right - src.bounds.left) / src.width\nres_y = (src.bounds.top - src.bounds.bottom) / src.height\nprint(f\"  X resolution: {res_x:.6f}° (~{res_x * 111:.1f} km at equator)\")\nprint(f\"  Y resolution: {res_y:.6f}° (~{res_y * 111:.1f} km)\")\n\nprint(f\"\\nAffine Transform:\")\nprint(f\"{src.transform}\")\n\nprint(f\"\\nBand Descriptions:\")\nfor i in range(1, src.count + 1):\n    desc = src.descriptions[i-1] or f\"Band {i}\"\n    print(f\"  Band {i}: {desc}\")\n\nprint(\"\\n\" + \"=\"*60)\n\n\n\n4.4 Reading Raster Data as NumPy Arrays\n\n# Read individual bands\nblue = src.read(1)   # Band 1 (Blue)\ngreen = src.read(2)  # Band 2 (Green)\nred = src.read(3)    # Band 3 (Red)\nnir = src.read(4)    # Band 4 (NIR)\n\nprint(\"Band Arrays:\")\nprint(f\"  Blue:  shape={blue.shape}, dtype={blue.dtype}\")\nprint(f\"  Green: shape={green.shape}, dtype={green.dtype}\")\nprint(f\"  Red:   shape={red.shape}, dtype={red.dtype}\")\nprint(f\"  NIR:   shape={nir.shape}, dtype={nir.dtype}\")\n\n# Read all bands at once\nall_bands = src.read()  # Returns (bands, rows, cols)\nprint(f\"\\nAll bands: shape={all_bands.shape}\")\nprint(f\"  (bands, rows, columns) = ({all_bands.shape[0]}, {all_bands.shape[1]}, {all_bands.shape[2]})\")\n\n\n\n4.5 Calculating Band Statistics\n\n# Calculate statistics for each band\nbands_dict = {\n    'Blue (B2)': blue,\n    'Green (B3)': green,\n    'Red (B4)': red,\n    'NIR (B8)': nir\n}\n\nprint(\"=\"*80)\nprint(\"BAND STATISTICS (Sentinel-2 Reflectance, 0-10000 scale)\")\nprint(\"=\"*80)\nprint(f\"{'Band':&lt;15} {'Min':&gt;8} {'Max':&gt;8} {'Mean':&gt;10} {'Median':&gt;10} {'Std Dev':&gt;10}\")\nprint(\"-\"*80)\n\nfor band_name, band_data in bands_dict.items():\n    print(f\"{band_name:&lt;15} \"\n          f\"{band_data.min():&gt;8} \"\n          f\"{band_data.max():&gt;8} \"\n          f\"{band_data.mean():&gt;10.1f} \"\n          f\"{np.median(band_data):&gt;10.1f} \"\n          f\"{band_data.std():&gt;10.1f}\")\n\nprint(\"=\"*80)\n\n# Calculate percentiles\nprint(\"\\nPercentile Analysis (Red band):\")\npercentiles = [5, 25, 50, 75, 95]\nvalues = np.percentile(red, percentiles)\nfor p, v in zip(percentiles, values):\n    print(f\"  {p}th percentile: {v:.0f}\")\n\n\n\n4.6 Visualizing Single Bands\n\n# Visualize NIR band (grayscale)\nfig, ax = plt.subplots(figsize=(12, 10))\n\n# Convert to reflectance (0-1 scale)\nnir_refl = nir / 10000.0\n\nim = ax.imshow(nir_refl, cmap='gray', vmin=0, vmax=0.6)\ncbar = plt.colorbar(im, ax=ax, shrink=0.8)\ncbar.set_label('NIR Reflectance', fontsize=11)\n\nax.set_title('Sentinel-2 Near-Infrared Band (B8)', fontsize=14, fontweight='bold', pad=15)\nax.set_xlabel('Column (pixel)', fontsize=11)\nax.set_ylabel('Row (pixel)', fontsize=11)\n\n# Add explanation text\nexplanation = (\n    \"NIR (Near-Infrared):\\n\"\n    \"• Bright = High reflectance (vegetation)\\n\"\n    \"• Dark = Low reflectance (water, bare soil)\"\n)\nax.text(0.02, 0.98, explanation,\n        transform=ax.transAxes,\n        fontsize=9,\n        verticalalignment='top',\n        bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n\nplt.tight_layout()\nplt.show()\n\n\n# Visualize all 4 bands in subplots\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\naxes = axes.flatten()\n\nbands_to_plot = [\n    (blue, 'Blue (B2)', 'Blues'),\n    (green, 'Green (B3)', 'Greens'),\n    (red, 'Red (B4)', 'Reds'),\n    (nir, 'NIR (B8)', 'gray')\n]\n\nfor idx, (band, title, cmap) in enumerate(bands_to_plot):\n    im = axes[idx].imshow(band / 10000.0, cmap=cmap, vmin=0, vmax=0.6)\n    axes[idx].set_title(title, fontsize=12, fontweight='bold')\n    axes[idx].set_xlabel('Column', fontsize=9)\n    axes[idx].set_ylabel('Row', fontsize=9)\n    plt.colorbar(im, ax=axes[idx], fraction=0.046, pad=0.04)\n\nplt.suptitle('Sentinel-2 Multispectral Bands - Palawan', \n             fontsize=15, fontweight='bold', y=0.995)\nplt.tight_layout()\nplt.show()\n\n\n\n4.7 Creating RGB True Color Composite\n\n# Stack RGB bands (Red, Green, Blue)\nrgb = np.dstack([red, green, blue])\n\n# Convert to reflectance (0-1 scale)\nrgb_refl = rgb / 10000.0\n\n# Apply contrast stretch for better visualization\n# Method 1: Simple linear stretch (2nd to 98th percentile)\np2, p98 = np.percentile(rgb_refl, (2, 98))\nrgb_stretched = np.clip((rgb_refl - p2) / (p98 - p2), 0, 1)\n\n# Create side-by-side comparison\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n\n# Original\nax1.imshow(rgb_refl)\nax1.set_title('True Color (Original)', fontsize=12, fontweight='bold')\nax1.set_xlabel('Column')\nax1.set_ylabel('Row')\nax1.text(0.02, 0.98, 'May appear dark\\ndue to reflectance scale',\n         transform=ax1.transAxes, fontsize=9,\n         verticalalignment='top',\n         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n\n# Contrast stretched\nax2.imshow(rgb_stretched)\nax2.set_title('True Color (2-98% Stretch)', fontsize=12, fontweight='bold')\nax2.set_xlabel('Column')\nax2.set_ylabel('Row')\nax2.text(0.02, 0.98, 'Enhanced contrast\\nfor better visualization',\n         transform=ax2.transAxes, fontsize=9,\n         verticalalignment='top',\n         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n\nplt.suptitle('Sentinel-2 True Color Composite (RGB) - Palawan',\n             fontsize=14, fontweight='bold', y=0.98)\nplt.tight_layout()\nplt.show()\n\nprint(\"✓ True color composite created!\")\nprint(\"  This is how the area would look from space in natural color\")\n\n\n\n4.8 False Color Composites\nFalse color composites use non-visible bands to highlight specific features.\n\n# False Color Composite: NIR-Red-Green (Vegetation appears bright red)\nfalse_color_nrg = np.dstack([nir, red, green]) / 10000.0\n\n# Apply stretch\np2, p98 = np.percentile(false_color_nrg, (2, 98))\nfalse_color_nrg_stretched = np.clip((false_color_nrg - p2) / (p98 - p2), 0, 1)\n\n# Display\nfig, ax = plt.subplots(figsize=(12, 10))\n\nax.imshow(false_color_nrg_stretched)\nax.set_title('False Color Composite (NIR-R-G) - Vegetation Analysis',\n             fontsize=14, fontweight='bold', pad=15)\nax.set_xlabel('Column', fontsize=11)\nax.set_ylabel('Row', fontsize=11)\n\n# Add legend\nlegend_text = (\n    \"False Color Interpretation:\\n\"\n    \"• Bright Red = Dense vegetation\\n\"\n    \"• Pink/Light Red = Moderate vegetation\\n\"\n    \"• Dark Blue/Black = Water\\n\"\n    \"• Gray/White = Urban, bare soil\\n\\n\"\n    \"Band Assignment:\\n\"\n    \"R = NIR (B8), G = Red (B4), B = Green (B3)\"\n)\nax.text(1.02, 0.5, legend_text,\n        transform=ax.transAxes,\n        fontsize=9,\n        verticalalignment='center',\n        bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.9))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Why False Color?\")\nprint(\"  • Vegetation reflects STRONGLY in NIR (invisible to human eye)\")\nprint(\"  • By mapping NIR to Red channel, vegetation appears bright red\")\nprint(\"  • Makes vegetation identification much easier!\")\nprint(\"  • Critical for agriculture, forestry, and NRM applications\")\n\n\n\n4.9 Calculating NDVI (Normalized Difference Vegetation Index)\nNDVI is THE most important vegetation index in remote sensing.\n\\[NDVI = \\frac{NIR - Red}{NIR + Red}\\]\nInterpretation: - -1 to 0: Water, bare soil, snow - 0 to 0.2: Sparse vegetation, rock - 0.2 to 0.5: Shrubs, grassland - 0.5 to 0.8: Dense vegetation, healthy crops - 0.8 to 1: Very dense vegetation (tropical forest)\n\n# Calculate NDVI using our function from earlier\nndvi = calculate_ndvi(nir, red)\n\n# Print statistics\nprint(\"=\"*60)\nprint(\"NDVI STATISTICS\")\nprint(\"=\"*60)\nprint(f\"Minimum:   {ndvi.min():.4f}\")\nprint(f\"Maximum:   {ndvi.max():.4f}\")\nprint(f\"Mean:      {ndvi.mean():.4f}\")\nprint(f\"Median:    {np.median(ndvi):.4f}\")\nprint(f\"Std Dev:   {ndvi.std():.4f}\")\nprint(\"=\"*60)\n\n# Calculate area by vegetation class\npixel_area_km2 = (res_x * 111) * (res_y * 111)  # Approximate pixel area\n\nwater_pixels = np.sum(ndvi &lt; 0)\nsparse_pixels = np.sum((ndvi &gt;= 0) & (ndvi &lt; 0.2))\nmoderate_pixels = np.sum((ndvi &gt;= 0.2) & (ndvi &lt; 0.5))\ndense_pixels = np.sum((ndvi &gt;= 0.5) & (ndvi &lt; 0.8))\nvery_dense_pixels = np.sum(ndvi &gt;= 0.8)\n\nprint(\"\\nVegetation Cover Analysis:\")\nprint(f\"  Water/Bare (&lt;0):       {water_pixels:&gt;6} pixels ({water_pixels * pixel_area_km2:.1f} km²)\")\nprint(f\"  Sparse (0-0.2):        {sparse_pixels:&gt;6} pixels ({sparse_pixels * pixel_area_km2:.1f} km²)\")\nprint(f\"  Moderate (0.2-0.5):    {moderate_pixels:&gt;6} pixels ({moderate_pixels * pixel_area_km2:.1f} km²)\")\nprint(f\"  Dense (0.5-0.8):       {dense_pixels:&gt;6} pixels ({dense_pixels * pixel_area_km2:.1f} km²)\")\nprint(f\"  Very Dense (&gt;0.8):     {very_dense_pixels:&gt;6} pixels ({very_dense_pixels * pixel_area_km2:.1f} km²)\")\n\n# Calculate vegetation percentage\nveg_pixels = moderate_pixels + dense_pixels + very_dense_pixels\ntotal_pixels = width * height\nveg_percentage = (veg_pixels / total_pixels) * 100\n\nprint(f\"\\n✓ Overall Vegetation Coverage: {veg_percentage:.1f}%\")\n\n\n# Visualize NDVI\nfig, ax = plt.subplots(figsize=(12, 10))\n\n# Use diverging colormap (red-yellow-green)\nim = ax.imshow(ndvi, cmap='RdYlGn', vmin=-0.2, vmax=0.9)\ncbar = plt.colorbar(im, ax=ax, shrink=0.8, extend='both')\ncbar.set_label('NDVI', fontsize=12, fontweight='bold')\n\n# Add horizontal lines for class boundaries\ncbar.ax.axhline(y=0, color='blue', linewidth=2, linestyle='--', alpha=0.7)\ncbar.ax.axhline(y=0.2, color='orange', linewidth=1.5, linestyle='--', alpha=0.7)\ncbar.ax.axhline(y=0.5, color='yellow', linewidth=1.5, linestyle='--', alpha=0.7)\ncbar.ax.axhline(y=0.8, color='darkgreen', linewidth=1.5, linestyle='--', alpha=0.7)\n\nax.set_title('NDVI - Normalized Difference Vegetation Index',\n             fontsize=14, fontweight='bold', pad=15)\nax.set_xlabel('Column (pixel)', fontsize=11)\nax.set_ylabel('Row (pixel)', fontsize=11)\n\n# Add interpretation legend\nlegend_text = (\n    \"NDVI Interpretation:\\n\\n\"\n    \"&lt; 0 (Red/Brown)\\n\"\n    \"  Water, bare soil\\n\\n\"\n    \"0 - 0.2 (Orange/Yellow)\\n\"\n    \"  Sparse vegetation\\n\\n\"\n    \"0.2 - 0.5 (Light Green)\\n\"\n    \"  Moderate vegetation\\n\\n\"\n    \"0.5 - 0.8 (Green)\\n\"\n    \"  Dense vegetation\\n\\n\"\n\"&gt; 0.8 (Dark Green)\\n\"\n    \"  Very dense vegetation\"\n)\nax.text(1.15, 0.5, legend_text,\n        transform=ax.transAxes,\n        fontsize=9,\n        verticalalignment='center',\n        bbox=dict(boxstyle='round', facecolor='white', alpha=0.9),\n        family='monospace')\n\nplt.tight_layout()\nplt.show()\n\n\n\n4.10 NDVI Histogram and Distribution Analysis\n\n# Create comprehensive histogram\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# Histogram\nax1.hist(ndvi.flatten(), bins=100, color='green', alpha=0.7, edgecolor='darkgreen')\nax1.axvline(ndvi.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {ndvi.mean():.3f}')\nax1.axvline(np.median(ndvi), color='blue', linestyle='--', linewidth=2, label=f'Median: {np.median(ndvi):.3f}')\n\n# Add class boundary lines\nax1.axvline(0, color='black', linestyle=':', linewidth=1.5, alpha=0.5)\nax1.axvline(0.2, color='orange', linestyle=':', linewidth=1.5, alpha=0.5)\nax1.axvline(0.5, color='yellow', linestyle=':', linewidth=1.5, alpha=0.5)\nax1.axvline(0.8, color='darkgreen', linestyle=':', linewidth=1.5, alpha=0.5)\n\nax1.set_xlabel('NDVI Value', fontsize=11, fontweight='bold')\nax1.set_ylabel('Frequency (pixel count)', fontsize=11, fontweight='bold')\nax1.set_title('NDVI Distribution', fontsize=13, fontweight='bold')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Box plot\nbox_data = [ndvi[ndvi &lt; 0].flatten(),\n            ndvi[(ndvi &gt;= 0) & (ndvi &lt; 0.2)].flatten(),\n            ndvi[(ndvi &gt;= 0.2) & (ndvi &lt; 0.5)].flatten(),\n            ndvi[(ndvi &gt;= 0.5) & (ndvi &lt; 0.8)].flatten(),\n            ndvi[ndvi &gt;= 0.8].flatten()]\n\nbp = ax2.boxplot(box_data, \n                 labels=['Water\\n(&lt;0)', 'Sparse\\n(0-0.2)', 'Moderate\\n(0.2-0.5)', \n                        'Dense\\n(0.5-0.8)', 'Very Dense\\n(&gt;0.8)'],\n                 patch_artist=True)\n\n# Color boxes\ncolors = ['brown', 'orange', 'yellow', 'green', 'darkgreen']\nfor patch, color in zip(bp['boxes'], colors):\n    patch.set_facecolor(color)\n    patch.set_alpha(0.6)\n\nax2.set_ylabel('NDVI Value', fontsize=11, fontweight='bold')\nax2.set_title('NDVI by Vegetation Class', fontsize=13, fontweight='bold')\nax2.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\n\n\n📝 Exercise 2: Calculate and Visualize NDWI (Water Index)\nNDWI (Normalized Difference Water Index) is used to detect water bodies.\n\\[NDWI = \\frac{Green - NIR}{Green + NIR}\\]\nTask: 1. Write a function to calculate NDWI 2. Calculate NDWI from the Green and NIR bands 3. Create a visualization showing water bodies 4. Calculate statistics (min, max, mean)\nHints: - NDWI &gt; 0.3: Water - NDWI 0 to 0.3: Wetlands/moist soil - NDWI &lt; 0: Dry land/vegetation\n\n# YOUR CODE HERE\n# Step 1: Write NDWI function\n\n# def calculate_ndwi(green, nir):\n#     \"\"\"\n#     Calculate Normalized Difference Water Index.\n#     NDWI = (Green - NIR) / (Green + NIR)\n#     \"\"\"\n#     # Your code here\n#     pass\n\n# Step 2: Calculate NDWI\n# ndwi = calculate_ndwi(green, nir)\n\n# Step 3: Visualize\n# fig, ax = plt.subplots(figsize=(12, 10))\n# im = ax.imshow(ndwi, cmap='Blues', vmin=-0.5, vmax=0.5)\n# # Add colorbar, title, labels\n# plt.show()\n\n# Step 4: Calculate statistics\n# print(f\"NDWI Statistics:\")\n# print(f\"  Min: {ndwi.min():.3f}\")\n# # ... etc\n\n\n\nClick to see solution\n\ndef calculate_ndwi(green, nir):\n    \"\"\"\n    Calculate Normalized Difference Water Index.\n    NDWI = (Green - NIR) / (Green + NIR)\n    \"\"\"\n    green = green.astype(float)\n    nir = nir.astype(float)\n    \n    denominator = green + nir\n    ndwi = np.where(denominator != 0, (green - nir) / denominator, 0)\n    \n    return ndwi\n\n# Calculate NDWI\nndwi = calculate_ndwi(green, nir)\n\n# Statistics\nprint(\"NDWI Statistics:\")\nprint(f\"  Min:    {ndwi.min():.3f}\")\nprint(f\"  Max:    {ndwi.max():.3f}\")\nprint(f\"  Mean:   {ndwi.mean():.3f}\")\nprint(f\"  Median: {np.median(ndwi):.3f}\")\n\n# Water area calculation\nwater_pixels = np.sum(ndwi &gt; 0.3)\nwater_area_km2 = water_pixels * pixel_area_km2\nprint(f\"\\nWater bodies (NDWI &gt; 0.3): {water_area_km2:.1f} km²\")\n\n# Visualization\nfig, ax = plt.subplots(figsize=(12, 10))\n\nim = ax.imshow(ndwi, cmap='Blues', vmin=-0.5, vmax=0.5)\ncbar = plt.colorbar(im, ax=ax, shrink=0.8)\ncbar.set_label('NDWI', fontsize=12, fontweight='bold')\n\nax.set_title('NDWI - Normalized Difference Water Index',\n             fontsize=14, fontweight='bold', pad=15)\nax.set_xlabel('Column (pixel)', fontsize=11)\nax.set_ylabel('Row (pixel)', fontsize=11)\n\n# Add legend\nlegend_text = (\n    \"NDWI Interpretation:\\n\\n\"\n    \"&gt; 0.3 (Dark Blue)\\n\"\n    \"  Water bodies\\n\\n\"\n    \"0 to 0.3 (Light Blue)\\n\"\n    \"  Wetlands, moist soil\\n\\n\"\n    \"&lt; 0 (White/Gray)\\n\"\n    \"  Dry land, vegetation\"\n)\nax.text(1.12, 0.5, legend_text,\n        transform=ax.transAxes,\n        fontsize=9,\n        verticalalignment='center',\n        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.9),\n        family='monospace')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/Day1_Session3_Python_Geospatial_Data.html#part-5-combined-operations---vector-and-raster-integration",
    "href": "notebooks/Day1_Session3_Python_Geospatial_Data.html#part-5-combined-operations---vector-and-raster-integration",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Part 5: Combined Operations - Vector and Raster Integration",
    "text": "Part 5: Combined Operations - Vector and Raster Integration\nThe real power of geospatial analysis comes from combining vector and raster data.\nCommon workflows: - Clip raster to administrative boundaries - Extract statistics per province/region - Overlay boundaries on satellite imagery - Sample raster values at point locations\n\n5.1 Clipping Raster to Vector Boundary\n\nfrom rasterio.mask import mask as rasterio_mask\n\n# Select Palawan province\npalawan_gdf = philippines_gdf[philippines_gdf['Province'] == 'Palawan']\n\n# Get geometry in format rasterio expects (GeoJSON-like)\npalawan_geom = [palawan_gdf.geometry.values[0].__geo_interface__]\n\n# Open raster and clip\nwith rasterio.open(raster_path) as src:\n    # Clip raster to Palawan boundary\n    out_image, out_transform = rasterio_mask(src, palawan_geom, crop=True, filled=True)\n    out_meta = src.meta.copy()\n\n# Update metadata\nout_meta.update({\n    \"height\": out_image.shape[1],\n    \"width\": out_image.shape[2],\n    \"transform\": out_transform\n})\n\nprint(\"✓ Raster clipped to Palawan boundary!\")\nprint(f\"  Original size: {height} x {width} pixels\")\nprint(f\"  Clipped size:  {out_image.shape[1]} x {out_image.shape[2]} pixels\")\nprint(f\"  Reduction:     {(1 - (out_image.shape[1] * out_image.shape[2]) / (height * width)) * 100:.1f}%\")\n\n# Extract clipped bands\nclipped_red = out_image[2, :, :]\nclipped_nir = out_image[3, :, :]\n\n# Calculate NDVI for clipped area\nclipped_ndvi = calculate_ndvi(clipped_nir, clipped_red)\n\nprint(f\"\\nClipped NDVI statistics:\")\nprint(f\"  Mean: {clipped_ndvi.mean():.3f}\")\nprint(f\"  Min:  {clipped_ndvi.min():.3f}\")\nprint(f\"  Max:  {clipped_ndvi.max():.3f}\")\n\n\n# Visualize clipped NDVI\nfig, ax = plt.subplots(figsize=(12, 10))\n\nim = ax.imshow(clipped_ndvi, cmap='RdYlGn', vmin=-0.2, vmax=0.9)\ncbar = plt.colorbar(im, ax=ax, shrink=0.8)\ncbar.set_label('NDVI', fontsize=12)\n\nax.set_title('NDVI - Palawan Province Only (Clipped)',\n             fontsize=14, fontweight='bold', pad=15)\nax.set_xlabel('Column', fontsize=11)\nax.set_ylabel('Row', fontsize=11)\n\nplt.tight_layout()\nplt.show()\n\n\n\n5.2 Overlay Vector Boundaries on Raster\n\n# Create combined visualization\nfig, ax = plt.subplots(figsize=(14, 12))\n\n# Display NDVI as background\nextent = [src.bounds.left, src.bounds.right, src.bounds.bottom, src.bounds.top]\nim = ax.imshow(ndvi, cmap='RdYlGn', vmin=-0.2, vmax=0.9,\n               extent=extent, origin='upper')\n\n# Overlay province boundaries\nphilippines_gdf.boundary.plot(ax=ax, edgecolor='blue', linewidth=2, label='Province Boundaries')\n\n# Highlight Palawan\npalawan_gdf.boundary.plot(ax=ax, edgecolor='red', linewidth=3, label='Palawan (highlighted)')\n\n# Add colorbar\ncbar = plt.colorbar(im, ax=ax, shrink=0.7, pad=0.02)\ncbar.set_label('NDVI', fontsize=12)\n\nax.set_xlabel('Longitude (°E)', fontsize=11)\nax.set_ylabel('Latitude (°N)', fontsize=11)\nax.set_title('NDVI with Province Boundaries Overlay',\n             fontsize=14, fontweight='bold', pad=20)\nax.legend(loc='upper right', fontsize=10)\nax.grid(True, alpha=0.3, linestyle='--')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"✓ Combined vector-raster visualization created!\")\nprint(\"  This demonstrates spatial integration of different data types\")\n\n\n\n5.3 Zonal Statistics - Calculate Mean NDVI per Province\n\nfrom rasterio.features import rasterize\nfrom rasterio.transform import rowcol\n\n# Simple approach: Sample NDVI at province centroids\n# For full zonal statistics, use rasterstats library (not installed by default)\n\ndef sample_raster_at_point(lon, lat, raster_array, transform):\n    \"\"\"\n    Sample raster value at given coordinates.\n    \"\"\"\n    from rasterio.transform import rowcol\n    \n    # Convert geographic to pixel coordinates\n    row, col = rowcol(transform, lon, lat)\n    \n    # Check bounds\n    if 0 &lt;= row &lt; raster_array.shape[0] and 0 &lt;= col &lt; raster_array.shape[1]:\n        return raster_array[row, col]\n    else:\n        return np.nan\n\n# Sample NDVI at each province centroid\nndvi_values = []\nfor idx, row in philippines_gdf.iterrows():\n    centroid = row['centroid']\n    ndvi_val = sample_raster_at_point(centroid.x, centroid.y, ndvi, src.transform)\n    ndvi_values.append(ndvi_val)\n\nphilippines_gdf['NDVI_Centroid'] = ndvi_values\n\nprint(\"=\"*60)\nprint(\"MEAN NDVI BY PROVINCE (sampled at centroids)\")\nprint(\"=\"*60)\nprint(f\"{'Province':&lt;20} {'NDVI':&gt;10} {'Vegetation Class':&gt;20}\")\nprint(\"-\"*60)\n\nfor idx, row in philippines_gdf.iterrows():\n    ndvi_val = row['NDVI_Centroid']\n    if np.isnan(ndvi_val):\n        veg_class = \"Outside raster\"\n    elif ndvi_val &lt; 0:\n        veg_class = \"Water/Bare\"\n    elif ndvi_val &lt; 0.2:\n        veg_class = \"Sparse\"\n    elif ndvi_val &lt; 0.5:\n        veg_class = \"Moderate\"\n    elif ndvi_val &lt; 0.8:\n        veg_class = \"Dense\"\n    else:\n        veg_class = \"Very Dense\"\n    \n    print(f\"{row['Province']:&lt;20} {ndvi_val:&gt;10.3f} {veg_class:&gt;20}\")\n\nprint(\"=\"*60)\nprint(\"\\nNote: For accurate zonal statistics, use rasterstats library\")\nprint(\"      This provides full polygon statistics (mean, median, min, max)\")\n\n\n\n5.4 Saving Results\n\n# Save NDVI as GeoTIFF\nndvi_path = f'{work_dir}/outputs/palawan_ndvi.tif'\n\n# Copy metadata from source\nndvi_meta = src.meta.copy()\nndvi_meta.update({\n    'count': 1,\n    'dtype': 'float32',\n    'nodata': -9999\n})\n\nwith rasterio.open(ndvi_path, 'w', **ndvi_meta) as dst:\n    dst.write(ndvi.astype('float32'), 1)\n    dst.set_band_description(1, 'NDVI')\n\nprint(f\"✓ NDVI saved: {ndvi_path}\")\n\n# Save updated GeoDataFrame with NDVI values\nvector_path = f'{work_dir}/outputs/provinces_with_ndvi.geojson'\nphilippines_gdf.to_file(vector_path, driver='GeoJSON')\n\nprint(f\"✓ Vector data saved: {vector_path}\")\nprint(f\"\\nAll outputs saved to: {work_dir}/outputs/\")"
  },
  {
    "objectID": "notebooks/Day1_Session3_Python_Geospatial_Data.html#part-6-best-practices-and-common-pitfalls",
    "href": "notebooks/Day1_Session3_Python_Geospatial_Data.html#part-6-best-practices-and-common-pitfalls",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Part 6: Best Practices and Common Pitfalls",
    "text": "Part 6: Best Practices and Common Pitfalls\n\n6.1 Memory Management\n\nprint(\"BEST PRACTICES FOR MEMORY MANAGEMENT:\\n\")\n\nprint(\"1. ALWAYS use context managers (with statements):\")\nprint(\"   ✓ with rasterio.open('file.tif') as src:\")\nprint(\"       data = src.read()\")\nprint(\"   ✗ src = rasterio.open('file.tif')  # Don't forget to close!\\n\")\n\nprint(\"2. Read only what you need:\")\nprint(\"   ✓ band = src.read(1)  # Single band\")\nprint(\"   ✗ all_bands = src.read()  # All bands (if you only need one)\\n\")\n\nprint(\"3. Use windowed reading for large files:\")\nprint(\"   from rasterio.windows import Window\")\nprint(\"   window = Window(0, 0, 1000, 1000)  # 1000x1000 subset\")\nprint(\"   data = src.read(1, window=window)\\n\")\n\nprint(\"4. Process in chunks for very large datasets:\")\nprint(\"   for ji, window in src.block_windows(1):\")\nprint(\"       data = src.read(1, window=window)\")\nprint(\"       # Process chunk\")\nprint(\"       # Write result\\n\")\n\nprint(\"5. Delete large arrays when done:\")\nprint(\"   del large_array\")\nprint(\"   import gc; gc.collect()  # Force garbage collection\")\n\n\n\n6.2 CRS Alignment - CRITICAL!\n\nprint(\"CRS (Coordinate Reference System) ALIGNMENT:\\n\")\n\nprint(\"ALWAYS check CRS before combining data!\\n\")\n\n# Example: Check and align CRS\nprint(\"Step 1: Check CRS\")\nprint(f\"  Vector CRS: {philippines_gdf.crs}\")\nprint(f\"  Raster CRS: {src.crs}\")\n\nprint(\"\\nStep 2: Reproject if needed\")\nprint(\"  if vector.crs != raster.crs:\")\nprint(\"      vector = vector.to_crs(raster.crs)\")\nprint(\"      print('Vector reprojected!')\\n\")\n\nprint(\"COMMON CRS IN PHILIPPINES:\")\nprint(\"  EPSG:4326  - WGS84 Geographic (lat/lon in degrees)\")\nprint(\"  EPSG:32651 - WGS84 / UTM Zone 51N (meters, for Luzon/Visayas)\")\nprint(\"  EPSG:32652 - WGS84 / UTM Zone 52N (meters, for Mindanao)\")\nprint(\"  EPSG:3123  - PRS92 / Philippines Zone I\")\nprint(\"  EPSG:3124  - PRS92 / Philippines Zone II\")\nprint(\"  EPSG:3125  - PRS92 / Philippines Zone III\\n\")\n\nprint(\"PRO TIP: Use UTM for accurate area/distance calculations!\")\n\n\n\n6.3 Handling NoData Values\n\nprint(\"HANDLING NODATA VALUES:\\n\")\n\n# Check for nodata value\nprint(f\"Current raster nodata value: {src.nodata}\")\n\nprint(\"\\nMethod 1: Read with masked=True\")\nprint(\"  data = src.read(1, masked=True)  # Returns np.ma.MaskedArray\")\nprint(\"  valid_mean = data.mean()  # Automatically ignores nodata\")\n\nprint(\"\\nMethod 2: Manual masking\")\nprint(\"  data = src.read(1)\")\nprint(\"  if src.nodata is not None:\")\nprint(\"      valid_data = data[data != src.nodata]\")\nprint(\"      valid_mean = valid_data.mean()\")\n\nprint(\"\\nMethod 3: NumPy masked arrays\")\nprint(\"  import numpy.ma as ma\")\nprint(\"  masked_data = ma.masked_equal(data, src.nodata)\")\nprint(\"  valid_mean = masked_data.mean()\")\n\nprint(\"\\nWHY IT MATTERS:\")\nprint(\"  NoData pixels can skew statistics if not handled!\")\nprint(\"  Example: mean() of [100, 100, -9999] = -3266 (WRONG!)\")\nprint(\"           mean() excluding nodata = 100 (CORRECT!)\")\n\n\n\n6.4 Common Errors and Solutions\n\nprint(\"COMMON ERRORS AND SOLUTIONS:\\n\")\nprint(\"=\"*70)\n\nprint(\"\\n1. 'ValueError: cannot set EPSG:4326 CRS'\")\nprint(\"   CAUSE: CRS already set or incompatible\")\nprint(\"   FIX: gdf.set_crs('EPSG:4326', allow_override=True)\\n\")\n\nprint(\"2. 'IndexError: index 1 is out of bounds'\")\nprint(\"   CAUSE: Trying to read band that doesn't exist\")\nprint(\"   FIX: Check src.count before reading\")\nprint(\"        bands = src.read([1, 2, 3])  # Read multiple\\n\")\n\nprint(\"3. 'TypeError: integer argument expected, got float'\")\nprint(\"   CAUSE: Pixel coordinates must be integers\")\nprint(\"   FIX: row, col = int(row), int(col)\\n\")\n\nprint(\"4. 'MemoryError: Unable to allocate array'\")\nprint(\"   CAUSE: Trying to load massive raster into memory\")\nprint(\"   FIX: Use windowed reading or downsample\")\nprint(\"        data = src.read(1, out_shape=(500, 500))\\n\")\n\nprint(\"5. 'RuntimeWarning: invalid value encountered in divide'\")\nprint(\"   CAUSE: Division by zero in NDVI/NDWI calculation\")\nprint(\"   FIX: Use np.where() to handle zero denominators\")\nprint(\"        ndvi = np.where(denom != 0, (nir-red)/denom, 0)\\n\")\n\nprint(\"6. 'GeoDataFrame.to_file() slow for large datasets'\")\nprint(\"   CAUSE: Shapefile format is slow\")\nprint(\"   FIX: Use GeoPackage or GeoJSON\")\nprint(\"        gdf.to_file('data.gpkg', driver='GPKG')  # Faster!\\n\")\n\nprint(\"=\"*70)"
  },
  {
    "objectID": "notebooks/Day1_Session3_Python_Geospatial_Data.html#summary-and-key-takeaways",
    "href": "notebooks/Day1_Session3_Python_Geospatial_Data.html#summary-and-key-takeaways",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\n\nWhat You’ve Learned Today:\n\n1. GeoPandas for Vector Data\n✓ Loading and inspecting shapefiles/GeoJSON\n✓ Filtering by attributes and spatial queries\n✓ CRS transformations and projections\n✓ Creating professional maps and visualizations\n✓ Spatial operations (buffer, intersection, union)\n\n\n2. Rasterio for Raster Data\n✓ Reading multi-band satellite imagery\n✓ Extracting metadata and band information\n✓ Processing bands as NumPy arrays\n✓ Calculating statistics and percentiles\n✓ Creating RGB and false color composites\n\n\n3. Vegetation Indices\n✓ NDVI calculation and interpretation\n✓ NDWI for water body detection\n✓ Histogram analysis and thresholding\n✓ Land cover classification based on indices\n\n\n4. Integrated Workflows\n✓ Clipping rasters to vector boundaries\n✓ Overlaying vectors on rasters\n✓ Zonal statistics (per-province analysis)\n✓ Saving results in multiple formats\n\n\n5. Best Practices\n✓ Memory management techniques\n✓ CRS alignment (CRITICAL!)\n✓ NoData value handling\n✓ Error prevention and debugging\n\n\n\n\nWhy This Matters for AI/ML\nThese skills are ESSENTIAL for:\n\nData Preparation\n\nLoading training data (labeled polygons)\nPreprocessing satellite imagery\nCreating feature layers for models\n\nFeature Engineering\n\nCalculating spectral indices (NDVI, NDWI, etc.)\nExtracting texture features\nCreating multi-temporal composites\n\nModel Training\n\nSampling training pixels\nCreating validation datasets\nBalancing class distributions\n\nResult Analysis\n\nVisualizing model predictions\nCalculating accuracy metrics\nValidating against ground truth\n\nDeployment\n\nProcessing new satellite scenes\nGenerating operational products\nCreating decision support maps\n\n\n\n\n\nPhilippine EO Applications\nYou can now build applications for:\nDisaster Risk Reduction (DRR): - Flood extent mapping using NDWI - Landslide susceptibility analysis - Typhoon damage assessment\nClimate Change Adaptation (CCA): - Vegetation health monitoring (NDVI) - Drought impact assessment - Coastal erosion detection\nNatural Resource Management (NRM): - Forest cover monitoring - Agricultural land mapping - Marine protected area monitoring"
  },
  {
    "objectID": "notebooks/Day1_Session3_Python_Geospatial_Data.html#next-session-google-earth-engine-python-api",
    "href": "notebooks/Day1_Session3_Python_Geospatial_Data.html#next-session-google-earth-engine-python-api",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Next Session: Google Earth Engine Python API",
    "text": "Next Session: Google Earth Engine Python API\nSession 4 will cover: - Accessing petabytes of satellite data in the cloud - Processing Sentinel-1 and Sentinel-2 at scale - Cloud masking and temporal compositing - Exporting data for ML workflows - Integrating GEE with local Python analysis\nPreview:\nimport ee\nee.Initialize()\n\n# Access entire Sentinel-2 archive\ns2 = ee.ImageCollection('COPERNICUS/S2_SR') \\\n    .filterBounds(palawan) \\\n    .filterDate('2024-01-01', '2024-12-31') \\\n    .map(mask_clouds)\n\n# Create cloud-free composite\ncomposite = s2.median()\n\n# Calculate NDVI at planetary scale!\nndvi = composite.normalizedDifference(['B8', 'B4'])"
  },
  {
    "objectID": "notebooks/Day1_Session3_Python_Geospatial_Data.html#additional-resources",
    "href": "notebooks/Day1_Session3_Python_Geospatial_Data.html#additional-resources",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nDocumentation\n\nGeoPandas: https://geopandas.org/\nRasterio: https://rasterio.readthedocs.io/\nNumPy: https://numpy.org/doc/\nMatplotlib: https://matplotlib.org/\n\n\n\nTutorials\n\nCarpentries Geospatial Python: https://carpentries-incubator.github.io/geospatial-python/\nEarth Data Science: https://www.earthdatascience.org/\nPython for Geospatial Analysis: https://www.tomasbeuzen.com/python-for-geospatial-analysis/\n\n\n\nPhilippine Data Sources\n\nPhilSA: https://philsa.gov.ph/\nNAMRIA Geoportal: https://www.geoportal.gov.ph/\nDOST-ASTI DATOS: https://asti.dost.gov.ph/\nHDX Philippines: https://data.humdata.org/group/phl\nHazardHunterPH: https://hazardhunter.georisk.gov.ph/\n\n\n\nBooks\n\nGeoprocessing with Python (Garrard)\nLearning Geospatial Analysis with Python (Lawhead)\nPython for Data Analysis (McKinney)"
  },
  {
    "objectID": "notebooks/Day1_Session3_Python_Geospatial_Data.html#practice-exercises-optional-homework",
    "href": "notebooks/Day1_Session3_Python_Geospatial_Data.html#practice-exercises-optional-homework",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Practice Exercises (Optional Homework)",
    "text": "Practice Exercises (Optional Homework)\nTo reinforce your learning:\n\nExercise A: Multi-Province Analysis\nCalculate and compare NDVI statistics for all provinces in one island group.\n\n\nExercise B: Time-Series Simulation\nCreate multiple synthetic images representing different seasons and analyze NDVI changes.\n\n\nExercise C: Custom Index\nResearch and implement another vegetation index (EVI, SAVI, or MSAVI).\n\n\nExercise D: Real Data\nDownload actual Sentinel-2 data from Copernicus Data Space and apply these techniques.\n\n\nExercise E: Water Detection\nUse NDWI to create a binary water mask and calculate total water area."
  },
  {
    "objectID": "notebooks/Day1_Session3_Python_Geospatial_Data.html#clean-up",
    "href": "notebooks/Day1_Session3_Python_Geospatial_Data.html#clean-up",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Clean Up",
    "text": "Clean Up\n\n# Close raster file\nsrc.close()\n\n# Clean up temporary files (optional)\nimport os\ntemp_files = [raster_path]\n\nfor f in temp_files:\n    if os.path.exists(f):\n        os.remove(f)\n        print(f\"Removed: {f}\")\n\nprint(\"\\n✓ Cleanup complete!\")\nprint(f\"\\nYour outputs are saved in: {work_dir}/outputs/\")"
  },
  {
    "objectID": "notebooks/notebook2.html",
    "href": "notebooks/notebook2.html",
    "title": "Notebook 2: Google Earth Engine",
    "section": "",
    "text": "This notebook accompanies Session 4: Introduction to Google Earth Engine. You’ll learn to access, filter, and process massive satellite imagery collections using the Earth Engine Python API.\n\n\nBy completing this notebook, you will:\n\nAuthenticate and initialize Google Earth Engine\nAccess Sentinel-1 SAR and Sentinel-2 optical data\nFilter ImageCollections by location, date, and metadata\nApply cloud masking to optical imagery\nCreate temporal composites\nCalculate vegetation indices (NDVI)\nExport processed data for external use\nApply concepts to Philippine use cases"
  },
  {
    "objectID": "notebooks/notebook2.html#session-4-hands-on-notebook",
    "href": "notebooks/notebook2.html#session-4-hands-on-notebook",
    "title": "Notebook 2: Google Earth Engine",
    "section": "",
    "text": "This notebook accompanies Session 4: Introduction to Google Earth Engine. You’ll learn to access, filter, and process massive satellite imagery collections using the Earth Engine Python API.\n\n\nBy completing this notebook, you will:\n\nAuthenticate and initialize Google Earth Engine\nAccess Sentinel-1 SAR and Sentinel-2 optical data\nFilter ImageCollections by location, date, and metadata\nApply cloud masking to optical imagery\nCreate temporal composites\nCalculate vegetation indices (NDVI)\nExport processed data for external use\nApply concepts to Philippine use cases"
  },
  {
    "objectID": "notebooks/notebook2.html#getting-started",
    "href": "notebooks/notebook2.html#getting-started",
    "title": "Notebook 2: Google Earth Engine",
    "section": "Getting Started",
    "text": "Getting Started\n\nOption 1: Open in Google Colab (Required for Earth Engine)\nClick the button below to open this notebook in Google Colab:\n\n\n\n\nOpen In Colab\n\n\n\n\n\n\n\n\n\nImportantEarth Engine Account Required\n\n\n\nYou must have a registered Google Earth Engine account to run this notebook. If you haven’t registered yet, see the Setup Guide.\nRegistration takes 24-48 hours for approval.\n\n\nAdvantages of Colab: - Direct Earth Engine integration - No data downloads required - Cloud processing power - Interactive mapping with geemap\n\n\nOption 2: Download Notebook\nDownload the Jupyter notebook:\n\nDownload .ipynb File\nRequirements: - Google Earth Engine account - Authenticated credentials\npip install earthengine-api geemap folium"
  },
  {
    "objectID": "notebooks/notebook2.html#notebook-preview",
    "href": "notebooks/notebook2.html#notebook-preview",
    "title": "Notebook 2: Google Earth Engine",
    "section": "Notebook Preview",
    "text": "Notebook Preview\n\n\n\n\n\n\nNoteCloud Platform Access Required\n\n\n\nThis notebook requires an active internet connection and Google Earth Engine account. All processing happens in Google’s cloud infrastructure.\n\n\n\nTopics Covered\n\nEarth Engine Fundamentals\n\nAuthentication and initialization\nUnderstanding ee.Image and ee.ImageCollection\nGeometry definitions (points, polygons, rectangles)\nUnderstanding the Earth Engine data catalog\n\nAccessing Sentinel Data\n\nSentinel-2 optical imagery (COPERNICUS/S2_SR_HARMONIZED)\nSentinel-1 SAR imagery (COPERNICUS/S1_GRD)\nUnderstanding collection IDs and band names\nExploring metadata\n\nFiltering ImageCollections\n\nSpatial filtering (filterBounds)\nTemporal filtering (filterDate)\nMetadata filtering (cloud cover, orbit direction)\nCombining multiple filters\n\nCloud Masking & Compositing\n\nUsing the QA60 band for cloud detection\nBitwise operations for mask creation\nCreating median composites\nQuality mosaicking\n\nSpectral Indices\n\nNDVI (Normalized Difference Vegetation Index)\nNDWI (Normalized Difference Water Index)\nBand math operations\nVisualization parameters\n\nPhilippine Case Studies\n\nMetro Manila monitoring with Sentinel-2\nPalawan land cover analysis\nFlood detection with Sentinel-1\nAgricultural monitoring in Central Luzon\n\nData Export\n\nExporting to Google Drive\nExporting to Earth Engine Assets\nSetting scale and region parameters\nManaging export tasks"
  },
  {
    "objectID": "notebooks/notebook2.html#prerequisites",
    "href": "notebooks/notebook2.html#prerequisites",
    "title": "Notebook 2: Google Earth Engine",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore starting this notebook, ensure you have:\n\n✅ Google Earth Engine account (registered and approved)\n✅ Completed Setup Guide\n✅ Understanding of Session 4 concepts\n✅ Basic Python knowledge\n✅ Familiarity with Jupyter/Colab"
  },
  {
    "objectID": "notebooks/notebook2.html#notebook-contents",
    "href": "notebooks/notebook2.html#notebook-contents",
    "title": "Notebook 2: Google Earth Engine",
    "section": "Notebook Contents",
    "text": "Notebook Contents\nThe full interactive notebook includes:\n\n20+ code cells with step-by-step instructions\n15+ visualizations including interactive maps\n4 Philippine case studies with real-world applications\nExport workflows for downloading processed data\nTroubleshooting section for common errors\nExercises to reinforce learning"
  },
  {
    "objectID": "notebooks/notebook2.html#key-concepts-covered",
    "href": "notebooks/notebook2.html#key-concepts-covered",
    "title": "Notebook 2: Google Earth Engine",
    "section": "Key Concepts Covered",
    "text": "Key Concepts Covered\n\nEarth Engine Architecture\n# Basic Earth Engine workflow\nimport ee\nee.Initialize()\n\n# Define area of interest\nphilippines = ee.Geometry.Rectangle([116.0, 4.0, 127.0, 21.0])\n\n# Access Sentinel-2 collection\ncollection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n    .filterBounds(philippines) \\\n    .filterDate('2024-01-01', '2024-12-31') \\\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n\n# Create composite\ncomposite = collection.median()\n\n# Calculate NDVI\nndvi = composite.normalizedDifference(['B8', 'B4'])\n\n\nInteractive Mapping with geemap\nimport geemap\n\n# Create interactive map\nMap = geemap.Map()\nMap.centerObject(philippines, 6)\n\n# Add layers\nMap.addLayer(composite, {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 3000}, 'True Color')\nMap.addLayer(ndvi, {'min': 0, 'max': 1, 'palette': ['red', 'yellow', 'green']}, 'NDVI')\n\nMap"
  },
  {
    "objectID": "notebooks/notebook2.html#philippine-use-cases",
    "href": "notebooks/notebook2.html#philippine-use-cases",
    "title": "Notebook 2: Google Earth Engine",
    "section": "Philippine Use Cases",
    "text": "Philippine Use Cases\n\nCase Study 1: Metro Manila Urban Monitoring\nTrack urban expansion and changes in the National Capital Region using multi-temporal Sentinel-2 data.\n\n\nCase Study 2: Palawan Forest Cover\nMonitor forest cover and detect deforestation in Palawan Province using NDVI time series.\n\n\nCase Study 3: Central Luzon Flood Mapping\nDetect flood extents using Sentinel-1 SAR backscatter changes before and after typhoon events.\n\n\nCase Study 4: Mindanao Agricultural Drought\nAssess agricultural drought impacts using vegetation indices and SWIR bands."
  },
  {
    "objectID": "notebooks/notebook2.html#common-errors-solutions",
    "href": "notebooks/notebook2.html#common-errors-solutions",
    "title": "Notebook 2: Google Earth Engine",
    "section": "Common Errors & Solutions",
    "text": "Common Errors & Solutions\n\nError: “Please set project ID”\nCause: Earth Engine not authenticated\nSolution:\nee.Authenticate()  # Follow prompts\nee.Initialize()\n\n\nError: “User memory limit exceeded”\nSolution: Reduce spatial or temporal scope, increase scale parameter\n\n\nError: “Too many concurrent aggregations”\nSolution: Add .limit() to reduce collection size\nSee the FAQ for more troubleshooting help."
  },
  {
    "objectID": "notebooks/notebook2.html#support",
    "href": "notebooks/notebook2.html#support",
    "title": "Notebook 2: Google Earth Engine",
    "section": "Support",
    "text": "Support\n\nDuring the Training\n\nAsk questions in live session\nConsult teaching assistants\nShare your results with the group\n\n\n\nAfter the Training\n\nReview Earth Engine Cheat Sheet\nCheck FAQ\nJoin GEE Community Forum"
  },
  {
    "objectID": "notebooks/notebook2.html#related-resources",
    "href": "notebooks/notebook2.html#related-resources",
    "title": "Notebook 2: Google Earth Engine",
    "section": "Related Resources",
    "text": "Related Resources\n\nSession Materials: - Session 4: Introduction to Google Earth Engine - Session 4 Presentation Slides\nQuick References: - Earth Engine Python API Cheat Sheet - Sentinel Missions Reference\nOfficial Documentation: - Earth Engine Python API Guide - Earth Engine Data Catalog - geemap Documentation\nCommunity Resources: - Awesome Earth Engine - Earth Engine Tutorials"
  },
  {
    "objectID": "notebooks/notebook2.html#next-steps",
    "href": "notebooks/notebook2.html#next-steps",
    "title": "Notebook 2: Google Earth Engine",
    "section": "Next Steps",
    "text": "Next Steps\nAfter completing this notebook:\n\n✅ Practice with different Philippine regions\n✅ Experiment with other satellites (Landsat, MODIS)\n✅ Prepare for Day 2: Machine Learning for Land Cover Classification\n✅ Export data for your own projects\n\n\n\n\n\nPrevious\n\n\n← Notebook 1: Python Geospatial\n\n\n\n\nBack\n\n\nSession 4 Overview →\n\n\n\n\n\n\n\n\n\n\nTipReady to Explore Petabytes of Data?\n\n\n\nOpen the notebook in Colab and start accessing the entire Sentinel archive from your browser!\n\n\nAll processing happens in the cloud - no downloads required!"
  },
  {
    "objectID": "sessions/session1.html",
    "href": "sessions/session1.html",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "",
    "text": "Duration: 2 hours | Format: Lecture + Interactive Demo | Platform: Web browser",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "sessions/session1.html#session-overview",
    "href": "sessions/session1.html#session-overview",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Session Overview",
    "text": "Session Overview\nThis session introduces the European Copernicus Earth Observation program, focusing on Sentinel-1 (SAR) and Sentinel-2 (Optical) missions with 2025 updates. You’ll explore the Philippine EO landscape, including key agencies and data platforms that complement Copernicus data for disaster risk reduction, climate adaptation, and resource management applications.\n\nLearning Objectives\nBy the end of this session, you will be able to:\n\nDescribe the Copernicus programme and its mission for global Earth monitoring\nCompare Sentinel-1 SAR and Sentinel-2 optical satellite characteristics\nIdentify 2025 constellation updates (Sentinel-2C, Sentinel-1C operational status)\nNavigate the Copernicus Data Space Ecosystem and SentiBoard dashboard\nLocate Philippine EO data through PhilSA, NAMRIA, and DOST-ASTI platforms\nExplain how local and European datasets complement each other for Philippine applications\nAccess the CopPhil Mirror Site and Digital Space Campus",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "sessions/session1.html#part-1-the-copernicus-programme",
    "href": "sessions/session1.html#part-1-the-copernicus-programme",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Part 1: The Copernicus Programme",
    "text": "Part 1: The Copernicus Programme\n\nWhat is Copernicus?\nCopernicus is the European Union’s flagship Earth Observation programme providing free and open satellite data for environmental monitoring and societal applications worldwide. It represents the world’s largest single Earth observation programme, with a multi-billion euro investment.\n\n\n\n\n\n\nNoteWhy “Copernicus”?\n\n\n\nNamed after Nicolaus Copernicus (1473-1543), the Renaissance astronomer who formulated the heliocentric model of the universe. Just as Copernicus revolutionized our understanding of our place in the cosmos, the Copernicus programme transforms our understanding of Earth systems.\n\n\n\n\nCopernicus Components\nThe programme consists of:\n\nSpace Component - The Sentinel satellite family (1-6)\nIn Situ Component - Ground-based and airborne observations\nServices - Thematic information products:\n\nAtmosphere Monitoring\nMarine Environment Monitoring\nLand Monitoring\nClimate Change\nEmergency Management\nSecurity\n\n\n\n\nThe Sentinel Family\n\n\n\nSentinel\nType\nPrimary Application\n\n\n\n\nSentinel-1\nC-band SAR\nAll-weather radar imaging\n\n\nSentinel-2\nMultispectral Optical\nLand monitoring\n\n\nSentinel-3\nOcean & Land\nMarine/land surface monitoring\n\n\nSentinel-4\nGeostationary\nAir quality monitoring\n\n\nSentinel-5P\nAtmospheric\nAir quality and pollution\n\n\nSentinel-6\nRadar Altimetry\nOcean surface topography\n\n\n\nFor this training, we focus on Sentinel-1 and Sentinel-2 - the most widely used for DRR, CCA, and NRM applications.",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "sessions/session1.html#part-2-sentinel-1---synthetic-aperture-radar",
    "href": "sessions/session1.html#part-2-sentinel-1---synthetic-aperture-radar",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Part 2: Sentinel-1 - Synthetic Aperture Radar",
    "text": "Part 2: Sentinel-1 - Synthetic Aperture Radar\n\nMission Overview\n\nSentinel-1A (2014)\n\n\nSentinel-1B (2016-2022)\n\n\nSentinel-1C (2024)\n\nSentinel-1 is a C-band Synthetic Aperture Radar (SAR) mission providing all-weather, day-and-night imaging capability.\n\n\n2025 Constellation Status\n\nSentinel-1A: Operational since April 2014\nSentinel-1B: Operational 2016-2022 (anomaly ended operations)\nSentinel-1C: Launched December 2024, operational in 2025 - restoring dual-satellite coverage\n\n\n\n\n\n\n\nTipWhy SAR Matters for the Philippines\n\n\n\nThe Philippines experiences:\n\nFrequent cloud cover (tropical climate)\nMonsoon seasons with persistent rain\nNighttime disasters (earthquakes, floods)\n\nSAR sees through clouds and operates at night - critical for disaster response when optical satellites are blinded.\n\n\n\n\nKey Specifications\n\n\n\nParameter\nSpecification\n\n\n\n\nSensor Type\nC-band Synthetic Aperture Radar\n\n\nFrequency\n5.405 GHz\n\n\nOrbit\nSun-synchronous, polar\n\n\nAltitude\n693 km\n\n\nRevisit Time\n6 days (two satellites), 12 days (single)\n\n\nSwath Width\n250 km (IW mode)\n\n\nSpatial Resolution\n5 m × 20 m (range × azimuth, IW mode)\n\n\nPolarization\nDual (VV+VH or HH+HV)\n\n\n\n\n\nImaging Modes\nSentinel-1 operates in four acquisition modes:\n\nInterferometric Wide Swath (IW) - Most common, 250 km swath, 5×20 m resolution\nExtra Wide Swath (EW) - 400 km swath, maritime surveillance\nStrip Map (SM) - 80 km swath, 5 m resolution\nWave Mode (WV) - Ocean wave spectra\n\nFor land applications: IW mode is standard.\n\n\nData Products\n\nLevel-0 - Raw data\nLevel-1 SLC (Single Look Complex) - Complex imagery, for interferometry\nLevel-1 GRD (Ground Range Detected) - Multi-looked, detected, most common for analysis\nLevel-2 OCN - Ocean geophysical products\n\n\n\n\n\n\n\nNoteGRD vs SLC\n\n\n\nGRD (Ground Range Detected):\n\nMulti-looked (speckle reduced)\nReal-valued (amplitude only)\nProjected to ground range\nReady for most applications\nUse for: Change detection, flood mapping, land cover\n\nSLC (Single Look Complex):\n\nComplex values (amplitude + phase)\nSingle look (full speckle)\nRadar geometry\nRequired for interferometry\nUse for: InSAR, ground deformation, coherence analysis\n\n\n\n\n\nSentinel-1 Applications\nDisaster Risk Reduction:\n\nFlood extent mapping\nEarthquake damage assessment (coherence change)\nLandslide detection\n\nNatural Resource Management:\n\nDeforestation monitoring\nRice paddy mapping\nMangrove extent monitoring\n\nMaritime:\n\nShip detection\nOil spill monitoring\nSea ice mapping\n\n\n\nExample: Sentinel-1 Flood Mapping\nSAR backscatter characteristics:\n\nWater: Low backscatter (smooth surface) → dark\nUrban: High backscatter (corner reflectors) → bright\nVegetation: Moderate backscatter (volume scattering)\n\nDuring floods:\n\nNormally bright agricultural land becomes dark (covered by water)\nCompare pre-flood and flood images\nThreshold or classify to extract flood extent\nDeliver maps within hours of satellite overpass",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "sessions/session1.html#part-3-sentinel-2---multispectral-optical",
    "href": "sessions/session1.html#part-3-sentinel-2---multispectral-optical",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Part 3: Sentinel-2 - Multispectral Optical",
    "text": "Part 3: Sentinel-2 - Multispectral Optical\n\nMission Overview\n\nSentinel-2A (2015)\n\n\nSentinel-2B (2017)\n\n\nSentinel-2C (2025)\n\nSentinel-2 is a multispectral optical imaging mission focused on land monitoring with high spatial, spectral, and temporal resolution.\n\n\n2025 Constellation Update - Major News!\nJanuary 21, 2025: Operational transfer from Sentinel-2A to Sentinel-2C\nNew three-satellite constellation:\n\nSentinel-2C at 0° phase\nSentinel-2B at 180° phase\nSentinel-2A at 144° phase\n\nResult: Maintains 5-day revisit at the equator with enhanced resilience and data continuity.\n\n\n\n\n\n\nImportantWhat This Means for You\n\n\n\n\nMore frequent cloud-free observations\nBetter temporal coverage for change detection\nImproved likelihood of capturing events (typhoons, floods, harvest periods)\nEnhanced monitoring of fast-changing phenomena\n\n\n\n\n\nKey Specifications\n\n\n\nParameter\nSpecification\n\n\n\n\nSensor\nMultiSpectral Instrument (MSI)\n\n\nSpectral Bands\n13 (visible to SWIR)\n\n\nOrbit\nSun-synchronous, polar\n\n\nAltitude\n786 km\n\n\nSwath Width\n290 km\n\n\nRevisit Time\n5 days (at equator, three satellites)\n\n\nTile Size\n100 km × 100 km (UTM/WGS84)\n\n\n\n\n\nSpectral Bands\nSentinel-2’s 13 bands span visible, near-infrared, and shortwave infrared:\n\n\n\nBand\nName\nWavelength (nm)\nResolution (m)\nApplication\n\n\n\n\nB1\nCoastal Aerosol\n443\n60\nAerosol detection\n\n\nB2\nBlue\n490\n10\nTrue color, water\n\n\nB3\nGreen\n560\n10\nTrue color, vegetation\n\n\nB4\nRed\n665\n10\nTrue color, vegetation\n\n\nB5\nRed Edge 1\n705\n20\nVegetation health\n\n\nB6\nRed Edge 2\n740\n20\nVegetation health\n\n\nB7\nRed Edge 3\n783\n20\nVegetation health\n\n\nB8\nNIR\n842\n10\nVegetation, water\n\n\nB8A\nNarrow NIR\n865\n20\nVegetation\n\n\nB9\nWater Vapour\n945\n60\nAtmospheric correction\n\n\nB10\nCirrus\n1375\n60\nCloud detection\n\n\nB11\nSWIR 1\n1610\n20\nMoisture, snow/ice\n\n\nB12\nSWIR 2\n2190\n20\nMoisture, geology\n\n\n\nKey bands for most applications: B2, B3, B4, B8 (all 10m)\n\n\nData Products\nLevel-1C (Top-of-Atmosphere Reflectance):\n\nOrthorectified\nSub-pixel geometric registration\nRadiometric corrections applied\n100 km² tiles (UTM/WGS84)\n\nLevel-2A (Bottom-of-Atmosphere Reflectance):\n\nAtmospheric correction applied\nSurface reflectance\nScene Classification Layer (SCL) - cloud mask, shadows, snow, water\nRecommended for analysis\nOperationally produced since late 2018\n\n\n\n\n\n\n\nTipAlways Use Level-2A When Available\n\n\n\nLevel-2A products include:\n\nAtmospheric correction → more accurate surface reflectance\nScene Classification Layer → built-in cloud mask\nReady for analysis and comparison across dates\n\n\n\n\n\nCommon Spectral Indices\nNDVI (Normalized Difference Vegetation Index):\n\\[\nNDVI = \\frac{NIR - Red}{NIR + Red} = \\frac{B8 - B4}{B8 + B4}\n\\]\n\nRange: -1 to +1\nHigh values (0.6-0.9): Dense vegetation\nLow values (&lt;0.2): Bare soil, rock, water\n\nNDWI (Normalized Difference Water Index):\n\\[\nNDWI = \\frac{Green - NIR}{Green + NIR} = \\frac{B3 - B8}{B3 + B8}\n\\]\n\nHigh values: Water bodies\nLow values: Vegetation, soil\n\nNDBI (Normalized Difference Built-up Index):\n\\[\nNDBI = \\frac{SWIR - NIR}{SWIR + NIR} = \\frac{B11 - B8}{B11 + B8}\n\\]\n\nHigh values: Urban/built-up areas\n\n\n\nSentinel-2 Applications\nNatural Resource Management:\n\nLand cover/land use mapping\nForest health monitoring\nCrop type classification\nAgricultural productivity assessment\nCoral reef monitoring\n\nClimate Change Adaptation:\n\nDrought monitoring (vegetation stress)\nPhenology tracking\nSnow cover extent\nCoastal erosion\n\nDisaster Risk Reduction:\n\nPost-disaster damage assessment (burned areas)\nLandslide identification\nInfrastructure monitoring",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "sessions/session1.html#part-4-accessing-copernicus-data-2025",
    "href": "sessions/session1.html#part-4-accessing-copernicus-data-2025",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Part 4: Accessing Copernicus Data (2025)",
    "text": "Part 4: Accessing Copernicus Data (2025)\n\nCopernicus Data Space Ecosystem\nNEW in 2023-2025: The legacy Copernicus Open Access Hub (SciHub) has been replaced by the Copernicus Data Space Ecosystem.\n\n\n\n\n\n\nNoteCopernicus Data Space Ecosystem\n\n\n\nURL: https://dataspace.copernicus.eu\nFeatures:\n\nUnified access to all Sentinel data\nInteractive data browser and viewer\nAPI access for programmatic download\nCloud processing capabilities\nSentiBoard dashboard (October 2025 update)\n\n\n\n\n\nSentiBoard - Real-Time Mission Insights\nSentiBoard is an interactive dashboard providing real-time insights into Sentinel missions:\n\nSatellite orbital status\nData acquisition schedules\nProcessing baseline updates\nSystem performance metrics\nData availability status\n\nAccess: Via the Data Availability page on Copernicus Data Space Ecosystem\n\n\nAlternative Access Methods\nBeyond the Copernicus Data Space, you can access Sentinel data through:\n\nGoogle Earth Engine - Cloud-based analysis platform (covered in Session 4)\nAWS Open Data - Sentinel-2 on Amazon S3\nMicrosoft Planetary Computer - Cloud-native geospatial platform\nASF DAAC - Alaska Satellite Facility for Sentinel-1\nCopPhil Mirror Site - Coming soon for the Philippines!",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "sessions/session1.html#part-5-the-philippine-eo-ecosystem",
    "href": "sessions/session1.html#part-5-the-philippine-eo-ecosystem",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Part 5: The Philippine EO Ecosystem",
    "text": "Part 5: The Philippine EO Ecosystem\n\nOverview\nThe Philippines is building a robust Earth observation ecosystem with multiple agencies and platforms providing complementary data and services. These local resources enhance Copernicus data with Philippine-specific context, ground truth, and operational applications.\n\n\n\nPhilippine Space Agency (PhilSA)\n\n\nPhilSA\n\n\nPhilippine Space Agency\nEstablished: 2019 Website: https://philsa.gov.ph Email: info@philsa.gov.ph\nMandate:\n\nCentral civilian space agency of the Philippines\nFormulates and implements space policy\nPromotes space science and technology development\nCoordinates national space activities\n\nRole in CopPhil:\n\nProgramme co-chair with DOST\nHost of Copernicus Mirror Site\nProvider of training and capacity building\n\n\n\n\nSIYASAT Data Portal\nSIYASAT is PhilSA’s secure data archive, visualization, and distribution system.\nKey Features:\n\nData Source: Primarily NovaSAR-1 satellite (S-band SAR)\nCoverage: Maritime and terrestrial monitoring\nData Types:\n\nRadar images (SAR)\nAIS (Automatic Identification System) ship data\nDerived products\n\n\nApplications:\n\nMaritime domain awareness\nIllegal fishing monitoring\nPort and harbor monitoring\nCoastal zone management\n\nAccess: Through PhilSA official channels and partner agreements\n\n\n2025 PhilSA Initiatives\nSpace Business Innovation Challenge (SBIC) 2025:\n\nEmpowers Filipino innovators\nBuild solutions using free satellite data\nAccess to EO, weather, and environmental datasets\nSupport for startup development\n\nTraining Course on Downstream Data Utilization:\n\nDates: June 23-27, 2025\nLocation: Mandaluyong City\nFocus: Practical application of satellite data for various sectors\n\n\n\nCOARE Infrastructure\nCOARE (Computing and Archiving Research Environment) provides:\n\nHigh-performance computing facilities\nData archiving services\nScience cloud infrastructure\nSupport for data-intensive research\n\nRelevance: Critical for processing large EO datasets and training AI/ML models\n\n\n\n\nNAMRIA (National Mapping and Resource Information Authority)\n\n\nNAMRIA\n\n\nNational Mapping and Resource Information Authority\nEstablished: 1987 Website: https://www.geoportal.gov.ph\nMandate:\n\nNational mapping agency\nRepository of geospatial data\nCartographic and hydrographic services\nLand administration support\n\nRelevance for EO:\n\nAuthoritative reference data for validation\nBase maps for context and visualization\nHazard maps for risk assessment\nLand cover data for change detection\n\n\n\n\nNAMRIA Geoportal\nMain Portal: https://www.geoportal.gov.ph\nAvailable Data:\n\nBase Maps: Multiscale topographic maps (1:50,000, 1:250,000)\nAdministrative Boundaries: Provinces, municipalities, barangays\nInfrastructure: Roads, bridges, facilities\nThematic Layers: Land cover, geology, soil\nHazard Maps: Available through HazardHunterPH\n\n\n\nLand Cover Mapping Project\nPortal: https://land-cover-mapping-project-namria.hub.arcgis.com\nOfferings:\n\nNational land cover datasets\nTime series land cover (historical and recent)\nMultiple format downloads: CSV, KML, Zip, GeoJSON, GeoTIFF, PNG\nWeb services for GIS integration\n\nApplications:\n\nBaseline data for change detection\nTraining data for land cover classification\nValidation of EO-derived products\nPolicy and planning support\n\n\n\nHazardHunterPH\nURL: https://hazardhunter.georisk.gov.ph/map\nPurpose: National hazard assessment and visualization portal\nHazard Types:\n\nEarthquake-related: Ground shaking, liquefaction potential\nTsunami: Inundation zones\nLandslide: Susceptibility maps\nFlood: Susceptibility (to be enhanced with real-time EO)\n\nIntegration with EO:\n\nReference hazard zones for disaster mapping\nValidation of satellite-derived flood/landslide extents\nRisk-informed prioritization of monitoring areas\n\n\n\n\n\nDOST-ASTI AI Initiatives\n\n\nASTI\n\n\nDepartment of Science and Technology - Advanced Science and Technology Institute\nWebsite: https://asti.dost.gov.ph\nFocus:\n\nICT research and development\nRemote sensing and AI applications\nSoftware and systems development\nTechnology innovation and deployment\n\nNational Investment: P2.6 billion AI budget (until 2028) supporting government, academia, and industry\n\n\n\nSkAI-Pinas (Sky Artificial Intelligence Program)\nLaunched: 2021 prototype evolved into flagship R&D program Funding: DOST-PCIEERD\nMission: Bridge the gap between massive remote sensing data and sustainable processing frameworks through AI democratization.\nImpact:\n\nSupports over 300 institutions:\n\nState universities and colleges\nSMEs\nResearch teams\nLocal government units\n\n\nComponents:\n\nAI Knowledge Base: Experts, protocols, best practices\nAI Repository: Pre-trained models and labeled training images\nProcessing Infrastructure: Scalable compute for large-scale analysis\n\nVision: Make AI “part of daily decision-making and national progress” across the Philippines\n\n\nDIMER (Democratized Intelligent Model Exchange Repository)\nPurpose: AI model hub for democratizing access to optimized AI models\nKey Features:\n\nDigital “model store” with ready-to-use AI models\nModel sharing platform addressing Filipino-specific challenges\nLowers barriers to AI adoption by enabling model reuse\n\nTarget Users:\n\nAI researchers\nEngineers\nDomain experts (agriculture, disaster, environment)\nEnthusiasts and hobbyists\n\nApplications:\n\nLandslide detection from satellite imagery\nTraffic pattern analysis\nCrop health monitoring\nFlood risk assessment\nBuilding/infrastructure mapping\n\nImpact: Eliminates need to train models from scratch - accelerates deployment\n\n\nAIPI (AI Processing Interface)\nDeveloped by: ALaM-LSI team Purpose: Streamline large-scale remote sensing processing tasks\nCapabilities:\n\nLarge-scale satellite image processing\nBatch inference on AI models\nDistributed computing coordination\nReduces computational barriers for users\n\nHow It Works:\n\nUpload satellite imagery\nSelect AI model from DIMER\nAIPI orchestrates processing on backend infrastructure\nDownload results (classifications, detections, predictions)\n\nBenefit: Users don’t need powerful local computers or deep coding expertise\n\n\nALaM Project (Automated Labeling Machine)\nPurpose: Address the training data scarcity bottleneck\nFeatures:\n\nAutomated data labeling using AI\nCrowdsourcing capabilities for human-in-the-loop labeling\nQuality control mechanisms\nIntegration with DIMER for model training\n\nWhy It Matters: Creating labeled training data is time-consuming and expensive - ALaM dramatically reduces this burden.\n\n\nDATOS (Remote Sensing and Data Science Help Desk)\nFunction: Rapid analytics support service, particularly during disasters\nApplications:\n\nReal-time flood mapping from satellite imagery\nPost-disaster damage assessment\nCrop mapping (e.g., sugarcane via radar temporal signatures)\nInfrastructure monitoring\n\nOperational Model: On-demand analysis responding to agency requests\n\n\n\n\nPAGASA (Philippine Atmospheric, Geophysical and Astronomical Services Administration)\n\n\nPAGASA\n\n\nPAGASA\nFunction: National meteorological and hydrological agency\nWebsite: http://www.pagasa.dost.gov.ph\nData Offerings:\n\nWeather forecasts\nTyphoon tracks and forecasts\nRainfall data (historical and real-time)\nClimate projections\nENSO (El Niño / La Niña) monitoring\n\n\n\nIntegration with EO:\n\nRainfall + SAR flood mapping → Validate flood models\nDrought indices + NDVI time series → Agricultural drought monitoring\nTyphoon tracks + damage assessment → Post-disaster prioritization\nClimate data + land cover change → Climate change attribution studies",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "sessions/session1.html#part-6-synergies---combining-european-and-philippine-eo-data",
    "href": "sessions/session1.html#part-6-synergies---combining-european-and-philippine-eo-data",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Part 6: Synergies - Combining European and Philippine EO Data",
    "text": "Part 6: Synergies - Combining European and Philippine EO Data\n\nWhy Combine Data Sources?\nEuropean Copernicus data provides:\n\nGlobal coverage\nHigh temporal frequency\nConsistent data quality\nLong-term archives\nFree and open access\n\nPhilippine platforms provide:\n\nNational reference datasets (base maps, boundaries)\nLocal hazard information\nGround truth and validation data\nOperational AI models tuned for Philippine conditions\nFilipino language support and localized services\n\n\n\nExample Use Cases\n\nFlood Mapping After Typhoon\n\nSentinel-1 SAR → Detect flood extent under clouds\nPAGASA rainfall data → Understand precipitation drivers\nNAMRIA HazardHunterPH → Identify pre-existing high-risk zones\nDOST-ASTI DATOS → Rapid processing and delivery to disaster authorities\n\n\n\nLand Cover Change Detection\n\nSentinel-2 time series → Multi-temporal optical imagery\nNAMRIA land cover basemap → Reference classification\nDIMER models → Apply pre-trained classification model\nAIPI → Process large areas efficiently\n\n\n\nAgricultural Drought Monitoring\n\nSentinel-2 NDVI → Vegetation health indicator\nPAGASA rainfall/SPEI → Meteorological drought indices\nSkAI-Pinas models → Predict crop stress levels\nDOST agencies → Deliver alerts to DA and LGUs",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "sessions/session1.html#part-7-copphil-infrastructure",
    "href": "sessions/session1.html#part-7-copphil-infrastructure",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Part 7: CopPhil Infrastructure",
    "text": "Part 7: CopPhil Infrastructure\n\nCopPhil Mirror Site\nStatus: Under development, planned operational by 2025\nPurpose: Philippines-based data center hosting a mirror of Copernicus data focused on the Philippine region.\nBenefits:\n\nFaster download speeds - Local hosting reduces latency\nReliable access - Reduced dependence on international bandwidth\nCapacity building - Local expertise in data management\nDisaster resilience - Local backup during regional crises\n\nTechnical Details:\n\nFully scalable platform\nCo-located with PhilSA infrastructure\nSupport from CloudFerro (Copernicus Data Space provider)\nCoverage: Sentinel data for Philippines and Southeast Asia\n\nAccess: To be announced - likely integrated with PhilSA SIYASAT and Digital Space Campus\n\n\nCopPhil Digital Space Campus\nPurpose: Online portal for training materials and continued learning\nContents:\n\nAll CopPhil training modules (this course included)\nJupyter notebooks and code examples\nDatasets and sample imagery\nRecorded lecture videos\nDocumentation and guides\nCommunity forums (planned)\n\nAccess: Through PhilSA or CopPhil programme website\nYour Role: As participants in this training, you will contribute to and benefit from this growing knowledge repository.",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "sessions/session1.html#activity-exploring-the-data-platforms",
    "href": "sessions/session1.html#activity-exploring-the-data-platforms",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Activity: Exploring the Data Platforms",
    "text": "Activity: Exploring the Data Platforms\n\n\n\n\n\n\nTipHands-On Exploration (15 minutes)\n\n\n\nTask: Navigate to the following platforms and explore their interfaces:\n\nCopernicus Data Space Ecosystem\n\nGo to https://dataspace.copernicus.eu\nExplore the Browser\nFind the SentiBoard (Data Availability page)\nSearch for a recent Sentinel-2 image over the Philippines\n\nPhilSA Website\n\nVisit https://philsa.gov.ph\nExplore recent news and training opportunities\n\nNAMRIA Geoportal\n\nGo to https://www.geoportal.gov.ph\nBrowse available map layers\nVisit HazardHunterPH\n\nDOST-ASTI\n\nVisit https://asti.dost.gov.ph\nRead about SkAI-Pinas, DIMER, and AIPI initiatives\n\n\nDiscussion: What types of data or services are most relevant to your work?",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "sessions/session1.html#key-takeaways",
    "href": "sessions/session1.html#key-takeaways",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\n\n\n\n\nImportantSession 1 Summary\n\n\n\n\nCopernicus is the world’s leading Earth observation programme, providing free Sentinel data globally\nSentinel-1 SAR sees through clouds and at night - essential for tropical disaster monitoring\nSentinel-2 optical offers 13 spectral bands at 10-60m resolution with 5-day revisit (2025: three satellites)\nCopernicus Data Space Ecosystem (2025) is the new unified access portal with SentiBoard monitoring\nPhilippines has a rich EO ecosystem: PhilSA (SIYASAT), NAMRIA (Geoportal, HazardHunterPH), DOST-ASTI (SkAI-Pinas, DIMER, AIPI)\nP2.6 billion DOST AI investment (until 2028) supports democratized AI for EO across 300+ institutions\nCombining European and Philippine data creates powerful synergies for DRR, CCA, and NRM\nCopPhil Mirror Site and Digital Space Campus will provide local data access and sustainable capacity building",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "sessions/session1.html#further-reading",
    "href": "sessions/session1.html#further-reading",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Further Reading",
    "text": "Further Reading\n\nCopernicus Programme Overview\nSentinel-1 User Handbook\nSentinel-2 User Handbook\nPhilSA Strategic Plan\nNAMRIA Strategic Plan\nDOST-ASTI SkAI-Pinas Information\n\n\n\n\n\n← Back\n\n\nDay 1 Home\n\n\n\n\nNext Session\n\n\nSession 2: AI/ML Fundamentals →",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "sessions/session3.html",
    "href": "sessions/session3.html",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "",
    "text": "Duration: 2 hours | Format: Hands-on Coding | Platform: Google Colaboratory",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "sessions/session3.html#session-overview",
    "href": "sessions/session3.html#session-overview",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Session Overview",
    "text": "Session Overview\nThis hands-on session teaches you how to work with geospatial data in Python - the foundation of Earth Observation workflows. You’ll use Google Colab (no installation needed!), learn vector data operations with GeoPandas, and master raster processing with Rasterio. By the end, you’ll be able to load Philippine administrative boundaries, visualize them, read Sentinel-2 imagery, calculate NDVI, and combine vector and raster operations.\n\nLearning Objectives\nBy the end of this session, you will be able to:\n\nSet up Google Colaboratory for geospatial analysis\nMount Google Drive for data access and storage\nInstall Python geospatial libraries (GeoPandas, Rasterio)\nRead and inspect vector data (shapefiles, GeoJSON)\nQuery and filter geospatial DataFrames\nVisualize vector data with maps\nOpen and examine raster files and their metadata\nRead raster bands into NumPy arrays\nCalculate spectral indices (NDVI, NDWI)\nCreate RGB composites from multispectral imagery\nClip rasters to vector boundaries\nCombine vector and raster operations for analysis",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "sessions/session3.html#part-1-setting-up-google-colaboratory",
    "href": "sessions/session3.html#part-1-setting-up-google-colaboratory",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Part 1: Setting Up Google Colaboratory",
    "text": "Part 1: Setting Up Google Colaboratory\n\nWhat is Google Colab?\nGoogle Colaboratory (Colab) is a free cloud-based Jupyter notebook environment that allows you to:\n\nWrite and execute Python code in your browser\nAccess free GPU/TPU for machine learning\nCollaborate with others in real-time\nSave notebooks to Google Drive\nNo local installation required!\n\nPerfect for this training: Everyone has the same environment, no dependency issues, accessible from anywhere.\n\n\nCreating Your First Colab Notebook\n\n\n\n\n\n\nNoteAccess Colab\n\n\n\nURL: https://colab.research.google.com\nRequirements: - Google account - Modern web browser (Chrome, Firefox, Safari, Edge) - Stable internet connection\n\n\nSteps:\n\nGo to colab.research.google.com\nSign in with your Google account\nClick File → New Notebook\nRename: File → Rename → “Day1_Session3_Geospatial_Python”\n\n\n\nUnderstanding the Colab Interface\nKey components:\n\nCode cells: Where you write Python code (click or press Enter to edit)\nText cells: Markdown for documentation (Insert → Text cell)\nRun button: ▶ Execute current cell (or press Shift+Enter)\nRuntime menu: Manage execution environment\nTable of Contents: Navigate long notebooks (left sidebar icon)\n\nTry it: Create a code cell and run:\nprint(\"Hello from Google Colab!\")\nprint(\"This is the CopPhil EO AI/ML Training\")\nPress Shift+Enter to execute. You should see the output below the cell.\n\n\nConnecting Google Drive\nWhy mount Google Drive?\n\nAccess data files stored in Drive\nSave outputs permanently\nShare data with collaborators\n\nMount Drive:\nfrom google.colab import drive\ndrive.mount('/content/drive')\nWhat happens:\n\nClick the link that appears\nSelect your Google account\nClick “Allow” to grant access\nCopy the authorization code\nPaste into the input field and press Enter\n\nVerification:\nimport os\nos.listdir('/content/drive/MyDrive')\nYou should see your Google Drive folders listed!\n\n\n\n\n\n\nTipOrganizing Your Data\n\n\n\nCreate a folder structure in Google Drive:\nMyDrive/\n  CopPhil_Training/\n    data/\n      vector/          # Shapefiles, GeoJSON\n      raster/          # Satellite imagery\n    outputs/           # Processed results\n    notebooks/         # Saved Colab notebooks\nThis keeps your training materials organized.",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "sessions/session3.html#part-2-installing-geospatial-libraries",
    "href": "sessions/session3.html#part-2-installing-geospatial-libraries",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Part 2: Installing Geospatial Libraries",
    "text": "Part 2: Installing Geospatial Libraries\n\nRequired Libraries\nGoogle Colab comes with many libraries pre-installed, but specialized geospatial tools need installation.\nCore libraries we’ll use:\n\n\n\nLibrary\nPurpose\n\n\n\n\nGeoPandas\nVector data (shapefiles, GeoJSON, polygons, points)\n\n\nRasterio\nRaster data (GeoTIFF, satellite imagery)\n\n\nShapely\nGeometric operations (included with GeoPandas)\n\n\nMatplotlib\nVisualization\n\n\nNumPy\nArray operations (pre-installed)\n\n\n\n\n\nInstallation\nRun this cell (may take 1-2 minutes):\n# Install geospatial libraries\n!pip install geopandas rasterio fiona shapely pyproj -q\n\nprint(\"Installation complete! ✓\")\nThe -q flag makes installation quiet (less output).\nIf you see warnings: Usually safe to ignore. If errors occur, try:\n!pip install --upgrade geopandas rasterio\n\n\nVerify Installation\nimport geopandas as gpd\nimport rasterio\nfrom rasterio.plot import show\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nprint(\"✓ GeoPandas version:\", gpd.__version__)\nprint(\"✓ Rasterio version:\", rasterio.__version__)\nprint(\"✓ All libraries imported successfully!\")\n\n\n\n\n\n\nWarningRuntime Restart\n\n\n\nOccasionally, Colab may ask you to restart the runtime after installing libraries:\nRuntime → Restart runtime\nThen re-run your import cell. This is normal!",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "sessions/session3.html#part-3-python-basics-refresher",
    "href": "sessions/session3.html#part-3-python-basics-refresher",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Part 3: Python Basics Refresher",
    "text": "Part 3: Python Basics Refresher\nQuick recap of Python essentials for geospatial work:\n\nData Types\n# Numbers\npopulation = 1780148        # Integer\narea_km2 = 42.88           # Float\n\n# Strings\ncity = \"Manila\"\nprovince = \"Metro Manila\"\n\n# Lists (ordered collections)\nregions = [\"Luzon\", \"Visayas\", \"Mindanao\"]\ncoordinates = [14.5995, 120.9842]  # [latitude, longitude]\n\n# Dictionaries (key-value pairs)\nlocation_info = {\n    \"city\": \"Quezon City\",\n    \"population\": 2960048,\n    \"region\": \"NCR\"\n}\n\nprint(f\"{city} has population {population:,}\")\nprint(f\"Regions: {regions}\")\nprint(f\"Coordinates: {coordinates}\")\n\n\nControl Structures\n# If statements\ncloud_cover = 15\n\nif cloud_cover &lt; 10:\n    quality = \"Excellent\"\nelif cloud_cover &lt; 30:\n    quality = \"Good\"\nelse:\n    quality = \"Poor\"\n\nprint(f\"Cloud cover {cloud_cover}%: {quality} for optical imagery\")\n\n# For loops\nprovinces = [\"Palawan\", \"Zambales\", \"Quezon\"]\nfor province in provinces:\n    print(f\"Processing {province}...\")\n\n# List comprehension (Pythonic way)\nprovince_lengths = [len(p) for p in provinces]\nprint(\"Province name lengths:\", province_lengths)\n\n\nFunctions\ndef calculate_ndvi(nir, red):\n    \"\"\"Calculate Normalized Difference Vegetation Index.\"\"\"\n    ndvi = (nir - red) / (nir + red + 1e-10)  # Add small value to avoid division by zero\n    return ndvi\n\n# Example usage\nnir_value = 0.8\nred_value = 0.2\nresult = calculate_ndvi(nir_value, red_value)\nprint(f\"NDVI: {result:.3f}\")\nKey Python concepts for geospatial:\n\nIndentation matters: Use 4 spaces to define code blocks\n0-indexed: First element is list[0], not list[1]\nMethod chaining: gdf.filter(...).plot()\nContext managers: with rasterio.open() as src: (auto-cleanup)",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "sessions/session3.html#part-4-vector-data-with-geopandas",
    "href": "sessions/session3.html#part-4-vector-data-with-geopandas",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Part 4: Vector Data with GeoPandas",
    "text": "Part 4: Vector Data with GeoPandas\n\nWhat is Vector Data?\nVector data represents discrete features:\n\nPoints: Cities, field sites, observation locations\nLines: Roads, rivers, transects\nPolygons: Administrative boundaries, land parcels, watersheds\n\nCommon formats: Shapefile (.shp), GeoJSON, KML, GeoPackage\n\n\nLoading Philippine Administrative Boundaries\nLet’s load provincial boundaries of the Philippines:\n# Option 1: From Google Drive (if you uploaded data)\nprovinces = gpd.read_file('/content/drive/MyDrive/CopPhil_Training/data/vector/philippines_provinces.shp')\n\n# Option 2: From URL (using sample data)\nurl = \"https://raw.githubusercontent.com/altcoder/philippines-json-maps/master/geojson/provinces/hires/BOHOL.json\"\nsample_province = gpd.read_file(url)\n\n# For this example, let's use the sample\ngdf = sample_province\nprint(\"Loaded successfully! ✓\")\n\nPhilippine Geospatial Data Sources:\n\nNAMRIA Geoportal: Official administrative boundaries\nPhilGIS: Open-source Philippine GIS data\nGADM: Global administrative boundaries\nOpenStreetMap: Roads, buildings, POIs\n\nDownload shapefiles and upload to your Google Drive for this session.\n\n\n\nInspecting a GeoDataFrame\nView first rows:\ngdf.head()\nThis shows: - Attribute columns (NAME, PROVINCE, REGION, etc.) - geometry column: The shapes themselves (Polygon or MultiPolygon)\nCheck coordinate reference system:\nprint(\"CRS:\", gdf.crs)\nCommon CRS values: - EPSG:4326 - WGS84 latitude/longitude (geographic) - EPSG:32651 - UTM Zone 51N (projected, meters) - EPSG:3123 - PRS92 Philippines Zone III\nGet basic info:\nprint(f\"Number of features: {len(gdf)}\")\nprint(f\"Geometry type: {gdf.geometry.type[0]}\")\nprint(f\"Bounds: {gdf.total_bounds}\")  # [minx, miny, maxx, maxy]\nprint(f\"\\nColumns: {list(gdf.columns)}\")\nSummary statistics:\ngdf.info()\n\n\nFiltering and Querying\nFilter by attribute:\n# If using multi-province dataset:\n# luzon_provinces = gdf[gdf['ISLAND'] == 'LUZON']\n\n# Filter by area (if area column exists)\n# large_provinces = gdf[gdf['AREA_KM2'] &gt; 5000]\n\n# Example: Select specific province by name\ntarget = gdf[gdf['NAME'].str.contains('Bohol', case=False)]\nprint(f\"Selected: {target['NAME'].values}\")\nSpatial filtering:\n# Check if geometries are valid\nprint(f\"Valid geometries: {gdf.geometry.is_valid.all()}\")\n\n# Calculate centroids\ngdf['centroid'] = gdf.geometry.centroid\nprint(f\"Centroid of first feature: {gdf['centroid'].iloc[0]}\")\n\n# Calculate area (will be in units of CRS - degrees for EPSG:4326)\ngdf['area'] = gdf.geometry.area\nprint(f\"Areas: {gdf['area'].head()}\")\n\n\n\n\n\n\nTipWorking with CRS\n\n\n\nTo project to meters for accurate area calculation:\n# Reproject to UTM Zone 51N (appropriate for Philippines)\ngdf_utm = gdf.to_crs('EPSG:32651')\ngdf_utm['area_km2'] = gdf_utm.geometry.area / 1_000_000  # Convert m² to km²\nprint(f\"Province area: {gdf_utm['area_km2'].iloc[0]:.2f} km²\")\n\n\n\n\nVisualizing Vector Data\nSimple plot:\ngdf.plot(figsize=(8, 8), edgecolor='black', facecolor='lightblue')\nplt.title(\"Bohol Province, Philippines\")\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.show()\nStyled plot with colors:\n# If you have multiple provinces with a classification column:\n# gdf.plot(column='REGION', legend=True, figsize=(10, 10),\n#          cmap='Set3', edgecolor='black', linewidth=0.5)\n# plt.title(\"Philippines Provinces by Region\")\n\n# For single feature, style it:\nfig, ax = plt.subplots(figsize=(10, 10))\ngdf.plot(ax=ax, facecolor='#90EE90', edgecolor='darkgreen', linewidth=2, alpha=0.7)\ngdf['centroid'].plot(ax=ax, color='red', markersize=50)\nplt.title(\"Bohol Province with Centroid\", fontsize=16)\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.grid(True, alpha=0.3)\nplt.show()\nInteractive exploration:\n# Display attribute table interactively\ngdf[['NAME', 'geometry']]\n\n\nExample: Create Area of Interest (AOI)\n# Create a simple polygon AOI (example coordinates)\nfrom shapely.geometry import Polygon\n\n# Bounding box coordinates (minx, miny, maxx, maxy)\naoi_coords = [\n    (123.5, 9.5),   # Southwest corner\n    (125.0, 9.5),   # Southeast corner\n    (125.0, 11.0),  # Northeast corner\n    (123.5, 11.0),  # Northwest corner\n    (123.5, 9.5)    # Close polygon\n]\n\naoi_polygon = Polygon(aoi_coords)\naoi_gdf = gpd.GeoDataFrame([1], geometry=[aoi_polygon], crs='EPSG:4326')\naoi_gdf.columns = ['id', 'geometry']\n\n# Visualize AOI with province\nfig, ax = plt.subplots(figsize=(10, 8))\ngdf.plot(ax=ax, facecolor='lightgray', edgecolor='black')\naoi_gdf.plot(ax=ax, facecolor='none', edgecolor='red', linewidth=3)\nplt.title(\"Province with Area of Interest (Red Box)\")\nplt.show()\n\nprint(\"AOI created successfully! ✓\")",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "sessions/session3.html#part-5-raster-data-with-rasterio",
    "href": "sessions/session3.html#part-5-raster-data-with-rasterio",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Part 5: Raster Data with Rasterio",
    "text": "Part 5: Raster Data with Rasterio\n\nWhat is Raster Data?\nRaster data is a grid of pixels (cells), each with a value:\n\nSatellite imagery: Each pixel = reflectance values\nDEMs: Each pixel = elevation\nTemperature maps: Each pixel = temperature value\n\nCommon formats: GeoTIFF (.tif), NetCDF (.nc), HDF (.hdf)\nRaster structure:\n┌─────┬─────┬─────┬─────┐\n│ 120 │ 130 │ 125 │ 118 │  ← Row 1\n├─────┼─────┼─────┼─────┤\n│ 115 │ 140 │ 135 │ 122 │  ← Row 2\n├─────┼─────┼─────┼─────┤\n│ 110 │ 125 │ 130 │ 119 │  ← Row 3\n└─────┴─────┴─────┴─────┘\n  Col1  Col2  Col3  Col4\nEach cell has: - Value: Reflectance, elevation, class, etc. - Location: Defined by geotransform + CRS - Size: Spatial resolution (e.g., 10m)\n\n\nOpening a Raster File\nExample: Sentinel-2 imagery subset\n# Sample raster path (adjust to your data)\nraster_path = '/content/drive/MyDrive/CopPhil_Training/data/raster/sentinel2_bohol_subset.tif'\n\n# For demonstration, we'll create sample raster metadata\n# In real workflow, you'd open your actual Sentinel-2 file\n\n# Open raster\nsrc = rasterio.open(raster_path)\n\n# View metadata\nprint(\"Raster Metadata:\")\nprint(f\"  Width (pixels): {src.width}\")\nprint(f\"  Height (pixels): {src.height}\")\nprint(f\"  Number of bands: {src.count}\")\nprint(f\"  Data type: {src.dtypes[0]}\")\nprint(f\"  CRS: {src.crs}\")\nprint(f\"  Bounds: {src.bounds}\")\nprint(f\"  Resolution: {src.res}\")  # (x_res, y_res) in CRS units\nprint(f\"  Nodata value: {src.nodata}\")\n\nsrc.close()\n\n\n\n\n\n\nNoteRaster Metadata Explanation\n\n\n\nFor a Sentinel-2 10m band over Bohol:\n\nWidth/Height: 5000 x 5000 pixels → 50km x 50km area\nBands: 4 (Blue, Green, Red, NIR if subset)\nData type: uint16 (0-65535 range)\nCRS: EPSG:32651 (UTM Zone 51N)\nResolution: (10.0, -10.0) meters (negative y = north-up)\nNodata: 0 or 65535 (no valid data)\n\n\n\nBetter pattern: Context manager (auto-closes file)\nwith rasterio.open(raster_path) as src:\n    print(f\"Opened: {src.name}\")\n    print(f\"Bands: {src.count}\")\n    # Work with src here\n# File automatically closed after 'with' block\n\n\nReading Raster Data into Arrays\nRead a single band:\nwith rasterio.open(raster_path) as src:\n    # Read band 1 (Rasterio uses 1-indexing for bands)\n    band1 = src.read(1)\n\nprint(f\"Band 1 shape: {band1.shape}\")  # (height, width)\nprint(f\"Data type: {band1.dtype}\")\nprint(f\"Min value: {band1.min()}, Max value: {band1.max()}\")\nprint(f\"Mean value: {band1.mean():.2f}\")\nRead multiple bands:\nwith rasterio.open(raster_path) as src:\n    # Read all bands as 3D array (bands, height, width)\n    all_bands = src.read()\n\n    # Or read specific bands\n    blue = src.read(1)   # Band 1: Blue\n    green = src.read(2)  # Band 2: Green\n    red = src.read(3)    # Band 3: Red\n    nir = src.read(4)    # Band 4: NIR\n\nprint(f\"All bands shape: {all_bands.shape}\")  # (4, height, width)\nFor Sentinel-2, typical band order in L2A products:\n\n\n\nBand #\nWavelength\nName\nResolution\n\n\n\n\n1\n490 nm\nBlue (B2)\n10m\n\n\n2\n560 nm\nGreen (B3)\n10m\n\n\n3\n665 nm\nRed (B4)\n10m\n\n\n4\n842 nm\nNIR (B8)\n10m\n\n\n\n\n\nCalculating Spectral Indices\n\nNDVI (Normalized Difference Vegetation Index)\nwith rasterio.open(raster_path) as src:\n    red = src.read(3).astype(float)\n    nir = src.read(4).astype(float)\n\n# Calculate NDVI\n# NDVI = (NIR - Red) / (NIR + Red)\nndvi = (nir - red) / (nir + red + 1e-10)  # Small value prevents division by zero\n\nprint(f\"NDVI range: {ndvi.min():.3f} to {ndvi.max():.3f}\")\nprint(f\"NDVI mean: {ndvi.mean():.3f}\")\n\n# Visualize NDVI\nplt.figure(figsize=(10, 8))\nplt.imshow(ndvi, cmap='RdYlGn', vmin=-1, vmax=1)\nplt.colorbar(label='NDVI', shrink=0.8)\nplt.title(\"NDVI - Vegetation Index\")\nplt.xlabel(\"Column (pixels)\")\nplt.ylabel(\"Row (pixels)\")\nplt.show()\nNDVI interpretation:\n\n&lt;0: Water, clouds, snow\n0-0.2: Bare soil, rock, sand, urban\n0.2-0.4: Sparse vegetation, grassland\n0.4-0.7: Moderate vegetation, cropland\n&gt;0.7: Dense vegetation, healthy forest\n\n\nPhilippine Application: Rice Paddy Monitoring\n# Identify rice paddies (moderate-high NDVI during growing season)\nrice_mask = (ndvi &gt; 0.5) & (ndvi &lt; 0.85)\n\nplt.figure(figsize=(10, 8))\nplt.imshow(rice_mask, cmap='Greens')\nplt.title(\"Potential Rice Paddies (NDVI 0.5-0.85)\")\nplt.colorbar()\nplt.show()\n\nrice_pixels = rice_mask.sum()\ntotal_pixels = rice_mask.size\nprint(f\"Potential rice paddies: {rice_pixels:,} pixels ({rice_pixels/total_pixels*100:.1f}%)\")\n\n\n\nNDWI (Normalized Difference Water Index)\nwith rasterio.open(raster_path) as src:\n    green = src.read(2).astype(float)\n    nir = src.read(4).astype(float)\n\n# NDWI = (Green - NIR) / (Green + NIR)\nndwi = (green - nir) / (green + nir + 1e-10)\n\nplt.figure(figsize=(10, 8))\nplt.imshow(ndwi, cmap='Blues', vmin=-1, vmax=1)\nplt.colorbar(label='NDWI', shrink=0.8)\nplt.title(\"NDWI - Water Index\")\nplt.show()\n\n# Extract water bodies (NDWI &gt; 0.3)\nwater_mask = ndwi &gt; 0.3\nprint(f\"Water pixels: {water_mask.sum():,}\")\n\n\n\nCreating RGB Composites\nTrue color composite (Red, Green, Blue):\nwith rasterio.open(raster_path) as src:\n    red = src.read(3)\n    green = src.read(2)\n    blue = src.read(1)\n\n# Stack bands into RGB array\nrgb = np.stack([red, green, blue], axis=2)\n\n# Scale to 0-1 range for display (Sentinel-2 L2A is 0-10000)\nrgb_scaled = rgb.astype(float) / 10000.0\nrgb_scaled = np.clip(rgb_scaled, 0, 1)  # Clip any values outside 0-1\n\n# Enhance contrast (optional)\nfrom skimage import exposure\nrgb_enhanced = exposure.rescale_intensity(rgb_scaled, in_range=(0.0, 0.3), out_range=(0, 1))\n\n# Display\nplt.figure(figsize=(12, 10))\nplt.imshow(rgb_enhanced)\nplt.title(\"True Color Composite (Red-Green-Blue)\")\nplt.axis('off')\nplt.show()\nFalse color composite (NIR, Red, Green) - highlights vegetation:\nwith rasterio.open(raster_path) as src:\n    nir = src.read(4)\n    red = src.read(3)\n    green = src.read(2)\n\n# NIR-R-G composite\nfalse_color = np.stack([nir, red, green], axis=2)\nfalse_color_scaled = false_color.astype(float) / 10000.0\nfalse_color_scaled = np.clip(false_color_scaled, 0, 1)\n\nplt.figure(figsize=(12, 10))\nplt.imshow(false_color_scaled)\nplt.title(\"False Color Composite (NIR-Red-Green) - Vegetation appears red\")\nplt.axis('off')\nplt.show()\nWhy false color?\n\nVegetation: Appears bright red (high NIR reflectance)\nWater: Appears dark blue/black (absorbs NIR)\nUrban: Appears cyan/gray\nBare soil: Appears brown/tan\n\nEasier to distinguish land cover types!",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "sessions/session3.html#part-6-combining-vector-and-raster-operations",
    "href": "sessions/session3.html#part-6-combining-vector-and-raster-operations",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Part 6: Combining Vector and Raster Operations",
    "text": "Part 6: Combining Vector and Raster Operations\n\nClipping Raster to Vector Boundary\nExtract raster data only within AOI polygon:\nfrom rasterio.mask import mask\n\n# Load AOI polygon (from earlier GeoPandas section)\n# aoi_gdf = ...\n\nwith rasterio.open(raster_path) as src:\n    # Ensure CRS match\n    if aoi_gdf.crs != src.crs:\n        aoi_gdf = aoi_gdf.to_crs(src.crs)\n\n    # Get geometries in proper format\n    shapes = aoi_gdf.geometry.values\n\n    # Clip raster\n    out_image, out_transform = mask(src, shapes, crop=True)\n    out_meta = src.meta.copy()\n\n# Update metadata for clipped raster\nout_meta.update({\n    \"driver\": \"GTiff\",\n    \"height\": out_image.shape[1],\n    \"width\": out_image.shape[2],\n    \"transform\": out_transform\n})\n\nprint(f\"Original raster: {src.width} x {src.height}\")\nprint(f\"Clipped raster: {out_meta['width']} x {out_meta['height']}\")\n\n# Visualize clipped area\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Original\naxes[0].imshow(src.read(3), cmap='gray')\naxes[0].set_title(\"Original Raster\")\n\n# Clipped\naxes[1].imshow(out_image[2], cmap='gray')  # Band 3 (Red)\naxes[1].set_title(\"Clipped to AOI\")\n\nplt.tight_layout()\nplt.show()\nSave clipped raster:\noutput_path = '/content/drive/MyDrive/CopPhil_Training/outputs/clipped_raster.tif'\n\nwith rasterio.open(output_path, 'w', **out_meta) as dest:\n    dest.write(out_image)\n\nprint(f\"✓ Saved clipped raster to: {output_path}\")\n\n\nSampling Raster Values at Point Locations\nExtract pixel values at specific coordinates:\n# Example: Field survey locations\nsurvey_points = gpd.GeoDataFrame({\n    'site_id': ['Site_A', 'Site_B', 'Site_C'],\n    'geometry': gpd.points_from_xy([124.0, 124.5, 123.8], [10.0, 10.3, 9.8])\n}, crs='EPSG:4326')\n\n# Sample raster at points\nwith rasterio.open(raster_path) as src:\n    # Reproject points if needed\n    if survey_points.crs != src.crs:\n        survey_points = survey_points.to_crs(src.crs)\n\n    # Extract coordinates\n    coords = [(x, y) for x, y in zip(survey_points.geometry.x, survey_points.geometry.y)]\n\n    # Sample all bands\n    sampled_values = [x for x in src.sample(coords)]\n\n# Add to GeoDataFrame\nsurvey_points['blue'] = [v[0] for v in sampled_values]\nsurvey_points['green'] = [v[1] for v in sampled_values]\nsurvey_points['red'] = [v[2] for v in sampled_values]\nsurvey_points['nir'] = [v[3] for v in sampled_values]\n\n# Calculate NDVI at points\nsurvey_points['ndvi'] = (survey_points['nir'] - survey_points['red']) / \\\n                        (survey_points['nir'] + survey_points['red'])\n\nprint(survey_points[['site_id', 'ndvi']])",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "sessions/session3.html#part-7-complete-workflow-example",
    "href": "sessions/session3.html#part-7-complete-workflow-example",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Part 7: Complete Workflow Example",
    "text": "Part 7: Complete Workflow Example\n\nScenario: Forest Health Assessment in Philippine Protected Area\nGoal: Identify potential forest stress areas using Sentinel-2 NDVI\n# 1. Load protected area boundary\n# (Replace with actual protected area shapefile)\nprotected_area = aoi_gdf  # Using our AOI from earlier\n\n# 2. Load Sentinel-2 imagery\nwith rasterio.open(raster_path) as src:\n    # 3. Clip to protected area\n    if protected_area.crs != src.crs:\n        protected_area = protected_area.to_crs(src.crs)\n\n    shapes = protected_area.geometry.values\n    out_image, out_transform = mask(src, shapes, crop=True)\n\n    # Extract bands\n    red = out_image[2].astype(float)\n    nir = out_image[3].astype(float)\n\n# 4. Calculate NDVI\nndvi = (nir - red) / (nir + red + 1e-10)\n\n# 5. Identify stress areas (low NDVI where forest expected)\n# Assumption: Protected area should be &gt;0.6 NDVI (healthy forest)\nstress_threshold = 0.5\nstress_mask = (ndvi &lt; stress_threshold) & (ndvi &gt; 0.1)  # Exclude water/bare\n\n# 6. Visualize results\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# True color\nrgb_display = np.stack([red, green, blue], axis=2) / 10000.0\nrgb_display = np.clip(rgb_display, 0, 0.3) / 0.3  # Contrast stretch\naxes[0].imshow(rgb_display)\naxes[0].set_title(\"True Color\")\naxes[0].axis('off')\n\n# NDVI\nim1 = axes[1].imshow(ndvi, cmap='RdYlGn', vmin=0, vmax=1)\naxes[1].set_title(\"NDVI\")\naxes[1].axis('off')\nplt.colorbar(im1, ax=axes[1], shrink=0.8)\n\n# Stress areas\naxes[2].imshow(stress_mask, cmap='Reds')\naxes[2].set_title(f\"Potential Stress Areas (NDVI &lt; {stress_threshold})\")\naxes[2].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# 7. Statistics\ntotal_pixels = ndvi.size\nstress_pixels = stress_mask.sum()\narea_m2 = stress_pixels * 100  # 10m resolution → 100 m² per pixel\narea_ha = area_m2 / 10000\n\nprint(\"=\" * 50)\nprint(\"FOREST HEALTH ASSESSMENT RESULTS\")\nprint(\"=\" * 50)\nprint(f\"Protected Area: {protected_area['id'].values[0]}\")\nprint(f\"Average NDVI: {ndvi.mean():.3f}\")\nprint(f\"Stress pixels: {stress_pixels:,} ({stress_pixels/total_pixels*100:.1f}%)\")\nprint(f\"Stress area: {area_ha:.1f} hectares\")\nprint(\"=\" * 50)",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "sessions/session3.html#key-takeaways",
    "href": "sessions/session3.html#key-takeaways",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\n\n\n\n\nImportantSession 3 Summary\n\n\n\nGoogle Colab: - Cloud-based Python environment, no installation needed - Mount Google Drive for data access - Install geospatial libraries: !pip install geopandas rasterio\nGeoPandas (Vector Data): - Read shapefiles/GeoJSON: gpd.read_file() - Filter by attributes: gdf[gdf['column'] == value] - Visualize: gdf.plot() - Check/transform CRS: gdf.crs, gdf.to_crs()\nRasterio (Raster Data): - Open rasters: rasterio.open(path) - Read bands: src.read(band_number) - Arrays are NumPy: all array operations work - Calculate indices: NDVI = (NIR - Red) / (NIR + Red)\nCombining Vector + Raster: - Clip rasters: rasterio.mask.mask(src, shapes, crop=True) - Sample at points: src.sample(coordinates) - Match CRS before spatial operations!\nNext: Session 4 will leverage Google Earth Engine to access petabytes of Sentinel data without downloading!",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "sessions/session3.html#practice-exercises",
    "href": "sessions/session3.html#practice-exercises",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Practice Exercises",
    "text": "Practice Exercises\n\n\n\n\n\n\nTipTry These Challenges\n\n\n\nExercise 1: Load Your Own Boundary\nDownload a shapefile of your province from NAMRIA Geoportal and visualize it in Colab.\nExercise 2: Calculate NDWI\nModify the NDVI code to calculate NDWI and identify water bodies in the imagery.\nExercise 3: Multi-temporal NDVI\nIf you have two Sentinel-2 images (dry season, wet season), calculate NDVI for both and create a change map.\nExercise 4: Zonal Statistics\nCalculate average NDVI for each municipality using rasterstats library.\nBonus: RGB Composite from GEE Export\nExport a Sentinel-2 composite from Google Earth Engine (Session 4) and visualize it using today’s techniques!",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "sessions/session3.html#further-reading",
    "href": "sessions/session3.html#further-reading",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Further Reading",
    "text": "Further Reading\n\nGeoPandas\n\nOfficial Documentation\nGeoPandas Tutorial (DataCamp)\nGeoPandas Examples Gallery\n\n\n\nRasterio\n\nOfficial Documentation\nRasterio Quickstart\nPython Raster Tutorial (WUR)\n\n\n\nCombined Workflows\n\nCarpentries Geospatial Python\nCropping Rasters with Vector Boundaries",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "sessions/session3.html#jupyter-notebook",
    "href": "sessions/session3.html#jupyter-notebook",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Jupyter Notebook",
    "text": "Jupyter Notebook\n\n\n\n\n\n\nNoteAccess the Interactive Notebook\n\n\n\nA complete Jupyter notebook with all code examples from this session is available:\nOpen Notebook 1: Python Geospatial Data →\nThis notebook includes: - All code examples ready to run - Additional exercises - Sample datasets - Detailed comments and explanations\n\n\n\n\n\n\n← Previous\n\n\nSession 2: AI/ML Fundamentals\n\n\n\n\nNext Session\n\n\nSession 4: Google Earth Engine →",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Day 1",
    "section": "",
    "text": "Advanced training for Philippine EO professionals on AI/ML applications for Disaster Risk Reduction, Climate Change Adaptation, and Natural Resource Management",
    "crumbs": [
      "Home",
      "Getting Started",
      "Welcome to Day 1"
    ]
  },
  {
    "objectID": "index.html#about-this-training",
    "href": "index.html#about-this-training",
    "title": "Welcome to Day 1",
    "section": "About This Training",
    "text": "About This Training\nWelcome to Day 1 of the 4-Day Advanced Online Training on AI/ML for Earth Observation for Philippine EO Professionals. This training is part of the CopPhil Programme (EU-Philippines Copernicus Capacity Support Programme), a flagship initiative under the European Union’s Global Gateway strategy.\n\n\n\n\n\n\nNoteTraining Context\n\n\n\nThis course strengthens the Philippines’ capacity to use Copernicus Earth Observation data for:\n\nDisaster Risk Reduction (DRR) - Flood mapping, typhoon monitoring, landslide assessment\nClimate Change Adaptation (CCA) - Drought monitoring, agricultural resilience, coastal changes\nNatural Resource Management (NRM) - Forest monitoring, land cover mapping, marine resources",
    "crumbs": [
      "Home",
      "Getting Started",
      "Welcome to Day 1"
    ]
  },
  {
    "objectID": "index.html#day-1-overview",
    "href": "index.html#day-1-overview",
    "title": "Welcome to Day 1",
    "section": "Day 1 Overview",
    "text": "Day 1 Overview\nDay 1 provides the foundation for your AI/ML journey in Earth Observation. By the end of today, you will:\n\nUnderstand the Copernicus Sentinel missions and 2025 updates (Sentinel-2C, Sentinel-1C)\nNavigate the Philippine EO ecosystem (PhilSA, NAMRIA, DOST-ASTI)\nGrasp core AI/ML concepts and workflows for EO applications\nHandle geospatial data with Python (GeoPandas, Rasterio)\nAccess and preprocess satellite data using Google Earth Engine\n\n\nToday’s Schedule (8 hours)\n\n\n\n1\n\n\nCopernicus & PH EO 2 hours\n\n\n\n\n2\n\n\nAI/ML Fundamentals 2 hours\n\n\n\n\n3\n\n\nPython Geospatial 2 hours\n\n\n\n\n4\n\n\nGoogle Earth Engine 2 hours",
    "crumbs": [
      "Home",
      "Getting Started",
      "Welcome to Day 1"
    ]
  },
  {
    "objectID": "index.html#training-sessions",
    "href": "index.html#training-sessions",
    "title": "Welcome to Day 1",
    "section": "Training Sessions",
    "text": "Training Sessions\n\n\nSession 1\nCopernicus Sentinel Data & Philippine EO Ecosystem\n\nSentinel-1 SAR\n\n\nSentinel-2 Optical\n\nLearn about Europe’s flagship EO program and the Philippine agencies advancing EO in the country.\nGo to Session 1\n\n\nSession 2\nCore Concepts of AI/ML for Earth Observation\nDemystify AI/ML workflows, supervised vs unsupervised learning, neural networks, and data-centric approaches.\nGo to Session 2\n\n\nSession 3\nHands-on Python for Geospatial Data\nMaster vector data with GeoPandas and raster data with Rasterio - the foundations of EO data processing.\nGo to Session 3\n\n\nSession 4\nIntroduction to Google Earth Engine\nLeverage cloud computing power to access, filter, and preprocess petabytes of Earth observation data.\nGo to Session 4",
    "crumbs": [
      "Home",
      "Getting Started",
      "Welcome to Day 1"
    ]
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Welcome to Day 1",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of Day 1, you will be able to:\n\nKey Learning Outcomes\n\nIdentify the characteristics and applications of Sentinel-1 and Sentinel-2 missions\nNavigate Philippine EO platforms including PhilSA SIYASAT, NAMRIA Geoportal, and DOST-ASTI tools\nExplain the AI/ML workflow for Earth Observation applications\nDistinguish between supervised and unsupervised learning with EO examples\nUnderstand neural network fundamentals and data-centric AI principles\nLoad and visualize vector data using GeoPandas\nRead and process raster imagery using Rasterio\nQuery and filter satellite imagery collections in Google Earth Engine\nApply cloud masking and create temporal composites\nExport processed EO data for AI/ML workflows",
    "crumbs": [
      "Home",
      "Getting Started",
      "Welcome to Day 1"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Welcome to Day 1",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nWhat You Need\nBefore starting Day 1:\n\nGoogle account (for Colab and Earth Engine)\nGoogle Earth Engine account (sign up at earthengine.google.com)\nBasic Python knowledge (variables, loops, functions)\nFamiliarity with remote sensing concepts (helpful but not required)\n\nTechnical Setup:\nAll exercises run in Google Colaboratory - no local installation required! See our Setup Guide for detailed instructions.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Welcome to Day 1"
    ]
  },
  {
    "objectID": "index.html#updates-highlighted",
    "href": "index.html#updates-highlighted",
    "title": "Welcome to Day 1",
    "section": "2025 Updates Highlighted",
    "text": "2025 Updates Highlighted\nThis training incorporates the latest 2025 developments in Earth Observation and AI:\n\nSentinel-2C operational (January 2025) - Three-satellite constellation with 5-day revisit\nSentinel-1C active - Restored dual-satellite SAR coverage\nCopernicus Data Space Ecosystem - New data access platform with SentiBoard dashboard\nPhilSA SIYASAT portal - Secure data archive for NovaSAR-1 and maritime monitoring\nDOST P2.6B AI investment (until 2028) - SkAI-Pinas, DIMER, AIPI platforms\nESA Φsat-2 mission - On-board AI processing demonstration\nNASA-IBM Geospatial Foundation Model - Open-source pre-trained model for EO\nData-centric AI paradigm - Emphasis on data quality over model complexity",
    "crumbs": [
      "Home",
      "Getting Started",
      "Welcome to Day 1"
    ]
  },
  {
    "objectID": "index.html#quick-links",
    "href": "index.html#quick-links",
    "title": "Welcome to Day 1",
    "section": "Quick Links",
    "text": "Quick Links\n\nSetup Guide Philippine EO Resources Download Materials FAQ Glossary",
    "crumbs": [
      "Home",
      "Getting Started",
      "Welcome to Day 1"
    ]
  },
  {
    "objectID": "index.html#copphil-programme",
    "href": "index.html#copphil-programme",
    "title": "Welcome to Day 1",
    "section": "CopPhil Programme",
    "text": "CopPhil Programme\n\n\n\n\n\n\nTipAbout CopPhil\n\n\n\nThe Technical Assistance for the Philippines’ Copernicus Capacity Support Programme (CopPhil) is part of the EU-Philippines cooperation programme and the EU’s Global Gateway strategy.\nKey Partners:\n\nPhilippine Space Agency (PhilSA) - Co-chair and space data authority\nDepartment of Science and Technology (DOST) - Co-chair and technology advancement\nEuropean Union - Funding and technical cooperation\nEuropean Space Agency (ESA) - Copernicus programme expertise\n\nObjectives:\n\nEstablish Copernicus Mirror Site in the Philippines\nBuild capacity in EO data analysis and AI/ML applications\nCo-develop pilot services for DRR, CCA, and NRM\nCreate sustainable Digital Space Campus for continued learning",
    "crumbs": [
      "Home",
      "Getting Started",
      "Welcome to Day 1"
    ]
  },
  {
    "objectID": "index.html#need-help",
    "href": "index.html#need-help",
    "title": "Welcome to Day 1",
    "section": "Need Help?",
    "text": "Need Help?\nThroughout the training, you can:\n\nAsk questions in the live session\nConsult the FAQ for common issues\nCheck the Glossary for term definitions\nDownload Cheat Sheets for quick reference\nAccess the Philippine EO Resources directory\n\n\n\n\n\n\n\nImportantTechnical Support\n\n\n\nFor technical issues during the training:\n\nGoogle Colab issues: Check Setup Guide\nData access problems: See session-specific troubleshooting sections\nGeneral questions: Contact your instructors or teaching assistants",
    "crumbs": [
      "Home",
      "Getting Started",
      "Welcome to Day 1"
    ]
  },
  {
    "objectID": "index.html#ready-to-begin",
    "href": "index.html#ready-to-begin",
    "title": "Welcome to Day 1",
    "section": "Ready to Begin?",
    "text": "Ready to Begin?\n\n\n\nFirst Step\n\n\nComplete Setup Guide →\n\n\n\n\nNext Session\n\n\nSession 1: Copernicus & Philippine EO →\n\n\n\n\nThis training is funded by the European Union under the Global Gateway initiative and delivered in partnership with the Philippine Space Agency (PhilSA) and the Department of Science and Technology (DOST).",
    "crumbs": [
      "Home",
      "Getting Started",
      "Welcome to Day 1"
    ]
  },
  {
    "objectID": "resources/setup.html",
    "href": "resources/setup.html",
    "title": "Setup Guide",
    "section": "",
    "text": "This guide will help you set up everything you need for the CopPhil EO AI/ML Training. All exercises run in Google Colaboratory, so you won’t need to install Python or libraries on your local machine.\n\n\n\n\n\n\nNoteTime Required\n\n\n\nAllow 15-20 minutes to complete all setup steps before Day 1 begins.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Setup Guide"
    ]
  },
  {
    "objectID": "resources/setup.html#welcome",
    "href": "resources/setup.html#welcome",
    "title": "Setup Guide",
    "section": "",
    "text": "This guide will help you set up everything you need for the CopPhil EO AI/ML Training. All exercises run in Google Colaboratory, so you won’t need to install Python or libraries on your local machine.\n\n\n\n\n\n\nNoteTime Required\n\n\n\nAllow 15-20 minutes to complete all setup steps before Day 1 begins.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Setup Guide"
    ]
  },
  {
    "objectID": "resources/setup.html#prerequisites-checklist",
    "href": "resources/setup.html#prerequisites-checklist",
    "title": "Setup Guide",
    "section": "Prerequisites Checklist",
    "text": "Prerequisites Checklist\nBefore starting the training, ensure you have:\n\n\nA Google account (Gmail)\nGoogle Earth Engine access (registration required)\nStable internet connection (minimum 5 Mbps recommended)\nModern web browser (Chrome, Firefox, Safari, or Edge)\nHeadphones/speakers for audio",
    "crumbs": [
      "Home",
      "Getting Started",
      "Setup Guide"
    ]
  },
  {
    "objectID": "resources/setup.html#step-1-google-account-setup",
    "href": "resources/setup.html#step-1-google-account-setup",
    "title": "Setup Guide",
    "section": "Step 1: Google Account Setup",
    "text": "Step 1: Google Account Setup\n\n1.1 Create or Verify Google Account\nYou need a Google account to access Google Colaboratory and Google Earth Engine.\n\n\n\n\n\n\nTipAlready Have Gmail?\n\n\n\nIf you have a Gmail account, you’re all set! Skip to Step 2.\n\n\nTo create a new account:\n\nGo to accounts.google.com\nClick “Create account”\nFollow the registration process\nVerify your email address",
    "crumbs": [
      "Home",
      "Getting Started",
      "Setup Guide"
    ]
  },
  {
    "objectID": "resources/setup.html#step-2-google-colaboratory-setup",
    "href": "resources/setup.html#step-2-google-colaboratory-setup",
    "title": "Setup Guide",
    "section": "Step 2: Google Colaboratory Setup",
    "text": "Step 2: Google Colaboratory Setup\n\n2.1 What is Google Colab?\nGoogle Colaboratory (Colab) is a free cloud service that lets you write and execute Python code in your browser. It provides:\n\nFree access to GPUs (Graphics Processing Units)\nPre-installed Python libraries (NumPy, Pandas, Matplotlib, etc.)\nCloud storage integration with Google Drive\nShareable notebooks\n\n\n\n2.2 Access Google Colab\n\nGo to colab.research.google.com\nSign in with your Google account\nYou’ll see the welcome screen with example notebooks\n\n\n\n2.3 Test Your Colab Setup\nLet’s verify everything works:\n\nCreate a new notebook:\n\nClick “File” → “New notebook”\nA new notebook opens with an empty code cell\n\nRun a test:\n\nCopy and paste this code into the first cell:\n\nimport sys\nprint(f\"Python version: {sys.version}\")\nprint(\"Google Colab is working!\")\n\nPress Shift + Enter to run the cell\nYou should see the Python version and success message\n\nTest package installation:\n# Test common geospatial packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nprint(\"✓ NumPy:\", np.__version__)\nprint(\"✓ Pandas:\", pd.__version__)\nprint(\"✓ Matplotlib:\", plt.matplotlib.__version__)\n\n\n\n\n\n\n\nWarningFirst Run Takes Longer\n\n\n\nThe first time you run code in a new Colab session, it may take 30-60 seconds to allocate resources. This is normal.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Setup Guide"
    ]
  },
  {
    "objectID": "resources/setup.html#step-3-google-earth-engine-registration",
    "href": "resources/setup.html#step-3-google-earth-engine-registration",
    "title": "Setup Guide",
    "section": "Step 3: Google Earth Engine Registration",
    "text": "Step 3: Google Earth Engine Registration\n\n3.1 What is Google Earth Engine?\nGoogle Earth Engine (GEE) is a cloud platform for planetary-scale geospatial analysis. It provides:\n\nAccess to petabytes of satellite imagery (Landsat, Sentinel, MODIS, etc.)\nCloud-based processing (no downloads needed)\nPython and JavaScript APIs\nFast analysis over large areas and time periods\n\n\n\n3.2 Register for Earth Engine\n\n\n\n\n\n\nImportantRegistration Required\n\n\n\nEarth Engine registration can take 24-48 hours for approval. Register well before the training starts!\n\n\nRegistration steps:\n\nGo to: earthengine.google.com\nClick “Get Started” or “Sign Up”\nSign in with your Google account\nChoose account type:\n\nSelect “Register a Noncommercial or Commercial Cloud project”\nFor this training, select “Noncommercial” if applicable\n\nComplete the registration form:\n\nProject type: Education/Research\nOrganization: Your institution (e.g., “DOST-ASTI”, “NAMRIA”, “University of the Philippines”)\nProject description: “CopPhil EO AI/ML Training - Earth Observation data analysis for DRR/CCA/NRM applications”\nIntended use: Describe your interest in using EO data\n\nSubmit and wait for approval\n\nYou’ll receive an email when approved (usually within 24-48 hours)\nCheck your spam folder if you don’t see the approval email\n\n\n\n\n3.3 Verify Earth Engine Access\nOnce approved, test your access:\n\nOpen Google Colab: colab.research.google.com\nCreate a new notebook\nAuthenticate Earth Engine:\n\n# Install Earth Engine API (if needed)\n!pip install earthengine-api --quiet\n\n# Import and authenticate\nimport ee\n\n# Authenticate (first time only)\nee.Authenticate()\n\n# Initialize Earth Engine\nee.Initialize()\n\n# Test: Get an image\nimage = ee.Image('COPERNICUS/S2/20230101T000000_20230101T000000_T48PYS')\nprint(\"✓ Earth Engine is working!\")\nprint(f\"Image ID: {image.id().getInfo()}\")\n\nFollow authentication prompts:\n\nClick the link that appears\nSign in with your Google account\nCopy the authorization code\nPaste it back into Colab\n\n\n\n\n\n\n\n\nTipAuthentication Only Once\n\n\n\nAfter the first authentication, Earth Engine will remember your credentials in future Colab sessions.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Setup Guide"
    ]
  },
  {
    "objectID": "resources/setup.html#step-4-install-geospatial-python-packages",
    "href": "resources/setup.html#step-4-install-geospatial-python-packages",
    "title": "Setup Guide",
    "section": "Step 4: Install Geospatial Python Packages",
    "text": "Step 4: Install Geospatial Python Packages\nIn Google Colab, most packages are pre-installed. For specialized geospatial libraries, we’ll install them when needed.\n\nCommon packages we’ll use:\n\n\n\nPackage\nPurpose\nPre-installed?\n\n\n\n\nnumpy\nNumerical computing\nYes ✓\n\n\npandas\nData manipulation\nYes ✓\n\n\nmatplotlib\nVisualization\nYes ✓\n\n\ngeopandas\nVector data\nNo (we’ll install)\n\n\nrasterio\nRaster data\nNo (we’ll install)\n\n\nearthengine-api\nGoogle Earth Engine\nNo (we’ll install)\n\n\nfolium\nInteractive maps\nYes ✓\n\n\n\n\n\nInstallation template for notebooks:\nEach training notebook will include installation cells like this:\n# Install geospatial packages\n!pip install geopandas rasterio earthengine-api --quiet\n\n# Import packages\nimport geopandas as gpd\nimport rasterio\nimport ee\n\nprint(\"✓ All packages installed successfully!\")",
    "crumbs": [
      "Home",
      "Getting Started",
      "Setup Guide"
    ]
  },
  {
    "objectID": "resources/setup.html#step-5-google-drive-integration-optional",
    "href": "resources/setup.html#step-5-google-drive-integration-optional",
    "title": "Setup Guide",
    "section": "Step 5: Google Drive Integration (Optional)",
    "text": "Step 5: Google Drive Integration (Optional)\nTo save your work and access datasets, you can mount Google Drive in Colab:\nfrom google.colab import drive\ndrive.mount('/content/drive')\nBenefits: - Save notebooks directly to Drive - Access datasets stored in Drive - Work persists between sessions\n\n\n\n\n\n\nNoteStorage Limits\n\n\n\nFree Google accounts get 15 GB of Drive storage. For large datasets, we’ll stream data directly from Earth Engine instead.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Setup Guide"
    ]
  },
  {
    "objectID": "resources/setup.html#step-6-download-training-notebooks",
    "href": "resources/setup.html#step-6-download-training-notebooks",
    "title": "Setup Guide",
    "section": "Step 6: Download Training Notebooks",
    "text": "Step 6: Download Training Notebooks\nAll training notebooks will be provided during the sessions. You can:\n\nAccess via shared links (provided by instructors)\nDownload from the training portal (see Downloads)\nClone from GitHub (if repository is available)",
    "crumbs": [
      "Home",
      "Getting Started",
      "Setup Guide"
    ]
  },
  {
    "objectID": "resources/setup.html#troubleshooting-common-issues",
    "href": "resources/setup.html#troubleshooting-common-issues",
    "title": "Setup Guide",
    "section": "Troubleshooting Common Issues",
    "text": "Troubleshooting Common Issues\n\nIssue 1: “Runtime disconnected” in Colab\nCause: Colab sessions timeout after 90 minutes of inactivity (12 hours maximum)\nSolution: - Reconnect by clicking “Reconnect” button - Re-run setup cells (imports, authentication) - Consider using Colab Pro for longer sessions\n\n\nIssue 2: Earth Engine authentication fails\nCause: Not registered or registration not approved\nSolution: - Verify registration status at earthengine.google.com - Wait for approval email (24-48 hours) - Check spam folder for approval notification\n\n\nIssue 3: Package installation fails\nCause: Network issues or package conflicts\nSolution:\n# Force reinstall\n!pip install --upgrade --force-reinstall geopandas\n\n# Or use specific versions\n!pip install geopandas==0.14.0\n\n\nIssue 4: Slow performance in Colab\nCause: Limited free resources\nSolutions: - Close other browser tabs - Restart runtime: Runtime → Restart runtime - Use GPU acceleration: Runtime → Change runtime type → GPU - Reduce data processing scope\n\n\nIssue 5: Cannot access Google Drive\nCause: Permission not granted\nSolution: - Re-run the mount command - Click the authorization link - Grant access to Google Drive",
    "crumbs": [
      "Home",
      "Getting Started",
      "Setup Guide"
    ]
  },
  {
    "objectID": "resources/setup.html#system-requirements",
    "href": "resources/setup.html#system-requirements",
    "title": "Setup Guide",
    "section": "System Requirements",
    "text": "System Requirements\n\nMinimum Requirements\n\nInternet: 5 Mbps download speed\nBrowser: Chrome 90+, Firefox 88+, Safari 14+, Edge 90+\nRAM: 4 GB (8 GB recommended)\nScreen: 1280x720 resolution minimum\n\n\n\nRecommended Setup\n\nInternet: 10+ Mbps for smooth streaming\nBrowser: Latest version of Chrome (best compatibility)\nRAM: 8+ GB for comfortable multitasking\nScreen: Dual monitors (one for presentation, one for coding)",
    "crumbs": [
      "Home",
      "Getting Started",
      "Setup Guide"
    ]
  },
  {
    "objectID": "resources/setup.html#pre-training-checklist",
    "href": "resources/setup.html#pre-training-checklist",
    "title": "Setup Guide",
    "section": "Pre-Training Checklist",
    "text": "Pre-Training Checklist\nBefore Day 1 starts, ensure:\n\n\nGoogle account created and verified\nGoogle Colab accessible and tested\nGoogle Earth Engine registered and approved\nTest notebook runs successfully\nEarth Engine authentication completed\nBrowser and internet connection tested\nHeadphones/speakers working\nQuiet workspace prepared",
    "crumbs": [
      "Home",
      "Getting Started",
      "Setup Guide"
    ]
  },
  {
    "objectID": "resources/setup.html#getting-help",
    "href": "resources/setup.html#getting-help",
    "title": "Setup Guide",
    "section": "Getting Help",
    "text": "Getting Help\n\nDuring Training\n\nAsk questions in the live session chat\nConsult teaching assistants\nCheck the FAQ for common issues\n\n\n\nBefore Training\n\nReview this setup guide thoroughly\nTest all components at least 1 day before\nContact training coordinators if you encounter issues\n\n\n\n\n\n\n\nImportantTechnical Support Contacts\n\n\n\nFor urgent setup issues: - Email: training@philsa.gov.ph - Slack/Discord: Check invitation email for links - WhatsApp Group: Join via link in confirmation email",
    "crumbs": [
      "Home",
      "Getting Started",
      "Setup Guide"
    ]
  },
  {
    "objectID": "resources/setup.html#additional-resources",
    "href": "resources/setup.html#additional-resources",
    "title": "Setup Guide",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nGoogle Colab Tutorials\n\nOfficial Colab Welcome Notebook\nColab Markdown Guide\n\n\n\nGoogle Earth Engine Resources\n\nEarth Engine Guides\nPython API Documentation\nCommunity Forum\n\n\n\nPython for Geospatial\n\nGeoPandas Documentation\nRasterio Documentation\nGeospatial Python Workshop",
    "crumbs": [
      "Home",
      "Getting Started",
      "Setup Guide"
    ]
  },
  {
    "objectID": "resources/setup.html#ready-to-start",
    "href": "resources/setup.html#ready-to-start",
    "title": "Setup Guide",
    "section": "Ready to Start?",
    "text": "Ready to Start?\nOnce you’ve completed all setup steps, you’re ready for the training!\n\n\n\nBack\n\n\n← Return to Home\n\n\n\n\nNext\n\n\nSession 1: Copernicus & Philippine EO →\n\n\n\n\nSetup questions? Contact the training coordinators or check the FAQ.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Setup Guide"
    ]
  },
  {
    "objectID": "resources/faq.html",
    "href": "resources/faq.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "The Technical Assistance for the Philippines’ Copernicus Capacity Support Programme (CopPhil) is an EU-Philippines cooperation initiative under the Global Gateway strategy. It aims to:\n\nEstablish a Copernicus Mirror Site in the Philippines\nBuild capacity in EO data analysis and AI/ML\nCo-develop pilot services for DRR, CCA, and NRM\nCreate a sustainable Digital Space Campus for training\n\nKey Partners: Philippine Space Agency (PhilSA), Department of Science and Technology (DOST), European Union, European Space Agency (ESA)\n\n\n\n\nThis training is designed for:\n\nPhilippine government employees working in EO, disaster management, agriculture, or environment\nResearchers at universities and research institutions\nGIS professionals looking to expand into AI/ML\nData scientists interested in geospatial applications\nPhilSA, NAMRIA, DOST-ASTI, PAGASA, DENR staff\n\nPrerequisites: Basic Python knowledge and familiarity with remote sensing concepts (helpful but not required)\n\n\n\n\nMinimum: - Google account (Gmail) - Stable internet (5 Mbps+) - Modern web browser (Chrome, Firefox, Safari, Edge) - 4 GB RAM\nRecommended: - 10+ Mbps internet - Chrome browser (best compatibility) - 8+ GB RAM - Dual monitors (one for presentation, one for coding)\nNote: All exercises run in Google Colaboratory - no local software installation required!\n\n\n\n\nNo! All hands-on exercises use Google Colaboratory, which provides:\n\nFree cloud computing resources\nPre-installed Python libraries\nAccess to GPUs\nNo local installation needed\n\nHowever, if you prefer working locally, we provide installation guides in the Setup Guide.\n\n\n\n\nYes! The CopPhil training programme is fully funded by the European Union under the Global Gateway initiative. There are no fees for participants.",
    "crumbs": [
      "Home",
      "Resources",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "resources/faq.html#general-questions",
    "href": "resources/faq.html#general-questions",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "The Technical Assistance for the Philippines’ Copernicus Capacity Support Programme (CopPhil) is an EU-Philippines cooperation initiative under the Global Gateway strategy. It aims to:\n\nEstablish a Copernicus Mirror Site in the Philippines\nBuild capacity in EO data analysis and AI/ML\nCo-develop pilot services for DRR, CCA, and NRM\nCreate a sustainable Digital Space Campus for training\n\nKey Partners: Philippine Space Agency (PhilSA), Department of Science and Technology (DOST), European Union, European Space Agency (ESA)\n\n\n\n\nThis training is designed for:\n\nPhilippine government employees working in EO, disaster management, agriculture, or environment\nResearchers at universities and research institutions\nGIS professionals looking to expand into AI/ML\nData scientists interested in geospatial applications\nPhilSA, NAMRIA, DOST-ASTI, PAGASA, DENR staff\n\nPrerequisites: Basic Python knowledge and familiarity with remote sensing concepts (helpful but not required)\n\n\n\n\nMinimum: - Google account (Gmail) - Stable internet (5 Mbps+) - Modern web browser (Chrome, Firefox, Safari, Edge) - 4 GB RAM\nRecommended: - 10+ Mbps internet - Chrome browser (best compatibility) - 8+ GB RAM - Dual monitors (one for presentation, one for coding)\nNote: All exercises run in Google Colaboratory - no local software installation required!\n\n\n\n\nNo! All hands-on exercises use Google Colaboratory, which provides:\n\nFree cloud computing resources\nPre-installed Python libraries\nAccess to GPUs\nNo local installation needed\n\nHowever, if you prefer working locally, we provide installation guides in the Setup Guide.\n\n\n\n\nYes! The CopPhil training programme is fully funded by the European Union under the Global Gateway initiative. There are no fees for participants.",
    "crumbs": [
      "Home",
      "Resources",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "resources/faq.html#setup-account-issues",
    "href": "resources/faq.html#setup-account-issues",
    "title": "Frequently Asked Questions",
    "section": "Setup & Account Issues",
    "text": "Setup & Account Issues\n\nHow do I register for Google Earth Engine?\n\nGo to earthengine.google.com\nClick “Get Started” or “Sign Up”\nSign in with your Google account\nSelect “Noncommercial” (for this training)\nFill out the registration form:\n\nOrganization: Your institution\nProject description: “CopPhil EO AI/ML Training”\n\nSubmit and wait for approval (24-48 hours)\n\n\n\n\n\n\n\nImportant\n\n\n\nRegister at least 2 days before the training starts to ensure approval!\n\n\nFor detailed instructions, see the Setup Guide.\n\n\n\nMy Earth Engine registration is taking too long\nNormal approval time: 24-48 hours\nIf it’s been longer: 1. Check your spam folder for the approval email 2. Verify registration status at earthengine.google.com 3. Re-submit registration if it shows as not received 4. Contact Earth Engine support: earthengine-support@google.com\n\n\n\nI forgot to authenticate Earth Engine in Colab\nSymptoms: ee commands throw errors like “Please set project ID”\nSolution:\nimport ee\n\n# Authenticate (follow prompts)\nee.Authenticate()\n\n# Initialize\nee.Initialize()\nThis only needs to be done once per Google account. Future sessions will remember your credentials.\n\n\n\nGoogle Colab says “Runtime disconnected”\nCauses: - 90 minutes of inactivity - Maximum session length (12 hours for free accounts) - Browser tab closed or crashed\nSolution: 1. Click “Reconnect” button 2. Re-run setup cells (imports, authentication) 3. Continue from where you left off\n\n\n\n\n\n\nTip\n\n\n\nPrevent disconnections: - Keep browser tab active - Save work to Google Drive regularly - Use Ctrl/Cmd + S to save notebooks\n\n\n\n\n\nHow do I save my work in Google Colab?\nOption 1: Save to Drive (Recommended)\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Save outputs to Drive\noutput_path = '/content/drive/My Drive/CopPhil_Training/'\nOption 2: Download Files - Click folder icon in left sidebar - Right-click file → Download\nOption 3: Save Notebook - File → Save a copy in Drive - File → Download .ipynb",
    "crumbs": [
      "Home",
      "Resources",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "resources/faq.html#python-coding-issues",
    "href": "resources/faq.html#python-coding-issues",
    "title": "Frequently Asked Questions",
    "section": "Python & Coding Issues",
    "text": "Python & Coding Issues\n\nI’m getting “ModuleNotFoundError”\nExample: ModuleNotFoundError: No module named 'geopandas'\nCause: Package not installed in current Colab session\nSolution:\n# Install the missing package\n!pip install geopandas\n\n# Then import it\nimport geopandas as gpd\nCommon packages to install: - geopandas - rasterio - earthengine-api - geemap\n\n\n\nPackage installation is failing\nError: ERROR: Could not find a version that satisfies the requirement...\nSolutions:\n1. Update pip first:\n!pip install --upgrade pip\n!pip install geopandas\n2. Install specific version:\n!pip install geopandas==0.14.0\n3. Use conda (if local):\nconda install -c conda-forge geopandas\n4. Force reinstall:\n!pip install --upgrade --force-reinstall geopandas\n\n\n\nMy code runs locally but fails in Colab\nCommon causes:\n1. File paths: - Local: C:/Users/name/data.shp - Colab: /content/data.shp or from Drive\n2. Package versions: - Check versions: import package; print(package.__version__) - Install specific version if needed\n3. Missing files: - Upload files: Click folder icon → Upload - Or mount Google Drive\n\n\n\n“MemoryError” or “Kernel crashed”\nCauses: - Loading too much data - Processing large rasters - Insufficient RAM\nSolutions:\n1. Reduce data scope:\n# Read smaller window\nwindow = rasterio.windows.Window(0, 0, 1000, 1000)\ndata = src.read(1, window=window)\n\n# Or downsample\ndata = src.read(1, out_shape=(src.height // 4, src.width // 4))\n2. Use chunking:\n# Process in chunks\nfor window in src.block_windows():\n    data = src.read(1, window=window)\n    process(data)\n3. Enable GPU in Colab: - Runtime → Change runtime type → GPU\n4. Upgrade to Colab Pro: - More RAM (up to 50 GB) - Longer sessions - colab.research.google.com/signup",
    "crumbs": [
      "Home",
      "Resources",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "resources/faq.html#earth-engine-issues",
    "href": "resources/faq.html#earth-engine-issues",
    "title": "Frequently Asked Questions",
    "section": "Earth Engine Issues",
    "text": "Earth Engine Issues\n\n“User memory limit exceeded” in Earth Engine\nCause: Trying to process too much data at once\nSolutions:\n1. Increase scale parameter:\n# Before (10m resolution)\nresult = image.reduceRegion(\n    reducer=ee.Reducer.mean(),\n    geometry=roi,\n    scale=10  # 10m pixels\n)\n\n# After (100m resolution)\nresult = image.reduceRegion(\n    reducer=ee.Reducer.mean(),\n    geometry=roi,\n    scale=100  # 100m pixels\n)\n2. Reduce region size:\n# Smaller bounding box\nsmall_roi = ee.Geometry.Rectangle([120.0, 14.0, 120.5, 14.5])\n3. Filter dates more strictly:\n# Shorter time period\ncollection = collection.filterDate('2024-01-01', '2024-01-31')  # 1 month instead of 1 year\n4. Use maxPixels parameter:\ntask = ee.batch.Export.image.toDrive(\n    image=image,\n    scale=10,\n    maxPixels=1e13  # Allow more pixels\n)\n\n\n\nEarth Engine export is stuck at “RUNNING”\nCheck status:\n# Check task status\nprint(task.status())\nPossible statuses: - READY: Queued, waiting to start - RUNNING: Currently processing - COMPLETED: Successfully finished - FAILED: Error occurred (check status for details)\nIf stuck: 1. Wait - large exports can take hours 2. Check Earth Engine Task Manager 3. Cancel and restart with smaller parameters 4. Check Google Drive storage space\n\n\n\nCloud-free composite still has clouds\nCause: Cloud masking didn’t work perfectly\nSolutions:\n1. Use better cloud masking:\ndef aggressive_cloud_mask(image):\n    qa = image.select('QA60')\n    # Mask both cloud and cirrus\n    cloud_mask = qa.bitwiseAnd(1 &lt;&lt; 10).eq(0).And(\n                 qa.bitwiseAnd(1 &lt;&lt; 11).eq(0))\n    # Also mask cloud shadows\n    scl = image.select('SCL')\n    shadow_mask = scl.neq(3)  # 3 = cloud shadow\n    return image.updateMask(cloud_mask).updateMask(shadow_mask)\n2. Use percentile reduction instead of median:\n# Use 20th percentile (darker, less clouds)\ncomposite = collection.reduce(ee.Reducer.percentile([20]))\n3. Filter by cloud cover first:\ncollection = collection.filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))",
    "crumbs": [
      "Home",
      "Resources",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "resources/faq.html#data-visualization-issues",
    "href": "resources/faq.html#data-visualization-issues",
    "title": "Frequently Asked Questions",
    "section": "Data & Visualization Issues",
    "text": "Data & Visualization Issues\n\nMy map doesn’t display in Colab\nCause: Missing visualization library\nSolution:\n# Install geemap for interactive maps\n!pip install geemap\n\nimport geemap\n\n# Create map\nMap = geemap.Map()\nMap.centerObject(roi, 10)\nMap.addLayer(image, vis_params, 'Image')\nMap\n\n\n\nColors in my visualization look wrong\nCheck visualization parameters:\n# For Sentinel-2 true color\nvis_params = {\n    'bands': ['B4', 'B3', 'B2'],  # Red, Green, Blue\n    'min': 0,\n    'max': 3000,  # Adjust based on your data\n    'gamma': 1.4\n}\n\n# For NDVI\nndvi_vis = {\n    'min': -1,\n    'max': 1,\n    'palette': ['red', 'yellow', 'green']\n}\nAdjust min/max: - Too dark → decrease max value - Too bright → increase max value - Washed out → adjust gamma\n\n\n\nGeoPandas plot shows nothing\nCommon issues:\n1. Empty GeoDataFrame:\nprint(len(gdf))  # Check if it has rows\nprint(gdf.head())\n2. Wrong CRS:\nprint(gdf.crs)  # Check coordinate reference system\ngdf = gdf.to_crs('EPSG:4326')  # Reproject if needed\n3. Data outside visible area:\nprint(gdf.total_bounds)  # Check bounding box\ngdf.plot(figsize=(10, 10))  # Larger figure size\n\n\n\nRasterio shows “All-NaN slice encountered”\nCause: Trying to visualize a band with all nodata values\nSolution:\n# Check for valid data\nprint(f\"Min: {band.min()}, Max: {band.max()}\")\nprint(f\"Valid pixels: {np.count_nonzero(~np.isnan(band))}\")\n\n# Mask nodata\nvalid_mask = ~np.isnan(band)\nif valid_mask.any():\n    plt.imshow(band, cmap='gray')\nelse:\n    print(\"No valid data in this band\")",
    "crumbs": [
      "Home",
      "Resources",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "resources/faq.html#philippine-specific-questions",
    "href": "resources/faq.html#philippine-specific-questions",
    "title": "Frequently Asked Questions",
    "section": "Philippine-Specific Questions",
    "text": "Philippine-Specific Questions\n\nWhere can I get Philippine administrative boundaries?\nSources:\n\nPhilGIS: philgis.org\n\nShapefile format\nAll administrative levels\n\nNAMRIA GeoPortal: geoportal.namria.gov.ph\n\nOfficial government source\nRegistration may be required\n\nHumanitarian Data Exchange: data.humdata.org\n\nOpen data\nGeoJSON and Shapefile\n\nIn Earth Engine:\n\n# FAO GAUL administrative boundaries\nphilippines = ee.FeatureCollection(\"FAO/GAUL/2015/level1\") \\\n    .filter(ee.Filter.eq('ADM0_NAME', 'Philippines'))\n\n\n\nHow do I get Sentinel data specifically for the Philippines?\nIn Google Earth Engine:\n# Define Philippines bounding box\nphilippines_bbox = ee.Geometry.Rectangle([116.0, 4.0, 127.0, 21.0])\n\n# Filter Sentinel-2 collection\ncollection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n    .filterBounds(philippines_bbox) \\\n    .filterDate('2024-01-01', '2024-12-31') \\\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n\nprint(f\"Found {collection.size().getInfo()} images\")\nVia Copernicus Data Space: 1. Go to dataspace.copernicus.eu 2. Use SentiBoard to browse visually 3. Draw bounding box over Philippines 4. Filter by date and cloud cover 5. Download tiles\n\n\n\nWhat are the best satellite data sources for Philippine disasters?\nFloods: - Sentinel-1 SAR (works through clouds) - Planet Labs (daily imagery, commercial) - Landsat-8/9 (free, 16-day revisit)\nTyphoons: - Sentinel-2 (damage assessment) - MODIS (rapid assessment) - Himawari-8 (near real-time, via PAGASA)\nLandslides: - PlanetScope (3m resolution) - Sentinel-2 (10m, change detection) - LiDAR (elevation, via LiPAD portal)\nDrought: - MODIS (vegetation indices, 8-day) - Sentinel-2 (higher resolution) - SMAP (soil moisture)",
    "crumbs": [
      "Home",
      "Resources",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "resources/faq.html#training-specific-questions",
    "href": "resources/faq.html#training-specific-questions",
    "title": "Frequently Asked Questions",
    "section": "Training-Specific Questions",
    "text": "Training-Specific Questions\n\nCan I get a certificate for completing this training?\nYes! Participants who complete all 4 days and pass the final assessment will receive a CopPhil Training Programme Certificate issued by PhilSA and DOST.\nRequirements: - Attend all 4 days - Complete hands-on exercises - Submit final project - Pass assessment (70% minimum)\n\n\n\nWill the training materials be available after the course?\nYes! All materials will remain accessible:\n\nTraining portal stays online\nNotebooks available on GitHub\nRecorded sessions (if applicable)\nOngoing access to Digital Space Campus (under development)\n\n\n\n\nCan I share these materials with colleagues?\nYes! All training materials are licensed under Creative Commons BY-SA 4.0, which means you can:\n\nShare freely\nUse for teaching\nModify and adapt\nUse commercially\n\nRequirements: - Provide attribution: “CopPhil EO AI/ML Training Programme” - Share derivatives under the same license\n\n\n\nWhat comes after Day 1?\nDay 2: Classical Machine Learning for Land Cover Classification - Random Forests - Support Vector Machines - Feature engineering - Palawan land cover case study\nDay 3: Deep Learning for Flood Mapping & Object Detection - U-Net architecture - Sentinel-1 flood detection - YOLOv8 for infrastructure - Central Luzon flood case study\nDay 4: Advanced Topics & Practical Application - Foundation models for EO - Time series analysis - Transfer learning - Final project",
    "crumbs": [
      "Home",
      "Resources",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "resources/faq.html#technical-support",
    "href": "resources/faq.html#technical-support",
    "title": "Frequently Asked Questions",
    "section": "Technical Support",
    "text": "Technical Support\n\nWho do I contact for technical issues?\nDuring training: - Ask in the live session chat - Consult teaching assistants - Check this FAQ first\nOutside training hours: - Email: training@philsa.gov.ph - Slack/Discord: Check invitation email - GitHub Issues: Report issue\n\n\n\nI found an error in the training materials\nThank you for helping improve the training!\nTo report: 1. GitHub Issues: Create issue 2. Email: training@philsa.gov.ph 3. During session: Notify instructors\nInclude: - Which session/notebook - What the error is - Steps to reproduce - Your environment (Colab or local)",
    "crumbs": [
      "Home",
      "Resources",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "resources/faq.html#additional-resources",
    "href": "resources/faq.html#additional-resources",
    "title": "Frequently Asked Questions",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nWhere can I learn more about Earth Observation?\nOnline Courses: - Copernicus Training - ESA EO Training - Google Earth Engine Tutorials\nBooks: - “Remote Sensing and Image Interpretation” - Lillesand et al. - “Python for Geospatial Data Analysis” - Garrard - “Deep Learning for the Earth Sciences” - Camps-Valls et al.\nCommunities: - Google Earth Engine Developers - Stack Exchange GIS - r/gis\n\n\n\nWhere can I get help with Python programming?\nLearning Resources: - Python.org Tutorial - Real Python - DataCamp\nGetting Help: - Stack Overflow - Python Discord - r/learnpython",
    "crumbs": [
      "Home",
      "Resources",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "resources/faq.html#still-have-questions",
    "href": "resources/faq.html#still-have-questions",
    "title": "Frequently Asked Questions",
    "section": "Still Have Questions?",
    "text": "Still Have Questions?\n\n\n\n\n\n\nTipCan’t Find Your Answer?\n\n\n\nContact Us: - Email: training@philsa.gov.ph - During training: Ask instructors directly - Slack/Discord: Join the community channels\nWe’re here to help ensure your success in the training!\n\n\n\nThis FAQ is regularly updated based on participant questions. Last updated: 2025-01-15",
    "crumbs": [
      "Home",
      "Resources",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "resources/downloads.html",
    "href": "resources/downloads.html",
    "title": "Downloads",
    "section": "",
    "text": "Download all training materials for Day 1 of the CopPhil EO AI/ML Training. All materials are provided under open licenses for educational use.\n\n\n\n\n\n\nNoteDownload Package\n\n\n\nFor convenience, you can download all Day 1 materials as a single ZIP file, or download individual components below.",
    "crumbs": [
      "Home",
      "Resources",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#overview",
    "href": "resources/downloads.html#overview",
    "title": "Downloads",
    "section": "",
    "text": "Download all training materials for Day 1 of the CopPhil EO AI/ML Training. All materials are provided under open licenses for educational use.\n\n\n\n\n\n\nNoteDownload Package\n\n\n\nFor convenience, you can download all Day 1 materials as a single ZIP file, or download individual components below.",
    "crumbs": [
      "Home",
      "Resources",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#complete-day-1-package",
    "href": "resources/downloads.html#complete-day-1-package",
    "title": "Downloads",
    "section": "Complete Day 1 Package",
    "text": "Complete Day 1 Package\n\nAll-in-One Download\nPackage Contents: - All 4 session notebooks (Jupyter .ipynb) - Presentation slides (PDF) - Cheat sheets (PDF) - Sample datasets - Session handouts\nSize: ~150 MB\nDownload Complete Package (ZIP)\nLast updated: 2025-01-15",
    "crumbs": [
      "Home",
      "Resources",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#jupyter-notebooks",
    "href": "resources/downloads.html#jupyter-notebooks",
    "title": "Downloads",
    "section": "Jupyter Notebooks",
    "text": "Jupyter Notebooks\nDownload interactive notebooks for hands-on practice:\n\nSession 3: Python for Geospatial Data\n\nDay1_Session3_Python_Geospatial_Data.ipynb\nLearn to work with vector data (GeoPandas) and raster data (Rasterio) in Python.\nTopics Covered: - GeoPandas for vector data - Reading shapefiles, GeoJSON, and GeoPackages - Coordinate reference systems and reprojection - Rasterio for raster data - Reading GeoTIFF files - Band operations and visualization - Philippine case study: Palawan land cover\nRequirements: - Google Colab (recommended) - Or local Jupyter with geopandas, rasterio installed\nDownload Notebook Open in Colab\n\n\n\nSession 4: Google Earth Engine\n\nDay1_Session4_Google_Earth_Engine.ipynb\nMaster Google Earth Engine for accessing and processing Copernicus data.\nTopics Covered: - Earth Engine authentication and initialization - ImageCollection filtering (spatial, temporal, metadata) - Sentinel-1 SAR data access - Sentinel-2 optical data access - Cloud masking techniques - Temporal compositing - Data export workflows - Philippine case study: Metro Manila monitoring\nRequirements: - Google Earth Engine account (register at earthengine.google.com) - Google Colab\nDownload Notebook Open in Colab",
    "crumbs": [
      "Home",
      "Resources",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#presentation-slides",
    "href": "resources/downloads.html#presentation-slides",
    "title": "Downloads",
    "section": "Presentation Slides",
    "text": "Presentation Slides\nDownload presentation slides for reference:\n\nSession 1: Copernicus Sentinel Data & Philippine EO Ecosystem\n\nTopics: - Copernicus Programme overview - Sentinel-1 SAR characteristics - Sentinel-2 optical specifications - 2025 updates (Sentinel-2C, Sentinel-1C) - Philippine EO agencies (PhilSA, NAMRIA, DOST-ASTI) - CopPhil Mirror Site introduction\nFormat: PDF (slides with notes)\nSize: ~15 MB\nDownload PDF View Online\n\n\n\nSession 2: AI/ML Fundamentals for Earth Observation\n\nTopics: - What is AI/ML and the EO workflow - Supervised vs. Unsupervised learning - Classification and regression examples - Introduction to neural networks - Convolutional Neural Networks (CNNs) - Data-centric AI paradigm - EO-specific considerations\nFormat: PDF (slides with notes)\nSize: ~12 MB\nDownload PDF View Online\n\n\n\nSession 3: Python for Geospatial Data (Intro)\n\nTopics: - Google Colab setup - Python environment for geospatial - Vector data concepts - Raster data concepts - Coordinate reference systems\nFormat: PDF (slides with notes)\nSize: ~8 MB\nDownload PDF View Online\n\n\n\nSession 4: Introduction to Google Earth Engine (Coming Soon)\n\nTopics: - Earth Engine architecture - Python API overview - Data catalog navigation - Philippine EO applications\nFormat: PDF (slides with notes)\nStatus: In preparation\nComing Soon",
    "crumbs": [
      "Home",
      "Resources",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#cheat-sheets-pdf",
    "href": "resources/downloads.html#cheat-sheets-pdf",
    "title": "Downloads",
    "section": "Cheat Sheets (PDF)",
    "text": "Cheat Sheets (PDF)\nPrint-friendly quick reference guides:\n\n\nPython Basics\nEssential Python syntax and operations\nDownload PDF\n\n\nGeoPandas Reference\nVector data operations\nDownload PDF\n\n\nRasterio Commands\nRaster data handling\nDownload PDF\n\n\nEarth Engine API\nGEE Python commands\nDownload PDF\n\n\nSentinel Missions\nBand specifications\nDownload PDF\n\n\nSpectral Indices\nCommon formulas\nDownload PDF",
    "crumbs": [
      "Home",
      "Resources",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#sample-datasets",
    "href": "resources/downloads.html#sample-datasets",
    "title": "Downloads",
    "section": "Sample Datasets",
    "text": "Sample Datasets\nPractice datasets for offline work:\n\nPhilippine Administrative Boundaries\n\nDescription: Provincial and municipal boundaries for the Philippines\nFormat: GeoPackage (.gpkg)\nSize: ~5 MB\nSource: PhilGIS / NAMRIA\nLicense: Open Data\nDownload Dataset\nContents: - Regions (17) - Provinces (81) - Municipalities and Cities - Attribute table with population data\n\n\n\nPalawan Land Cover Sample\n\nDescription: Sentinel-2 true color composite and land cover classification\nFormat: GeoTIFF (.tif)\nSize: ~80 MB\nArea: Palawan Province subset\nDate: 2024 dry season composite\nDownload Dataset\nFiles: - palawan_s2_composite.tif (RGB composite) - palawan_landcover.tif (Classification) - palawan_boundary.gpkg (Vector boundary) - README.txt (Metadata)\n\n\n\nMetro Manila Sentinel-1 Time Series (Sample)\n\nDescription: Sentinel-1 VV/VH polarization samples\nFormat: GeoTIFF (.tif)\nSize: ~40 MB\nArea: Metro Manila subset\nDates: Monthly composites (Jan-Dec 2024)\nDownload Dataset\nUse Cases: - Urban monitoring - Flood detection practice - Time series analysis",
    "crumbs": [
      "Home",
      "Resources",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#session-handouts",
    "href": "resources/downloads.html#session-handouts",
    "title": "Downloads",
    "section": "Session Handouts",
    "text": "Session Handouts\nPrinted materials for participants:\n\nDay 1 Participant Handbook\nContents: - Session summaries - Learning objectives - Key concepts - Exercise instructions - Note-taking space\nFormat: PDF (A4, printable)\nSize: ~3 MB\nDownload PDF\n\n\nGlossary of Terms\nContents: - EO terminology - AI/ML concepts - Acronyms and abbreviations - Philippine EO agencies\nFormat: PDF (booklet style)\nSize: ~2 MB\nDownload PDF",
    "crumbs": [
      "Home",
      "Resources",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#code-examples-repository",
    "href": "resources/downloads.html#code-examples-repository",
    "title": "Downloads",
    "section": "Code Examples Repository",
    "text": "Code Examples Repository\nAccess all code examples and utilities:\n\nGitHub Repository\nRepository: copphil-training/day1-materials\nContents: - Jupyter notebooks (latest versions) - Python utility functions - Data processing scripts - Example workflows - Troubleshooting guides\nVisit Repository Clone via Git\n# Clone repository\ngit clone https://github.com/copphil-training/day1-materials.git\n\n# Or download ZIP\nwget https://github.com/copphil-training/day1-materials/archive/main.zip",
    "crumbs": [
      "Home",
      "Resources",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#google-colab-links",
    "href": "resources/downloads.html#google-colab-links",
    "title": "Downloads",
    "section": "Google Colab Links",
    "text": "Google Colab Links\nDirect links to open notebooks in Google Colab:\n\nSession 3: Python for Geospatial Data\n\n\n\nOpen In Colab\n\n\n\nSession 4: Google Earth Engine\n\n\n\nOpen In Colab\n\n\n\n\n\n\n\n\n\nTipWorking in Colab\n\n\n\nOpening notebooks in Colab requires no local installation. All code runs in the cloud with access to GPUs and pre-installed libraries.\nFirst Time? See the Setup Guide for authentication instructions.",
    "crumbs": [
      "Home",
      "Resources",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#additional-resources",
    "href": "resources/downloads.html#additional-resources",
    "title": "Downloads",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nRecommended Reading\n\nEarth Observation: - Copernicus Open Access Hub User Guide - Sentinel-1 Toolbox Documentation - Sentinel-2 User Handbook\nPython for Geospatial: - GeoPandas User Guide - Rasterio Quickstart - Python Geospatial Development\nGoogle Earth Engine: - Earth Engine Python API Guide - Earth Engine Community Tutorials - Awesome Earth Engine\nAI/ML for EO: - Deep Learning for Earth Observation (Springer) - Fundamentals of Machine Learning for Predictive Data Analytics - Data-Centric AI Resource Hub\n\n\n\nVideo Tutorials\n\n\nCopernicus Programme Introduction (15 min) - Watch\nGetting Started with Google Earth Engine (30 min) - Watch\nGeoPandas Tutorial for Beginners (45 min) - Watch\nPhilippine EO Ecosystem Overview (20 min) - Watch",
    "crumbs": [
      "Home",
      "Resources",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#installation-scripts",
    "href": "resources/downloads.html#installation-scripts",
    "title": "Downloads",
    "section": "Installation Scripts",
    "text": "Installation Scripts\n\nLocal Python Environment Setup\nFor participants who want to work locally (not in Colab):\n\nrequirements.txt\nnumpy&gt;=1.24.0\npandas&gt;=2.0.0\nmatplotlib&gt;=3.7.0\ngeopandas&gt;=0.13.0\nrasterio&gt;=1.3.0\nearthengine-api&gt;=0.1.350\nfolium&gt;=0.14.0\ngeemap&gt;=0.30.0\njupyter&gt;=1.0.0\nDownload requirements.txt\nInstallation:\n# Create virtual environment\npython -m venv eo-env\nsource eo-env/bin/activate  # On Windows: eo-env\\Scripts\\activate\n\n# Install packages\npip install -r requirements.txt\n\n# Verify installation\npython -c \"import geopandas; print('GeoPandas installed successfully!')\"\n\n\n\nConda Environment (Alternative)\n\nenvironment.yml\nname: copphil-day1\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.10\n  - numpy\n  - pandas\n  - matplotlib\n  - geopandas\n  - rasterio\n  - jupyter\n  - pip\n  - pip:\n    - earthengine-api\n    - geemap\nDownload environment.yml\nInstallation:\nconda env create -f environment.yml\nconda activate copphil-day1",
    "crumbs": [
      "Home",
      "Resources",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#license-attribution",
    "href": "resources/downloads.html#license-attribution",
    "title": "Downloads",
    "section": "License & Attribution",
    "text": "License & Attribution\n\n\n\n\n\n\nImportantUsage Terms\n\n\n\nTraining Materials License: - Licensed under Creative Commons BY-SA 4.0 - Free to use for educational purposes - Attribution required: “CopPhil EO AI/ML Training Programme”\nCode Examples: - Licensed under MIT License - Free to use, modify, and distribute\nSample Datasets: - Copernicus Sentinel data: Free and open (Copernicus Data Policy) - Philippine administrative boundaries: Open Data (government sources) - Processed derivatives: CC BY-SA 4.0\nCitation:\nCopPhil Training Programme (2025). Day 1: EO Data, AI/ML Fundamentals &\nGeospatial Python. EU-Philippines Copernicus Capacity Support Programme.",
    "crumbs": [
      "Home",
      "Resources",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#support-feedback",
    "href": "resources/downloads.html#support-feedback",
    "title": "Downloads",
    "section": "Support & Feedback",
    "text": "Support & Feedback\n\nHaving Issues?\n\nCheck the FAQ for common problems\nReview the Setup Guide for installation help\nContact training coordinators: training@philsa.gov.ph\n\n\n\nSuggest Improvements\n\nReport broken links or errors\nRequest additional materials\nShare feedback on content\n\nGitHub Issues: Report Issue",
    "crumbs": [
      "Home",
      "Resources",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#download-statistics",
    "href": "resources/downloads.html#download-statistics",
    "title": "Downloads",
    "section": "Download Statistics",
    "text": "Download Statistics\n\nMost Downloaded: 1. Complete Day 1 Package (ZIP) 2. Session 3 Jupyter Notebook 3. GeoPandas Cheat Sheet 4. Session 4 Jupyter Notebook 5. Palawan Land Cover Dataset\nTotal Downloads This Month: 1,247\n\n\nAll materials are regularly updated. Check back for new resources and improved versions.",
    "crumbs": [
      "Home",
      "Resources",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/glossary.html",
    "href": "resources/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "This glossary defines key terms used throughout the CopPhil EO AI/ML Training Programme. Terms are organized alphabetically within categories for easy reference.\nCategories: - Earth Observation Terms - AI/ML Terms - Geospatial Data Terms - Satellite & Sensor Terms - Philippine EO Organizations - Technical Acronyms",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary"
    ]
  },
  {
    "objectID": "resources/glossary.html#how-to-use-this-glossary",
    "href": "resources/glossary.html#how-to-use-this-glossary",
    "title": "Glossary",
    "section": "",
    "text": "This glossary defines key terms used throughout the CopPhil EO AI/ML Training Programme. Terms are organized alphabetically within categories for easy reference.\nCategories: - Earth Observation Terms - AI/ML Terms - Geospatial Data Terms - Satellite & Sensor Terms - Philippine EO Organizations - Technical Acronyms",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary"
    ]
  },
  {
    "objectID": "resources/glossary.html#earth-observation-terms",
    "href": "resources/glossary.html#earth-observation-terms",
    "title": "Glossary",
    "section": "Earth Observation Terms",
    "text": "Earth Observation Terms\n\nAbsorption Band\nWavelength region where atmospheric gases (water vapor, oxygen, CO2) absorb electromagnetic radiation, limiting remote sensing capabilities.\n\n\nBackscatter\nThe portion of radar energy reflected back to the sensor from a target. Used in SAR imaging to detect surface properties and moisture.\n\n\nCloud Masking\nThe process of identifying and removing cloud-contaminated pixels from optical satellite imagery to improve data quality.\n\n\nComposite Image\nA single image created by combining multiple images from different dates, often using statistical methods (median, mean) to reduce noise and clouds.\n\n\nEarth Observation (EO)\nThe gathering of information about Earth’s physical, chemical, and biological systems through remote sensing technologies, primarily satellites.\n\n\nFalse Color Composite\nAn image display where spectral bands are assigned to RGB colors differently than natural vision (e.g., NIR-Red-Green), revealing features invisible to the human eye.\n\n\nGround Truth\nField-collected reference data used to validate remote sensing classifications and train machine learning models.\n\n\nImage Collection\nA set of satellite images covering the same geographic area at different times, used for time series analysis.\n\n\nPixel\nThe smallest unit in a raster image, representing a specific ground area (spatial resolution) and spectral value.\n\n\nPreprocessing\nSteps taken to correct raw satellite data before analysis, including atmospheric correction, geometric correction, and radiometric calibration.\n\n\nRemote Sensing\nThe science of obtaining information about objects or areas from a distance, typically using sensors on satellites or aircraft.\n\n\nRevisit Time\nThe frequency with which a satellite can observe the same location on Earth (e.g., Sentinel-2 has 5-day revisit with 3 satellites).\n\n\nSpectral Signature\nThe unique reflectance pattern of an object across different wavelengths, used to identify materials and land cover types.\n\n\nTrue Color Composite\nAn image display using red, green, and blue bands to create a natural-looking image similar to human vision.",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary"
    ]
  },
  {
    "objectID": "resources/glossary.html#aiml-terms",
    "href": "resources/glossary.html#aiml-terms",
    "title": "Glossary",
    "section": "AI/ML Terms",
    "text": "AI/ML Terms\n\nActivation Function\nMathematical function in neural networks that introduces non-linearity (e.g., ReLU, Sigmoid, Tanh), enabling learning of complex patterns.\n\n\nArtificial Intelligence (AI)\nComputer systems capable of performing tasks that typically require human intelligence, including perception, reasoning, and decision-making.\n\n\nBackpropagation\nAlgorithm for training neural networks by calculating gradients of loss and adjusting weights to minimize error.\n\n\nBatch Size\nNumber of training samples processed before updating model weights. Smaller batches = more updates but noisier; larger batches = smoother but fewer updates.\n\n\nClassification\nSupervised learning task of assigning input data to predefined categories (e.g., forest, water, urban).\n\n\nClustering\nUnsupervised learning technique that groups similar data points together without predefined labels (e.g., K-means, DBSCAN).\n\n\nConfusion Matrix\nTable showing predicted vs. actual classifications, used to calculate accuracy, precision, recall, and F1-score.\n\n\nConvolutional Neural Network (CNN)\nDeep learning architecture specialized for image analysis, using convolutional layers to detect spatial patterns.\n\n\nData Augmentation\nTechnique to artificially increase training data by applying transformations (rotation, flipping, scaling) to existing samples.\n\n\nDeep Learning\nSubset of machine learning using multi-layer neural networks to learn hierarchical representations of data.\n\n\nEpoch\nOne complete pass through the entire training dataset during model training.\n\n\nFeature Engineering\nThe process of creating new input variables from raw data to improve model performance.\n\n\nFeature Extraction\nIdentifying and extracting relevant patterns or characteristics from raw data for use in machine learning models.\n\n\nGround Truth Labels\nVerified, accurate labels for training data, typically from field surveys or expert interpretation.\n\n\nHyperparameter\nModel configuration setting chosen before training (e.g., learning rate, number of layers) that affects model performance.\n\n\nLoss Function\nMathematical function measuring the difference between predicted and actual values, used to guide model training.\n\n\nMachine Learning (ML)\nSubset of AI enabling systems to learn and improve from experience without explicit programming.\n\n\nNeural Network\nComputing system inspired by biological brains, consisting of interconnected nodes (neurons) organized in layers.\n\n\nOverfitting\nWhen a model learns training data too well, including noise, resulting in poor performance on new data.\n\n\nPrecision\nProportion of positive predictions that are actually correct. Precision = TP / (TP + FP).\n\n\nRandom Forest\nEnsemble learning method using multiple decision trees to improve prediction accuracy and reduce overfitting.\n\n\nRecall (Sensitivity)\nProportion of actual positives correctly identified. Recall = TP / (TP + FN).\n\n\nRegression\nSupervised learning task of predicting continuous numerical values (e.g., crop yield, temperature).\n\n\nSupervised Learning\nMachine learning where models learn from labeled training data (input-output pairs).\n\n\nSupport Vector Machine (SVM)\nClassification algorithm that finds the optimal hyperplane separating different classes in feature space.\n\n\nTraining Set\nPortion of data used to train a machine learning model (typically 70-80% of total data).\n\n\nTransfer Learning\nReusing a pre-trained model on a new but related task, reducing training time and data requirements.\n\n\nUnderfitting\nWhen a model is too simple to capture the underlying patterns in data, resulting in poor performance.\n\n\nUnsupervised Learning\nMachine learning where models find patterns in unlabeled data without predefined categories.\n\n\nValidation Set\nData used to evaluate model performance during training and tune hyperparameters (typically 10-15% of data).",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary"
    ]
  },
  {
    "objectID": "resources/glossary.html#geospatial-data-terms",
    "href": "resources/glossary.html#geospatial-data-terms",
    "title": "Glossary",
    "section": "Geospatial Data Terms",
    "text": "Geospatial Data Terms\n\nAffine Transformation\nMathematical operation describing the relationship between pixel coordinates and geographic coordinates in raster data.\n\n\nBounding Box\nRectangular area defined by minimum and maximum coordinates [minX, minY, maxX, maxY], used to specify geographic extents.\n\n\nCoordinate Reference System (CRS)\nSystem defining how coordinates relate to real-world locations, including datum and projection (e.g., WGS84, UTM).\n\n\nDigital Elevation Model (DEM)\nRaster representation of terrain elevation, with each pixel value representing height above a reference level.\n\n\nFeature\nIn geospatial terms, a vector object (point, line, or polygon) with associated attributes.\n\n\nGeoJSON\nOpen standard JSON format for encoding geographic data structures, widely used for web mapping.\n\n\nGeoPackage\nOpen format for geospatial data storage in SQLite database, supporting both vector and raster data.\n\n\nGeometry\nThe spatial component of a geographic feature, defining its shape and location (point, line, polygon).\n\n\nGeoTIFF\nRaster image format with embedded geographic metadata (CRS, extent, resolution), standard for geospatial raster data.\n\n\nNoData Value\nSpecial value in raster data indicating missing or invalid data (e.g., -9999, NaN).\n\n\nPixel Resolution (Spatial Resolution)\nGround area represented by one pixel (e.g., 10m means each pixel covers 10m × 10m on the ground).\n\n\nProjection\nMathematical transformation converting 3D Earth coordinates to 2D map coordinates (e.g., Mercator, UTM).\n\n\nRaster Data\nGrid-based spatial data where each cell (pixel) contains a value, used for continuous phenomena (elevation, temperature, imagery).\n\n\nReproject\nConverting geospatial data from one coordinate reference system to another.\n\n\nShapefile\nPopular vector data format for GIS, consisting of multiple files (.shp, .shx, .dbf, .prj).\n\n\nSpatial Join\nCombining attributes from two geospatial datasets based on their spatial relationship (intersection, within, etc.).\n\n\nVector Data\nSpatial data representing discrete features as points, lines, or polygons with associated attributes.\n\n\nWell-Known Text (WKT)\nText markup language for representing vector geometry and spatial reference systems.",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary"
    ]
  },
  {
    "objectID": "resources/glossary.html#satellite-sensor-terms",
    "href": "resources/glossary.html#satellite-sensor-terms",
    "title": "Glossary",
    "section": "Satellite & Sensor Terms",
    "text": "Satellite & Sensor Terms\n\nActive Sensor\nSensor that emits its own energy and measures the reflected signal (e.g., SAR, LiDAR).\n\n\nAperture\nOpening in a sensor that controls the amount of light collected, affecting image brightness and quality.\n\n\nAtmospheric Correction\nProcessing step removing atmospheric effects (scattering, absorption) to retrieve surface reflectance.\n\n\nC-band\nRadar frequency band (4-8 GHz, wavelength 3.75-7.5 cm) used by Sentinel-1, good for vegetation and soil moisture.\n\n\nElectromagnetic Spectrum\nRange of all electromagnetic radiation wavelengths, from radio waves to gamma rays, including visible light.\n\n\nGeometric Correction\nCorrecting image distortions caused by sensor viewing angle, terrain, and Earth’s curvature.\n\n\nLevel 1C (L1C)\nSentinel-2 product with Top-of-Atmosphere (TOA) reflectance, geometrically corrected.\n\n\nLevel 2A (L2A)\nSentinel-2 product with Bottom-of-Atmosphere (BOA) surface reflectance, atmospherically corrected.\n\n\nLiDAR\nLight Detection and Ranging - active sensor using laser pulses to measure distance, creating high-resolution 3D point clouds.\n\n\nMultispectral\nImaging system capturing data in multiple (typically 3-15) wavelength bands across visible and infrared spectrum.\n\n\nNear Infrared (NIR)\nElectromagnetic radiation with wavelengths 0.7-1.4 μm, strongly reflected by healthy vegetation.\n\n\nOptical Sensor\nPassive sensor detecting reflected sunlight in visible and infrared wavelengths (e.g., Sentinel-2, Landsat).\n\n\nOrbit\nPath of a satellite around Earth, characterized by altitude, inclination, and period.\n\n\nPanchromatic\nSingle-band imagery capturing all visible wavelengths, typically at higher spatial resolution than multispectral bands.\n\n\nPassive Sensor\nSensor detecting naturally available energy, typically reflected sunlight (e.g., optical cameras).\n\n\nPolarization\nOrientation of radar waves (HH, VV, HV, VH), providing information about surface structure and moisture.\n\n\nRadiometric Calibration\nConverting raw sensor digital numbers to physical units (radiance or reflectance).\n\n\nSAR (Synthetic Aperture Radar)\nActive microwave imaging system creating high-resolution images independent of sunlight and clouds.\n\n\nShort-Wave Infrared (SWIR)\nElectromagnetic radiation with wavelengths 1.4-3.0 μm, useful for detecting moisture and minerals.\n\n\nSpectral Resolution\nAbility to distinguish between different wavelengths, determined by number and width of spectral bands.\n\n\nSun-Synchronous Orbit\nSatellite orbit maintaining constant local solar time, ensuring consistent illumination conditions.\n\n\nSwath Width\nWidth of the ground strip imaged by a satellite in a single pass (e.g., Sentinel-2 has 290 km swath).\n\n\nTemporal Resolution\nFrequency of repeat observations over the same location (synonymous with revisit time).\n\n\nThermal Infrared (TIR)\nElectromagnetic radiation with wavelengths 8-14 μm, measuring heat emitted by Earth’s surface.",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary"
    ]
  },
  {
    "objectID": "resources/glossary.html#philippine-eo-organizations",
    "href": "resources/glossary.html#philippine-eo-organizations",
    "title": "Glossary",
    "section": "Philippine EO Organizations",
    "text": "Philippine EO Organizations\n\nCopPhil Programme\nEU-Philippines Copernicus Capacity Support Programme - partnership to strengthen Philippine EO capabilities, establish a Copernicus Mirror Site, and develop AI/ML capacity.\n\n\nDENR\nDepartment of Environment and Natural Resources - responsible for forest monitoring, land cover mapping, and natural resource management using EO data.\n\n\nDOST\nDepartment of Science and Technology - leads science and technology advancement, co-chairs CopPhil programme, operates PAGASA and ASTI.\n\n\nDOST-ASTI\nAdvanced Science and Technology Institute - ICT research and development, AI platforms (SkAI-Pinas, DIMER, AIPI).\n\n\nLiPAD\nLiDAR Portal for Archiving and Distribution - repository of high-resolution elevation data for the Philippines.\n\n\nNAMRIA\nNational Mapping and Resource Information Authority - official mapping agency, operates GeoPortal and Resource Data Analysis Center.\n\n\nNDRRMC\nNational Disaster Risk Reduction and Management Council - coordinates disaster response, uses EO data for assessment and planning.\n\n\nPAGASA\nPhilippine Atmospheric, Geophysical and Astronomical Services Administration - weather, climate, and astronomical services.\n\n\nPhilGIS\nPhilippine GIS Data Clearinghouse - repository of geospatial datasets for the Philippines.\n\n\nPhilSA\nPhilippine Space Agency - national space authority, co-chairs CopPhil programme, operates SIYASAT portal and develops Copernicus Mirror Site.\n\n\nSIYASAT Portal\nPhilSA’s secure data archive for NovaSAR-1 data and maritime monitoring products.",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary"
    ]
  },
  {
    "objectID": "resources/glossary.html#technical-acronyms",
    "href": "resources/glossary.html#technical-acronyms",
    "title": "Glossary",
    "section": "Technical Acronyms",
    "text": "Technical Acronyms\n\nAI\nArtificial Intelligence - computer systems performing tasks requiring human intelligence.\n\n\nAPI\nApplication Programming Interface - set of functions allowing software to interact with services (e.g., Earth Engine Python API).\n\n\nASTER\nAdvanced Spaceborne Thermal Emission and Reflection Radiometer - NASA sensor providing multispectral imagery.\n\n\nBOA\nBottom of Atmosphere - surface reflectance after atmospheric correction (Level 2A products).\n\n\nCCA\nClimate Change Adaptation - strategies and actions to adjust to climate change impacts.\n\n\nCNN\nConvolutional Neural Network - deep learning architecture for image analysis.\n\n\nCRS\nCoordinate Reference System - defines how coordinates relate to Earth’s surface.\n\n\nDEM\nDigital Elevation Model - raster representation of terrain elevation.\n\n\nDL\nDeep Learning - subset of ML using multi-layer neural networks.\n\n\nDRR\nDisaster Risk Reduction - strategies to minimize disaster impacts.\n\n\nEO\nEarth Observation - gathering information about Earth through remote sensing.\n\n\nESA\nEuropean Space Agency - operates Copernicus Sentinel missions.\n\n\nEVI\nEnhanced Vegetation Index - vegetation index less sensitive to atmospheric effects than NDVI.\n\n\nGEE\nGoogle Earth Engine - cloud platform for planetary-scale geospatial analysis.\n\n\nGIS\nGeographic Information System - software for capturing, managing, and analyzing spatial data.\n\n\nGPU\nGraphics Processing Unit - hardware accelerating deep learning computations.\n\n\nGNSS\nGlobal Navigation Satellite System - satellite-based positioning (GPS, Galileo, GLONASS).\n\n\nHDF\nHierarchical Data Format - file format for storing large scientific datasets.\n\n\nIW\nInterferometric Wide Swath - Sentinel-1 acquisition mode with 250 km swath.\n\n\nJAXA\nJapan Aerospace Exploration Agency - operates Japanese Earth observation satellites.\n\n\nLiDAR\nLight Detection and Ranging - laser-based remote sensing for elevation mapping.\n\n\nLULC\nLand Use Land Cover - classification of Earth’s surface into categories (forest, urban, agriculture, etc.).\n\n\nML\nMachine Learning - algorithms enabling systems to learn from data.\n\n\nMODIS\nModerate Resolution Imaging Spectroradiometer - NASA sensor providing daily global coverage.\n\n\nNASA\nNational Aeronautics and Space Administration - operates Landsat and other EO missions.\n\n\nNDBI\nNormalized Difference Built-up Index - spectral index for detecting built-up areas.\n\n\nNDVI\nNormalized Difference Vegetation Index - spectral index measuring vegetation health/density.\n\n\nNDWI\nNormalized Difference Water Index - spectral index for detecting water bodies.\n\n\nNIR\nNear Infrared - electromagnetic radiation beyond visible red (0.7-1.4 μm).\n\n\nNRM\nNatural Resource Management - sustainable management of natural resources using EO monitoring.\n\n\nRGB\nRed-Green-Blue - color model using three primary colors, or the corresponding image bands.\n\n\nRF\nRandom Forest - ensemble machine learning algorithm using multiple decision trees.\n\n\nRL\nReinforcement Learning - ML paradigm where agents learn through interaction with environment.\n\n\nSAR\nSynthetic Aperture Radar - active microwave imaging system.\n\n\nSCL\nScene Classification Layer - Sentinel-2 cloud and land cover classification mask.\n\n\nSNAP\nSentinel Application Platform - ESA’s free software for processing Sentinel data.\n\n\nSWIR\nShort-Wave Infrared - electromagnetic radiation with wavelengths 1.4-3.0 μm.\n\n\nSVM\nSupport Vector Machine - classification algorithm finding optimal separating hyperplane.\n\n\nTIR\nThermal Infrared - electromagnetic radiation measuring surface temperature (8-14 μm).\n\n\nTOA\nTop of Atmosphere - apparent reflectance before atmospheric correction (Level 1C products).\n\n\nUSGS\nUnited States Geological Survey - distributes Landsat and other EO data.\n\n\nUTM\nUniversal Transverse Mercator - widely-used projected coordinate system dividing Earth into zones.\n\n\nWGS84\nWorld Geodetic System 1984 - global geographic coordinate system (EPSG:4326).",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary"
    ]
  },
  {
    "objectID": "resources/glossary.html#spectral-indices",
    "href": "resources/glossary.html#spectral-indices",
    "title": "Glossary",
    "section": "Spectral Indices",
    "text": "Spectral Indices\n\nNDVI (Normalized Difference Vegetation Index)\nFormula: (NIR - Red) / (NIR + Red)\nRange: -1 to +1\nInterpretation: - High values (0.6-0.9): Dense vegetation - Moderate (0.2-0.5): Sparse vegetation - Near zero: Bare soil, rock - Negative: Water, clouds\nSentinel-2 Bands: (B8 - B4) / (B8 + B4)\n\n\n\nNDWI (Normalized Difference Water Index)\nMcFeeters Formula: (Green - NIR) / (Green + NIR)\nGao Formula: (NIR - SWIR) / (NIR + SWIR)\nRange: -1 to +1\nInterpretation: - Positive values: Water bodies - Negative: Land surfaces\nSentinel-2 (McFeeters): (B3 - B8) / (B3 + B8)\nSentinel-2 (Gao): (B8 - B11) / (B8 + B11)\n\n\n\nNDBI (Normalized Difference Built-up Index)\nFormula: (SWIR - NIR) / (SWIR + NIR)\nRange: -1 to +1\nInterpretation: - Positive values: Built-up areas - Negative: Vegetation, water\nSentinel-2: (B11 - B8) / (B11 + B8)\n\n\n\nEVI (Enhanced Vegetation Index)\nFormula: 2.5 × ((NIR - Red) / (NIR + 6×Red - 7.5×Blue + 1))\nRange: -1 to +1\nAdvantages: Less sensitive to atmospheric effects and soil background than NDVI\nSentinel-2: 2.5 × ((B8 - B4) / (B8 + 6×B4 - 7.5×B2 + 1))",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary"
    ]
  },
  {
    "objectID": "resources/glossary.html#quick-reference-tables",
    "href": "resources/glossary.html#quick-reference-tables",
    "title": "Glossary",
    "section": "Quick Reference Tables",
    "text": "Quick Reference Tables\n\nSentinel-2 Band Summary\n\n\n\nBand\nName\nWavelength (nm)\nResolution (m)\nTypical Use\n\n\n\n\nB1\nCoastal aerosol\n443\n60\nAtmospheric correction\n\n\nB2\nBlue\n490\n10\nBathymetry, soil/vegetation\n\n\nB3\nGreen\n560\n10\nPeak vegetation sensitivity\n\n\nB4\nRed\n665\n10\nVegetation discrimination\n\n\nB5\nRed Edge 1\n705\n20\nVegetation health\n\n\nB6\nRed Edge 2\n740\n20\nVegetation stress\n\n\nB7\nRed Edge 3\n783\n20\nVegetation stress\n\n\nB8\nNIR\n842\n10\nBiomass, water bodies\n\n\nB8A\nNarrow NIR\n865\n20\nAtmospheric correction\n\n\nB9\nWater vapor\n945\n60\nAtmospheric correction\n\n\nB11\nSWIR 1\n1610\n20\nMoisture, soil/vegetation\n\n\nB12\nSWIR 2\n2190\n20\nMoisture, burned areas\n\n\n\n\n\n\nCommon CRS for Philippines\n\n\n\n\n\n\n\n\n\nName\nEPSG Code\nType\nUse Case\n\n\n\n\nWGS84\n4326\nGeographic\nGlobal datasets, web maps\n\n\nUTM Zone 50N\n32650\nProjected\nWestern Mindanao\n\n\nUTM Zone 51N\n32651\nProjected\nLuzon, Visayas, most of Philippines\n\n\nUTM Zone 52N\n32652\nProjected\nEastern Mindanao\n\n\nPRS92\n4683\nGeographic\nPhilippine Reference System",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary"
    ]
  },
  {
    "objectID": "resources/glossary.html#related-resources",
    "href": "resources/glossary.html#related-resources",
    "title": "Glossary",
    "section": "Related Resources",
    "text": "Related Resources\nFor more detailed information:\n\nSetup Guide - Technical setup instructions\nCheat Sheets - Quick reference commands\nFAQ - Common questions and troubleshooting\nPhilippine EO Resources - Organizations and platforms\n\n\nThis glossary will be expanded in subsequent training days. Suggest additional terms via training@philsa.gov.ph",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary"
    ]
  },
  {
    "objectID": "resources/philippine-eo.html",
    "href": "resources/philippine-eo.html",
    "title": "Philippine EO Resources",
    "section": "",
    "text": "The Philippines has a growing Earth Observation ecosystem with multiple agencies, platforms, and initiatives dedicated to using satellite data for national development, disaster risk reduction, and environmental monitoring.\n\n\n\n\n\n\nNoteCopPhil Programme Context\n\n\n\nThis resource directory supports the EU-Philippines Copernicus Capacity Support Programme (CopPhil), strengthening Philippine EO capabilities through international collaboration.",
    "crumbs": [
      "Home",
      "Resources",
      "Philippine EO Resources"
    ]
  },
  {
    "objectID": "resources/philippine-eo.html#overview",
    "href": "resources/philippine-eo.html#overview",
    "title": "Philippine EO Resources",
    "section": "",
    "text": "The Philippines has a growing Earth Observation ecosystem with multiple agencies, platforms, and initiatives dedicated to using satellite data for national development, disaster risk reduction, and environmental monitoring.\n\n\n\n\n\n\nNoteCopPhil Programme Context\n\n\n\nThis resource directory supports the EU-Philippines Copernicus Capacity Support Programme (CopPhil), strengthening Philippine EO capabilities through international collaboration.",
    "crumbs": [
      "Home",
      "Resources",
      "Philippine EO Resources"
    ]
  },
  {
    "objectID": "resources/philippine-eo.html#key-philippine-eo-agencies",
    "href": "resources/philippine-eo.html#key-philippine-eo-agencies",
    "title": "Philippine EO Resources",
    "section": "Key Philippine EO Agencies",
    "text": "Key Philippine EO Agencies\n\n1. Philippine Space Agency (PhilSA)\n\nRole: National space authority and co-chair of CopPhil programme\nEstablished: 2019 (Republic Act No. 11363)\nMandate: - Formulate national space policy - Establish space infrastructure - Promote space science and technology - Coordinate space-related activities across government\nKey Platforms & Services:\n\nSIYASAT Portal\n\nURL: siyasat.philsa.gov.ph\nDescription: Secure data archive for NovaSAR-1 data and maritime monitoring\nAccess: Registration required for Philippine government agencies and authorized researchers\nData Available:\n\nNovaSAR-1 SAR imagery\nMaritime domain awareness products\nAutomatic Identification System (AIS) data\nShip detection and tracking\n\n\n\n\nCopernicus Mirror Site (Under Development)\n\nStatus: Being established under CopPhil programme\nPurpose: Local access to Copernicus Sentinel data\nBenefits:\n\nReduced latency for Philippine users\nGuaranteed data availability\nSupport for real-time applications\n\n\nContact: - Website: philsa.gov.ph - Email: info@philsa.gov.ph - Address: UP Technology Innovation Center, UP Diliman, Quezon City\n\n\n\n\n\n2. Department of Science and Technology (DOST)\n\nRole: Science and technology advancement, co-chair of CopPhil programme\nRelevant Agencies:\n\nDOST-ASTI (Advanced Science and Technology Institute)\n\nWebsite: asti.dost.gov.ph\nFocus: ICT research and development, AI platforms\n\nKey Platforms:\nSkAI-Pinas (Skills for AI in the Philippines) - AI capacity building platform - Machine learning training resources - Part of ₱2.6 billion AI investment (2024-2028)\nDIMER (Disaster and Information Management, Early-Warning, and Response) - Real-time disaster monitoring - Integration of EO data for hazard assessment\nAIPI (AI Philippines Initiative) - National AI strategy implementation - AI applications across sectors\n\n\nDOST-PAGASA (Philippine Atmospheric, Geophysical and Astronomical Services Administration)\n\nWebsite: pagasa.dost.gov.ph\nFocus: Weather, climate, and astronomical services\nEO Applications:\n\nWeather satellite data (Himawari-8, GOES)\nRainfall estimation\nTropical cyclone monitoring\nFlood forecasting\n\n\nContact: - DOST Main: dost.gov.ph - PAGASA Portal: bagong.pagasa.dost.gov.ph\n\n\n\n\n\n3. National Mapping and Resource Information Authority (NAMRIA)\n\nRole: National mapping and geospatial information authority\nWebsite: namria.gov.ph\nMandate: - National mapping and charting - Resource data generation - Geospatial information management\nKey Platforms:\n\nNAMRIA GeoPortal\n\nURL: geoportal.namria.gov.ph\nDescription: Access to Philippine geospatial datasets\nData Available:\n\nTopographic maps\nAdministrative boundaries\nDigital elevation models\nLand cover maps\nCoastal and bathymetric data\n\n\n\n\nResource Data Analysis Center (RDAC)\n\nSatellite data reception and processing\nAnalysis of Landsat, SPOT, and other EO data\nCustom mapping services for government agencies\n\nServices: - Map production and printing - Geodetic surveys - Hydrographic surveys - Resource assessment\nContact: - Email: info@namria.gov.ph - Address: Lawton Avenue, Fort Andres Bonifacio, Taguig City\n\n\n\n\n\n4. Department of Environment and Natural Resources (DENR)\n\nRole: Natural resource management and environmental protection\nWebsite: denr.gov.ph\nRelevant Offices:\n\nDENR-GSIS (Geographic Information System Services)\n\nForest monitoring using satellite imagery\nLand cover and land use mapping\nBiodiversity conservation planning\n\n\n\nDENR-Forest Management Bureau (FMB)\n\nWebsite: forestry.denr.gov.ph\nNational Greening Program monitoring\nForest health assessment\nIllegal logging detection using EO\n\nEO Applications: - Forest cover change detection - Mangrove mapping - Mining site monitoring - Protected area surveillance\n\n\n\n\n\n5. Other Key Agencies\n\nNational Disaster Risk Reduction and Management Council (NDRRMC)\n\nWebsite: ndrrmc.gov.ph\nCoordinates disaster response\nUses EO data for damage assessment and evacuation planning\n\n\n\nDepartment of Agriculture (DA)\n\nWebsite: da.gov.ph\nCrop monitoring using satellite imagery\nAgricultural drought assessment\nRice production forecasting\n\n\n\nLocal Water Utilities Administration (LWUA)\n\nWater resource mapping\nWatershed monitoring\nFlood hazard assessment",
    "crumbs": [
      "Home",
      "Resources",
      "Philippine EO Resources"
    ]
  },
  {
    "objectID": "resources/philippine-eo.html#international-collaboration-platforms",
    "href": "resources/philippine-eo.html#international-collaboration-platforms",
    "title": "Philippine EO Resources",
    "section": "International Collaboration Platforms",
    "text": "International Collaboration Platforms\n\n1. Copernicus Programme Access\n\nCopernicus Data Space Ecosystem\n\nURL: dataspace.copernicus.eu\nDescription: Official Copernicus data access platform (launched 2023)\nFeatures:\n\nFree access to all Sentinel missions\nSentiBoard dashboard for data discovery\nOn-demand processing services\nAPI access for automated workflows\n\n\nPhilippine Focus: - CopPhil Mirror Site integration (upcoming) - Dedicated training resources - Local technical support\n\n\n\n2. ESA Earth Observation Portal\n\nURL: earth.esa.int\nMission information and data access\nEducational resources\n\n\n\n3. NASA Earthdata\n\nURL: earthdata.nasa.gov\nAccess to NASA EO missions\nFree data download (registration required)",
    "crumbs": [
      "Home",
      "Resources",
      "Philippine EO Resources"
    ]
  },
  {
    "objectID": "resources/philippine-eo.html#philippine-eo-data-repositories",
    "href": "resources/philippine-eo.html#philippine-eo-data-repositories",
    "title": "Philippine EO Resources",
    "section": "Philippine EO Data Repositories",
    "text": "Philippine EO Data Repositories\n\n1. PhilGIS (Philippine GIS Data Clearinghouse)\n\nURL: philgis.org\nDescription: Repository of Philippine geospatial datasets\nData Types:\n\nAdministrative boundaries (up to barangay level)\nRoads and infrastructure\nDigital elevation models\nLand cover classifications\n\n\n\n\n2. LiPAD (LiDAR Portal for Archiving and Distribution)\n\nURL: lipad.dream.upd.edu.ph\nDescription: High-resolution LiDAR data for the Philippines\nCoverage: Major river basins and flood-prone areas\nProducts:\n\nDigital Terrain Models (DTM)\nDigital Surface Models (DSM)\nFlood hazard maps\nPoint clouds\n\n\n\n\n3. Philippine Geoportal\n\nURL: geoportal.gov.ph\nDescription: Government geospatial data portal\nData: Various government datasets",
    "crumbs": [
      "Home",
      "Resources",
      "Philippine EO Resources"
    ]
  },
  {
    "objectID": "resources/philippine-eo.html#university-research-centers",
    "href": "resources/philippine-eo.html#university-research-centers",
    "title": "Philippine EO Resources",
    "section": "University Research Centers",
    "text": "University Research Centers\n\n1. UP Training Center for Applied Geodesy and Photogrammetry (TCAGP)\n\nWebsite: tcagp.org\nFocus: Geomatics training and research\nLocation: University of the Philippines, Diliman\n\n\n\n2. UP Resilience Institute\n\nWebsite: ovpri.upd.edu.ph/resilience-institute\nFocus: Disaster risk reduction research\nProjects: Hazard mapping, vulnerability assessment\n\n\n\n3. ATENEO Manila Observatory\n\nWebsite: observatory.ph\nFocus: Climate science, disaster preparedness\nEO Applications: Climate modeling, typhoon research",
    "crumbs": [
      "Home",
      "Resources",
      "Philippine EO Resources"
    ]
  },
  {
    "objectID": "resources/philippine-eo.html#training-and-capacity-building",
    "href": "resources/philippine-eo.html#training-and-capacity-building",
    "title": "Philippine EO Resources",
    "section": "Training and Capacity Building",
    "text": "Training and Capacity Building\n\n1. PhilSA Digital Space Campus\n\nStatus: Under development (CopPhil programme)\nPurpose: Sustainable EO training infrastructure\nFeatures:\n\nOnline learning management system\nHands-on workshops\nCertification programmes\nCommunity forums\n\n\n\n\n2. DOST-ASTI Training Programs\n\nRegular workshops on AI/ML\nData science bootcamps\nOnline courses via SkAI-Pinas\n\n\n\n3. NAMRIA Training Center\n\nGIS and remote sensing courses\nSurveying and mapping certification\nCustom agency training",
    "crumbs": [
      "Home",
      "Resources",
      "Philippine EO Resources"
    ]
  },
  {
    "objectID": "resources/philippine-eo.html#philippine-eo-applications",
    "href": "resources/philippine-eo.html#philippine-eo-applications",
    "title": "Philippine EO Resources",
    "section": "Philippine EO Applications",
    "text": "Philippine EO Applications\n\nDisaster Risk Reduction (DRR)\nFlood Mapping - SAR-based flood extent detection - Risk assessment for urban areas - Example: Metro Manila, Cagayan Valley, Central Luzon\nTyphoon Monitoring - Damage assessment using pre/post-event imagery - Infrastructure impact evaluation - Agricultural loss estimation\nLandslide Detection - Change detection in mountainous regions - Vulnerability mapping - Early warning systems\n\n\nClimate Change Adaptation (CCA)\nDrought Monitoring - Agricultural drought in Mindanao - Water resource management - Crop stress detection\nCoastal Change - Shoreline erosion monitoring - Mangrove forest tracking - Sea level rise impacts\nUrban Heat Islands - Temperature mapping in Metro Manila - Green space planning - Climate adaptation strategies\n\n\nNatural Resource Management (NRM)\nForest Monitoring - Deforestation detection - Illegal logging surveillance - Reforestation progress tracking\nLand Cover Mapping - National land cover database updates - Urban expansion analysis - Agricultural land use planning\nMarine Resources - Coral reef health assessment - Coastal water quality - Marine protected area monitoring",
    "crumbs": [
      "Home",
      "Resources",
      "Philippine EO Resources"
    ]
  },
  {
    "objectID": "resources/philippine-eo.html#data-access-guides",
    "href": "resources/philippine-eo.html#data-access-guides",
    "title": "Philippine EO Resources",
    "section": "Data Access Guides",
    "text": "Data Access Guides\n\nHow to Access Philippine EO Data\n\nFor Government Employees\n\nRegister with SIYASAT portal (PhilSA)\nAccess NAMRIA GeoPortal with agency credentials\nRequest datasets through official channels\n\n\n\nFor Researchers\n\nSubmit data request to PhilSA/NAMRIA\nProvide research proposal and intended use\nSign data sharing agreements if required\n\n\n\nFor Students\n\nAccess open datasets via PhilGIS\nUse LiPAD portal for LiDAR data\nRequest educational access to restricted datasets\n\n\n\nFor International Collaborators\n\nCoordinate through CopPhil programme channels\nEstablish memoranda of understanding (MOUs)\nFollow data sharing protocols",
    "crumbs": [
      "Home",
      "Resources",
      "Philippine EO Resources"
    ]
  },
  {
    "objectID": "resources/philippine-eo.html#philippine-case-studies",
    "href": "resources/philippine-eo.html#philippine-case-studies",
    "title": "Philippine EO Resources",
    "section": "Philippine Case Studies",
    "text": "Philippine Case Studies\n\n1. Typhoon Yolanda (Haiyan) Response (2013)\n\nRapid damage mapping using SAR imagery\nCoordination with international EO community\nLessons learned for future disasters\n\n\n\n2. Marawi Crisis (2017)\n\nSatellite-based damage assessment\nInfrastructure monitoring\nPost-conflict reconstruction planning\n\n\n\n3. Taal Volcano Eruption (2020)\n\nAsh fall extent mapping\nEvacuation zone delineation\nEnvironmental impact assessment\n\n\n\n4. COVID-19 Pandemic Monitoring (2020-2023)\n\nUrban mobility analysis\nAir quality changes\nLand use shifts",
    "crumbs": [
      "Home",
      "Resources",
      "Philippine EO Resources"
    ]
  },
  {
    "objectID": "resources/philippine-eo.html#upcoming-initiatives",
    "href": "resources/philippine-eo.html#upcoming-initiatives",
    "title": "Philippine EO Resources",
    "section": "Upcoming Initiatives",
    "text": "Upcoming Initiatives\n\nCopPhil Programme Milestones (2024-2026)\n\n2024-2025: - Copernicus Mirror Site installation - Digital Space Campus platform launch - Pilot service co-development (DRR, CCA, NRM)\n2025-2026: - Advanced AI/ML training programmes - Operational EO services rollout - Regional capacity building workshops\n\n\n\nDOST AI Investment (2024-2028)\n\n₱2.6 billion for AI infrastructure\nIntegration of EO data with AI platforms\nNational AI talent development",
    "crumbs": [
      "Home",
      "Resources",
      "Philippine EO Resources"
    ]
  },
  {
    "objectID": "resources/philippine-eo.html#contact-directory",
    "href": "resources/philippine-eo.html#contact-directory",
    "title": "Philippine EO Resources",
    "section": "Contact Directory",
    "text": "Contact Directory\n\nPhilSA - Email: info@philsa.gov.ph - Phone: +63 (2) 8981-8500\nDOST - Email: dostncr@dost.gov.ph - Phone: +63 (2) 8837-2071\nNAMRIA - Email: info@namria.gov.ph - Phone: +63 (2) 8810-4831\nPAGASA - Email: inquiry@pagasa.dost.gov.ph - Phone: +63 (2) 8284-0800",
    "crumbs": [
      "Home",
      "Resources",
      "Philippine EO Resources"
    ]
  },
  {
    "objectID": "resources/philippine-eo.html#additional-resources",
    "href": "resources/philippine-eo.html#additional-resources",
    "title": "Philippine EO Resources",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPolicy Documents\n\nRepublic Act No. 11363 (Philippine Space Act of 2019)\nExecutive Order No. 358 (NDRRMC Reorganization)\nDENR Administrative Orders on forest monitoring\n\n\n\nReports and Publications\n\nPhilSA Strategic Roadmap 2020-2040\nPhilippine Development Plan 2023-2028\nNational Disaster Risk Reduction and Management Plan\n\n\n\nInternational Partnerships\n\nJAXA (Japan Aerospace Exploration Agency) - Data sharing agreements\nESA (European Space Agency) - CopPhil programme\nUSGS (U.S. Geological Survey) - Landsat data access\nUNOSAT - Disaster response support",
    "crumbs": [
      "Home",
      "Resources",
      "Philippine EO Resources"
    ]
  },
  {
    "objectID": "resources/philippine-eo.html#stay-updated",
    "href": "resources/philippine-eo.html#stay-updated",
    "title": "Philippine EO Resources",
    "section": "Stay Updated",
    "text": "Stay Updated\nFollow Philippine EO developments:\n\nPhilSA Newsletter: Subscribe at philsa.gov.ph\nCopPhil Updates: Check training portal announcements\nSocial Media: Follow @PhilSpaceAgency on Twitter/Facebook\n\n\n\n\n\n\n\n\nTipContribute to This Directory\n\n\n\nKnow of additional Philippine EO resources? Contact the training coordinators to suggest additions to this page.\n\n\n\nThis resource directory is maintained by the CopPhil Training Programme. Last updated: 2025.",
    "crumbs": [
      "Home",
      "Resources",
      "Philippine EO Resources"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html",
    "href": "resources/cheatsheets.html",
    "title": "Cheat Sheets",
    "section": "",
    "text": "Quick reference guides for the tools, libraries, and concepts covered in Day 1. Bookmark this page for easy access during hands-on exercises!",
    "crumbs": [
      "Home",
      "Resources",
      "Cheat Sheets"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html#overview",
    "href": "resources/cheatsheets.html#overview",
    "title": "Cheat Sheets",
    "section": "",
    "text": "Quick reference guides for the tools, libraries, and concepts covered in Day 1. Bookmark this page for easy access during hands-on exercises!",
    "crumbs": [
      "Home",
      "Resources",
      "Cheat Sheets"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html#python-basics-cheat-sheet",
    "href": "resources/cheatsheets.html#python-basics-cheat-sheet",
    "title": "Cheat Sheets",
    "section": "Python Basics Cheat Sheet",
    "text": "Python Basics Cheat Sheet\n\nData Types\n# Numbers\ninteger = 42\nfloat_num = 3.14\n\n# Strings\ntext = \"Hello EO\"\nmultiline = \"\"\"Multiple\nlines\"\"\"\n\n# Lists (mutable)\ncoords = [14.5995, 120.9842]  # Manila lat/lon\nsensors = [\"Sentinel-1\", \"Sentinel-2\", \"Landsat-8\"]\n\n# Tuples (immutable)\nbbox = (120.0, 14.0, 121.0, 15.0)\n\n# Dictionaries\nmetadata = {\n    \"satellite\": \"Sentinel-2\",\n    \"date\": \"2025-01-15\",\n    \"cloud_cover\": 12.5\n}\n\n\nControl Flow\n# If statements\nif cloud_cover &lt; 20:\n    print(\"Good quality image\")\nelif cloud_cover &lt; 50:\n    print(\"Moderate quality\")\nelse:\n    print(\"Too cloudy\")\n\n# For loops\nfor sensor in sensors:\n    print(f\"Processing {sensor} data\")\n\n# List comprehension\nvalid_images = [img for img in images if img.cloud_cover &lt; 20]\n\n# While loop\ncount = 0\nwhile count &lt; 10:\n    count += 1\n\n\nFunctions\n# Basic function\ndef calculate_ndvi(nir, red):\n    \"\"\"Calculate Normalized Difference Vegetation Index.\"\"\"\n    return (nir - red) / (nir + red)\n\n# Function with default arguments\ndef load_image(path, band=\"B04\", scale=10):\n    return ee.Image(path).select(band).reproject(scale=scale)\n\n# Lambda function\nsquare = lambda x: x ** 2",
    "crumbs": [
      "Home",
      "Resources",
      "Cheat Sheets"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html#numpy-cheat-sheet",
    "href": "resources/cheatsheets.html#numpy-cheat-sheet",
    "title": "Cheat Sheets",
    "section": "NumPy Cheat Sheet",
    "text": "NumPy Cheat Sheet\n\nArray Creation\nimport numpy as np\n\n# From lists\narr = np.array([1, 2, 3, 4, 5])\nmatrix = np.array([[1, 2], [3, 4]])\n\n# Special arrays\nzeros = np.zeros((3, 3))           # 3x3 array of zeros\nones = np.ones((2, 4))             # 2x4 array of ones\nidentity = np.eye(4)               # 4x4 identity matrix\nrandom = np.random.rand(3, 3)      # 3x3 random [0, 1)\nrange_arr = np.arange(0, 10, 2)    # [0, 2, 4, 6, 8]\nlinspace = np.linspace(0, 1, 5)    # 5 values from 0 to 1\n\n\nArray Operations\n# Arithmetic (element-wise)\na + b          # Addition\na - b          # Subtraction\na * b          # Multiplication\na / b          # Division\na ** 2         # Power\n\n# Statistics\narr.mean()     # Mean\narr.std()      # Standard deviation\narr.min()      # Minimum\narr.max()      # Maximum\narr.sum()      # Sum\n\n# Indexing\narr[0]         # First element\narr[-1]        # Last element\narr[1:4]       # Slice indices 1-3\nmatrix[0, :]   # First row\nmatrix[:, 1]   # Second column\n\n# Boolean indexing\narr[arr &gt; 5]   # Elements greater than 5\n\n\nArray Manipulation\n# Shape operations\narr.reshape(3, 2)        # Reshape to 3x2\narr.flatten()            # Flatten to 1D\narr.transpose()          # Transpose\n\n# Stacking\nnp.vstack([a, b])        # Vertical stack\nnp.hstack([a, b])        # Horizontal stack\nnp.stack([a, b], axis=0) # Stack along axis\n\n# Concatenation\nnp.concatenate([a, b])",
    "crumbs": [
      "Home",
      "Resources",
      "Cheat Sheets"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html#geopandas-cheat-sheet",
    "href": "resources/cheatsheets.html#geopandas-cheat-sheet",
    "title": "Cheat Sheets",
    "section": "GeoPandas Cheat Sheet",
    "text": "GeoPandas Cheat Sheet\n\nReading/Writing Vector Data\nimport geopandas as gpd\n\n# Read files\ngdf = gpd.read_file(\"data.shp\")\ngdf = gpd.read_file(\"data.geojson\")\ngdf = gpd.read_file(\"data.gpkg\")\n\n# Write files\ngdf.to_file(\"output.shp\")\ngdf.to_file(\"output.geojson\", driver=\"GeoJSON\")\ngdf.to_file(\"output.gpkg\", driver=\"GPKG\")\n\n\nGeoDataFrame Operations\n# Inspect data\ngdf.head()              # First 5 rows\ngdf.info()              # Column info\ngdf.describe()          # Statistics\ngdf.crs                 # Coordinate Reference System\ngdf.geometry            # Geometry column\ngdf.total_bounds        # Bounding box [minx, miny, maxx, maxy]\n\n# Filtering\nmetro_manila = gdf[gdf[\"region\"] == \"NCR\"]\nlarge_areas = gdf[gdf.area &gt; 1000000]\n\n# Sorting\ngdf.sort_values(\"population\", ascending=False)\n\n\nSpatial Operations\n# Coordinate Reference System\ngdf.to_crs(\"EPSG:4326\")          # Reproject to WGS84\ngdf.to_crs(\"EPSG:32651\")         # Reproject to UTM Zone 51N\n\n# Geometric properties\ngdf.area                          # Area\ngdf.length                        # Perimeter/length\ngdf.centroid                      # Centroids\ngdf.bounds                        # Bounding boxes\n\n# Spatial relationships\ngdf1.intersects(gdf2)            # Intersection check\ngdf1.contains(point)             # Containment check\ngdf1.within(polygon)             # Within check\n\n# Spatial joins\ngpd.sjoin(points, polygons, how=\"inner\", predicate=\"within\")\n\n# Overlay operations\ngpd.overlay(gdf1, gdf2, how=\"intersection\")\ngpd.overlay(gdf1, gdf2, how=\"union\")\ngpd.overlay(gdf1, gdf2, how=\"difference\")\n\n\nVisualization\n# Basic plot\ngdf.plot()\n\n# Styled plot\ngdf.plot(column=\"population\",\n         cmap=\"YlOrRd\",\n         legend=True,\n         figsize=(10, 8))\n\n# Multiple layers\nax = gdf1.plot(color=\"blue\", alpha=0.5)\ngdf2.plot(ax=ax, color=\"red\", alpha=0.5)",
    "crumbs": [
      "Home",
      "Resources",
      "Cheat Sheets"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html#rasterio-cheat-sheet",
    "href": "resources/cheatsheets.html#rasterio-cheat-sheet",
    "title": "Cheat Sheets",
    "section": "Rasterio Cheat Sheet",
    "text": "Rasterio Cheat Sheet\n\nReading Raster Data\nimport rasterio\nfrom rasterio.plot import show\n\n# Open raster\nwith rasterio.open(\"image.tif\") as src:\n    # Metadata\n    print(src.crs)          # CRS\n    print(src.bounds)       # Bounding box\n    print(src.shape)        # (height, width)\n    print(src.count)        # Number of bands\n    print(src.transform)    # Affine transform\n\n    # Read data\n    band1 = src.read(1)     # Read band 1\n    all_bands = src.read()  # Read all bands\n\n    # Windowed read\n    window = rasterio.windows.Window(0, 0, 512, 512)\n    subset = src.read(1, window=window)\n\n\nWriting Raster Data\n# Write single band\nwith rasterio.open(\n    \"output.tif\",\n    \"w\",\n    driver=\"GTiff\",\n    height=data.shape[0],\n    width=data.shape[1],\n    count=1,\n    dtype=data.dtype,\n    crs=\"EPSG:32651\",\n    transform=transform\n) as dst:\n    dst.write(data, 1)\n\n# Write multiple bands\nwith rasterio.open(\"output.tif\", \"w\", ...) as dst:\n    for i, band in enumerate(bands, start=1):\n        dst.write(band, i)\n\n\nRaster Operations\n# Reproject\nfrom rasterio.warp import reproject, Resampling\n\nreproject(\n    source=src_array,\n    destination=dst_array,\n    src_transform=src.transform,\n    src_crs=src.crs,\n    dst_transform=dst_transform,\n    dst_crs=\"EPSG:4326\",\n    resampling=Resampling.bilinear\n)\n\n# Masking\nfrom rasterio.mask import mask\n\nwith rasterio.open(\"image.tif\") as src:\n    clipped, transform = mask(src, shapes, crop=True)\n\n# Calculate indices\nwith rasterio.open(\"sentinel2.tif\") as src:\n    red = src.read(4).astype(float)\n    nir = src.read(8).astype(float)\n    ndvi = (nir - red) / (nir + red)\n\n\nVisualization\nfrom rasterio.plot import show\n\n# Single band\nwith rasterio.open(\"image.tif\") as src:\n    show(src, cmap=\"gray\")\n\n# RGB composite\nwith rasterio.open(\"image.tif\") as src:\n    show((src, [4, 3, 2]))  # True color (R, G, B)",
    "crumbs": [
      "Home",
      "Resources",
      "Cheat Sheets"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html#google-earth-engine-python-api-cheat-sheet",
    "href": "resources/cheatsheets.html#google-earth-engine-python-api-cheat-sheet",
    "title": "Cheat Sheets",
    "section": "Google Earth Engine (Python API) Cheat Sheet",
    "text": "Google Earth Engine (Python API) Cheat Sheet\n\nInitialization\nimport ee\n\n# Authenticate (first time only)\nee.Authenticate()\n\n# Initialize\nee.Initialize()\n\n\nImage Operations\n# Load single image\nimage = ee.Image(\"COPERNICUS/S2/20250115T012345_20250115T012345_T51PTS\")\n\n# Load from collection\ncollection = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\")\nimage = collection.first()\n\n# Select bands\nrgb = image.select([\"B4\", \"B3\", \"B2\"])\nnir = image.select(\"B8\")\n\n# Band math\nndvi = image.normalizedDifference([\"B8\", \"B4\"]).rename(\"NDVI\")\n\n# Or manually\nnir = image.select(\"B8\")\nred = image.select(\"B4\")\nndvi = nir.subtract(red).divide(nir.add(red))\n\n\nImageCollection Filtering\n# Spatial filter\nroi = ee.Geometry.Rectangle([120.0, 14.0, 121.0, 15.0])\nfiltered = collection.filterBounds(roi)\n\n# Temporal filter\nfiltered = collection.filterDate(\"2024-01-01\", \"2024-12-31\")\n\n# Metadata filter\nlow_cloud = collection.filter(ee.Filter.lt(\"CLOUDY_PIXEL_PERCENTAGE\", 20))\n\n# Combined\nfiltered = (collection\n    .filterBounds(roi)\n    .filterDate(\"2024-01-01\", \"2024-12-31\")\n    .filter(ee.Filter.lt(\"CLOUDY_PIXEL_PERCENTAGE\", 20)))\n\n\nCloud Masking\n# Sentinel-2 cloud masking\ndef mask_s2_clouds(image):\n    qa = image.select(\"QA60\")\n    cloud_mask = qa.bitwiseAnd(1 &lt;&lt; 10).eq(0).And(\n                 qa.bitwiseAnd(1 &lt;&lt; 11).eq(0))\n    return image.updateMask(cloud_mask)\n\n# Apply to collection\nmasked = collection.map(mask_s2_clouds)\n\n\nReducers\n# Temporal reduction\nmedian = collection.median()\nmean = collection.mean()\nmax_val = collection.max()\n\n# Spatial reduction\nmean_value = image.reduceRegion(\n    reducer=ee.Reducer.mean(),\n    geometry=roi,\n    scale=10\n).getInfo()\n\n# Percentile\npercentile_90 = collection.reduce(ee.Reducer.percentile([90]))\n\n\nCompositing\n# Median composite\ncomposite = (collection\n    .filterBounds(roi)\n    .filterDate(\"2024-06-01\", \"2024-08-31\")\n    .median())\n\n# Quality mosaic (least cloudy pixels)\ncomposite = collection.qualityMosaic(\"B8\")\n\n\nExport\n# Export to Drive\ntask = ee.batch.Export.image.toDrive(\n    image=ndvi,\n    description=\"NDVI_Export\",\n    folder=\"EarthEngine\",\n    fileNamePrefix=\"ndvi_palawan\",\n    scale=10,\n    region=roi,\n    maxPixels=1e13\n)\ntask.start()\n\n# Check status\nprint(task.status())\n\n# Export to Asset\ntask = ee.batch.Export.image.toAsset(\n    image=composite,\n    description=\"Composite_Export\",\n    assetId=\"users/yourname/composite\",\n    scale=10,\n    region=roi\n)\n\n\nVisualization\n# In Jupyter with geemap\nimport geemap\n\nMap = geemap.Map()\nMap.centerObject(roi, 10)\n\n# Add image\nvis_params = {\n    \"bands\": [\"B4\", \"B3\", \"B2\"],\n    \"min\": 0,\n    \"max\": 3000,\n    \"gamma\": 1.4\n}\nMap.addLayer(image, vis_params, \"Sentinel-2\")\n\n# Add NDVI\nndvi_vis = {\n    \"min\": 0,\n    \"max\": 1,\n    \"palette\": [\"red\", \"yellow\", \"green\"]\n}\nMap.addLayer(ndvi, ndvi_vis, \"NDVI\")\n\nMap",
    "crumbs": [
      "Home",
      "Resources",
      "Cheat Sheets"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html#sentinel-mission-quick-reference",
    "href": "resources/cheatsheets.html#sentinel-mission-quick-reference",
    "title": "Cheat Sheets",
    "section": "Sentinel Mission Quick Reference",
    "text": "Sentinel Mission Quick Reference\n\nSentinel-1 (SAR)\n\n\n\nParameter\nValue\n\n\n\n\nType\nC-band SAR\n\n\nBands\nVV, VH\n\n\nResolution\n10m (IW mode)\n\n\nSwath\n250 km\n\n\nRevisit\n6 days (2 satellites)\n\n\nGEE Collection\nCOPERNICUS/S1_GRD\n\n\n\nCommon Applications: - Flood mapping (water detection) - Ship detection - Crop monitoring - Land subsidence\n\n\nSentinel-2 (Optical)\n\n\n\nBand\nName\nWavelength (nm)\nResolution (m)\n\n\n\n\nB1\nCoastal aerosol\n443\n60\n\n\nB2\nBlue\n490\n10\n\n\nB3\nGreen\n560\n10\n\n\nB4\nRed\n665\n10\n\n\nB5\nRed edge 1\n705\n20\n\n\nB6\nRed edge 2\n740\n20\n\n\nB7\nRed edge 3\n783\n20\n\n\nB8\nNIR\n842\n10\n\n\nB8A\nNarrow NIR\n865\n20\n\n\nB9\nWater vapor\n945\n60\n\n\nB11\nSWIR 1\n1610\n20\n\n\nB12\nSWIR 2\n2190\n20\n\n\n\nRevisit Time: 5 days (3 satellites: 2A, 2B, 2C)\nGEE Collections: - COPERNICUS/S2_SR_HARMONIZED (Surface Reflectance) - COPERNICUS/S2_HARMONIZED (Top of Atmosphere)",
    "crumbs": [
      "Home",
      "Resources",
      "Cheat Sheets"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html#common-spectral-indices",
    "href": "resources/cheatsheets.html#common-spectral-indices",
    "title": "Cheat Sheets",
    "section": "Common Spectral Indices",
    "text": "Common Spectral Indices\n\nNDVI (Vegetation)\n# Google Earth Engine\nndvi = image.normalizedDifference([\"B8\", \"B4\"])\n\n# NumPy/Rasterio\nndvi = (nir - red) / (nir + red)\n\n\nNDWI (Water)\n# Green - NIR (McFeeters)\nndwi = image.normalizedDifference([\"B3\", \"B8\"])\n\n# NIR - SWIR (Gao)\nmndwi = image.normalizedDifference([\"B8\", \"B11\"])\n\n\nNDBI (Built-up)\nndbi = image.normalizedDifference([\"B11\", \"B8\"])\n\n\nEVI (Enhanced Vegetation Index)\nevi = image.expression(\n    \"2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))\",\n    {\n        \"NIR\": image.select(\"B8\"),\n        \"RED\": image.select(\"B4\"),\n        \"BLUE\": image.select(\"B2\")\n    }\n)",
    "crumbs": [
      "Home",
      "Resources",
      "Cheat Sheets"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html#philippine-regions-provinces",
    "href": "resources/cheatsheets.html#philippine-regions-provinces",
    "title": "Cheat Sheets",
    "section": "Philippine Regions & Provinces",
    "text": "Philippine Regions & Provinces\n\nAdministrative Levels\n\nRegion (17) → Province (81) → Municipality/City → Barangay\n\n\n\nUseful Bounding Boxes (WGS84)\n\n\n\nArea\nBounds [W, S, E, N]\n\n\n\n\nPhilippines\n[116.0, 4.0, 127.0, 21.0]\n\n\nLuzon\n[119.5, 12.0, 122.5, 19.0]\n\n\nMetro Manila\n[120.9, 14.4, 121.15, 14.8]\n\n\nPalawan\n[117.0, 7.5, 120.0, 12.0]\n\n\nMindanao\n[121.0, 5.0, 127.0, 10.0]",
    "crumbs": [
      "Home",
      "Resources",
      "Cheat Sheets"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html#keyboard-shortcuts",
    "href": "resources/cheatsheets.html#keyboard-shortcuts",
    "title": "Cheat Sheets",
    "section": "Keyboard Shortcuts",
    "text": "Keyboard Shortcuts\n\nGoogle Colab\n\nRun cell: Shift + Enter\nInsert cell above: Ctrl/Cmd + M A\nInsert cell below: Ctrl/Cmd + M B\nDelete cell: Ctrl/Cmd + M D\nInterrupt execution: Ctrl/Cmd + M I\nComment/uncomment: Ctrl/Cmd + /\n\n\n\nJupyter Notebook\n\nRun cell: Shift + Enter\nInsert cell below: B\nInsert cell above: A\nDelete cell: D D (press D twice)\nChange to markdown: M\nChange to code: Y",
    "crumbs": [
      "Home",
      "Resources",
      "Cheat Sheets"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html#common-error-messages",
    "href": "resources/cheatsheets.html#common-error-messages",
    "title": "Cheat Sheets",
    "section": "Common Error Messages",
    "text": "Common Error Messages\n\n“ee is not defined”\n# Solution: Initialize Earth Engine\nimport ee\nee.Initialize()\n\n\n“ModuleNotFoundError: No module named ‘geopandas’”\n# Solution: Install the package\n!pip install geopandas\n\n\n“RuntimeError: rasterio is not installed”\n# Solution: Install rasterio\n!pip install rasterio\n\n\n“User memory limit exceeded”\n# Solution: Reduce data scope\n# - Use smaller region\n# - Filter dates more strictly\n# - Increase scale parameter",
    "crumbs": [
      "Home",
      "Resources",
      "Cheat Sheets"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html#downloadable-pdfs",
    "href": "resources/cheatsheets.html#downloadable-pdfs",
    "title": "Cheat Sheets",
    "section": "Downloadable PDFs",
    "text": "Downloadable PDFs\n\n\n\n\n\n\nNotePrint-Friendly Versions\n\n\n\nDownload PDF versions of these cheat sheets for offline reference:\n\nPython Basics (PDF)\nGeoPandas Quick Reference (PDF)\nRasterio Commands (PDF)\nEarth Engine Python API (PDF)\nSentinel Missions (PDF)\n\nPDFs will be available in the Downloads section.",
    "crumbs": [
      "Home",
      "Resources",
      "Cheat Sheets"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html#additional-resources",
    "href": "resources/cheatsheets.html#additional-resources",
    "title": "Cheat Sheets",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nOfficial Documentation:\n\nPython Docs\nNumPy Docs\nGeoPandas Docs\nRasterio Docs\nEarth Engine Guides\n\nCommunity Cheat Sheets:\n\nNumPy Cheat Sheet (DataCamp)\nPandas Cheat Sheet\nEarth Engine Cheat Sheet\n\n\n\nBookmark this page for quick access during training exercises!",
    "crumbs": [
      "Home",
      "Resources",
      "Cheat Sheets"
    ]
  },
  {
    "objectID": "sessions/session4.html",
    "href": "sessions/session4.html",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "",
    "text": "Duration: 2 hours | Format: Hands-on Coding | Platform: Google Colaboratory + Earth Engine Python API",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "sessions/session4.html#session-overview",
    "href": "sessions/session4.html#session-overview",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Session Overview",
    "text": "Session Overview\nGoogle Earth Engine (GEE) is a planetary-scale platform for Earth science data and analysis. This session introduces you to GEE’s Python API, enabling you to access Sentinel-1 and Sentinel-2 data, filter massive image collections, perform cloud masking, create temporal composites, and export processed data - all without downloading terabytes of imagery. You’ll learn core GEE concepts and apply them to Philippine use cases.\n\nLearning Objectives\nBy the end of this session, you will be able to:\n\nExplain what Google Earth Engine is and its advantages for EO\nAuthenticate and initialize the Earth Engine Python API\nDefine core GEE concepts: Image, ImageCollection, Feature, FeatureCollection\nApply filters (spatial, temporal, metadata) to image collections\nAccess Sentinel-1 GRD and Sentinel-2 SR data catalogs\nImplement cloud masking using QA bands\nCreate temporal composites (median, mean) to reduce cloud cover\nCalculate spectral indices (NDVI, NDWI) at scale\nExport processed imagery to Google Drive\nUnderstand GEE’s capabilities and limitations for AI/ML workflows",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "sessions/session4.html#part-1-what-is-google-earth-engine",
    "href": "sessions/session4.html#part-1-what-is-google-earth-engine",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 1: What is Google Earth Engine?",
    "text": "Part 1: What is Google Earth Engine?\n\nOverview\nGoogle Earth Engine is a cloud-based platform combining:\n\nMulti-petabyte catalog of satellite imagery and geospatial datasets\nPlanetary-scale analysis capabilities via Google’s computational infrastructure\nCode Editor (JavaScript) and Python API for programmatic access\n\n\n\n\n\n\n\nNoteEarth Engine by the Numbers\n\n\n\n\n40+ years of historical imagery\n70+ petabytes of data\n700+ datasets including Landsat, Sentinel, MODIS, climate, terrain\nGlobal coverage updated daily\nFree for research, education, and non-profit use\n\n\n\n\n\nWhy Use Google Earth Engine?\nTraditional workflow problems:\n\nDownloading terabytes of satellite data\nStoring data locally (expensive storage)\nPre-processing each scene individually (time-consuming)\nLimited computational resources for large-area analysis\n\nEarth Engine solution:\n┌─────────────────────────────────────┐\n│   Your Computer                     │\n│   ┌──────────────┐                  │\n│   │ Write Code   │                  │\n│   │ (Python/JS)  │                  │\n│   └──────┬───────┘                  │\n│          │                           │\n│   ┌──────▼───────────────────────┐  │\n│   │ Send to Cloud                │  │\n│   └──────────────────────────────┘  │\n└──────────────┬──────────────────────┘\n               │\n               ▼\n┌──────────────────────────────────────────────┐\n│   Google Earth Engine Cloud                 │\n│   ┌──────────────┐  ┌──────────────┐        │\n│   │ Petabyte     │  │ Massive      │        │\n│   │ Data Catalog │  │ Computation  │        │\n│   └──────────────┘  └──────────────┘        │\n│                                              │\n│   Process → Results → Send back to you      │\n└──────────────────────────────────────────────┘\nKey advantages:\n\nNo downloading: Data stays in Google’s cloud\nParallel processing: Distributed computation across many machines\nPre-processed data: Analysis-ready collections (e.g., Sentinel-2 SR)\nTemporal analysis: Easily work with time series\nReproducible: Share code, not gigabytes of data\n\n\n\nUse Cases for Earth Observation\nIdeal for:\n\nLarge-area mapping (country/continent scale)\nMulti-temporal analysis (time series, change detection)\nRapid prototyping and exploration\nCloud-based pre-processing\nTeaching and learning (no infrastructure needed)\n\nLess ideal for:\n\nTraining custom deep learning models (CNNs, U-Net) - limited GPU support\nReal-time processing requiring millisecond latency\nWorkflows requiring full control over hardware\nProprietary/restricted datasets not in GEE catalog\n\n\nPhilippine Applications:\n\nNational land cover mapping: Process all of Philippines (~300,000 km²) at 10m resolution\nMulti-year deforestation monitoring: Annual forest loss detection 2015-2025\nTyphoon impact assessment: Before/after composites for disaster response\nRice paddy monitoring: Track planting/harvest cycles using SAR time series\nCoastal change detection: Erosion and accretion mapping using optical+SAR",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "sessions/session4.html#part-2-setting-up-earth-engine-in-python",
    "href": "sessions/session4.html#part-2-setting-up-earth-engine-in-python",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 2: Setting Up Earth Engine in Python",
    "text": "Part 2: Setting Up Earth Engine in Python\n\nPrerequisites\n\n\n\n\n\n\nWarningRequired Setup\n\n\n\nBefore using Earth Engine, you must:\n\nGoogle account (Gmail or G Suite)\nEarth Engine account - Register at earthengine.google.com\nCloud project - Create or select a project after registration\n\nRegistration is FREE and typically approved within 1-2 days.\n\n\n\n\nInstallation in Google Colab\nThe earthengine-api library is pre-installed in Colab, but let’s ensure it’s up-to-date:\n# Update Earth Engine API\n!pip install earthengine-api --upgrade -q\n\nprint(\"Earth Engine API updated! ✓\")\n\n\nAuthentication & Initialization\nFirst-time setup (one-time per environment):\nimport ee\n\n# Authenticate (opens browser window for authorization)\nee.Authenticate()\nThis will: 1. Open a new browser tab 2. Ask you to select your Google account 3. Request permission to access Earth Engine 4. Provide an authorization code 5. Automatically apply the code\n\n\n\n\n\n\nTipAuthentication Troubleshooting\n\n\n\nIf authentication fails:\n\nEnsure you’ve registered at earthengine.google.com\nCheck that you’re using the same Google account\nClear browser cookies and try again\nUse an incognito/private browsing window\n\n\n\nInitialize Earth Engine (required every session):\n# Initialize with your cloud project\n# Replace with your actual project ID\nee.Initialize(project='your-project-id')\n\nprint(\"Earth Engine initialized! ✓\")\nTo find your project ID: 1. Go to console.cloud.google.com 2. Select your project from the dropdown at the top 3. Copy the Project ID (not Project Name)\nComplete setup code:\nimport ee\nimport geemap  # Interactive mapping library\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Authenticate (first time only - comment out after first use)\n# ee.Authenticate()\n\n# Initialize\nee.Initialize(project='your-project-id')\n\n# Test that it works\nimage = ee.Image('USGS/SRTMGL1_003')\nprint(\"✓ Successfully connected to Earth Engine!\")\nprint(f\"  Test image bands: {image.bandNames().getInfo()}\")",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "sessions/session4.html#part-3-core-earth-engine-concepts",
    "href": "sessions/session4.html#part-3-core-earth-engine-concepts",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 3: Core Earth Engine Concepts",
    "text": "Part 3: Core Earth Engine Concepts\n\nThe Building Blocks\nEarth Engine has a unique data model optimized for planetary-scale analysis:\n\n\n\n\n\n\n\n\nConcept\nDescription\nExample\n\n\n\n\nImage\nSingle raster with multiple bands\nOne Sentinel-2 scene\n\n\nImageCollection\nStack of images (time series)\nAll Sentinel-2 over Philippines in 2024\n\n\nGeometry\nVector shapes\nPoint, polygon, line\n\n\nFeature\nGeometry + properties (attributes)\nProvince polygon with name, population\n\n\nFeatureCollection\nMultiple features\nAll Philippine provinces\n\n\n\n\n\n1. Geometry\nDefine locations and areas of interest:\n# Point (longitude, latitude)\nmanila = ee.Geometry.Point([121.0244, 14.5995])\n\n# Rectangle (min_lon, min_lat, max_lon, max_lat)\nbohol_bbox = ee.Geometry.Rectangle([123.8, 9.6, 124.6, 10.2])\n\n# Polygon (list of coordinate pairs)\ncustom_aoi = ee.Geometry.Polygon([\n    [[123.5, 9.5], [125.0, 9.5], [125.0, 11.0], [123.5, 11.0], [123.5, 9.5]]\n])\n\nprint(\"Geometries created! ✓\")\nprint(f\"Manila coordinates: {manila.coordinates().getInfo()}\")\nprint(f\"Bohol bbox area: {bohol_bbox.area().divide(1e6).getInfo():.2f} km²\")\n\n\n2. Image\nSingle raster image with one or more bands:\n# Load a Sentinel-2 image by ID\nsentinel2_image = ee.Image('COPERNICUS/S2_SR/20240315T015701_20240315T015659_T51PWN')\n\n# Examine properties\nprint(\"Image ID:\", sentinel2_image.id().getInfo())\nprint(\"Band names:\", sentinel2_image.bandNames().getInfo())\nprint(\"Cloud cover:\", sentinel2_image.get('CLOUDY_PIXEL_PERCENTAGE').getInfo(), \"%\")\n\n# Select specific bands\nrgb_bands = sentinel2_image.select(['B4', 'B3', 'B2'])  # Red, Green, Blue\nImage operations:\n# Calculate NDVI for single image\nndvi = sentinel2_image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n\n# Add as band to original image\nimage_with_ndvi = sentinel2_image.addBands(ndvi)\n\nprint(\"NDVI band added! ✓\")\n\n\n3. ImageCollection\nStack of images (time series):\n# Load Sentinel-2 collection\ns2_collection = ee.ImageCollection('COPERNICUS/S2_SR')\n\n# Check collection size (can be huge!)\nprint(\"Total Sentinel-2 images in catalog:\", s2_collection.size().getInfo())\n# This will be millions!\nImageCollection operations:\n# Filter by date, location, and cloud cover\nfiltered = (s2_collection\n    .filterBounds(bohol_bbox)\n    .filterDate('2024-01-01', '2024-12-31')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30)))\n\nprint(f\"Filtered to {filtered.size().getInfo()} images over Bohol in 2024 with &lt;30% clouds\")\n\n# Get image at specific index\nfirst_image = ee.Image(filtered.first())\nprint(\"First image date:\", first_image.date().format('YYYY-MM-dd').getInfo())\n\n\n4. Feature & FeatureCollection\nVector data with attributes:\n# Single feature (geometry + properties)\nmanila_feature = ee.Feature(\n    ee.Geometry.Point([121.0244, 14.5995]),\n    {'name': 'Manila', 'population': 1780148, 'country': 'Philippines'}\n)\n\nprint(\"Feature:\", manila_feature.getInfo())\n\n# Load feature collection (e.g., GADM administrative boundaries)\nphilippines = ee.FeatureCollection('FAO/GAUL/2015/level1').filter(\n    ee.Filter.eq('ADM0_NAME', 'Philippines')\n)\n\nprint(f\"Philippine provinces: {philippines.size().getInfo()}\")",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "sessions/session4.html#part-4-filtering-and-querying-data",
    "href": "sessions/session4.html#part-4-filtering-and-querying-data",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 4: Filtering and Querying Data",
    "text": "Part 4: Filtering and Querying Data\n\nFilter Types\nEarth Engine provides powerful filtering to reduce massive collections to relevant data.\n\n1. Spatial Filtering\nfilterBounds() - Keep images intersecting a geometry:\n# Define AOI\npalawan = ee.Geometry.Rectangle([117.5, 8.0, 119.5, 12.0])\n\n# Filter Sentinel-2 to Palawan\ns2_palawan = ee.ImageCollection('COPERNICUS/S2_SR').filterBounds(palawan)\n\nprint(f\"Total Sentinel-2 images over Palawan: {s2_palawan.size().getInfo()}\")\n\n\n2. Temporal Filtering\nfilterDate() - Keep images within date range:\n# Dry season 2024\ndry_season = s2_palawan.filterDate('2024-01-01', '2024-05-31')\nprint(f\"Dry season images: {dry_season.size().getInfo()}\")\n\n# Wet season 2024\nwet_season = s2_palawan.filterDate('2024-06-01', '2024-11-30')\nprint(f\"Wet season images: {wet_season.size().getInfo()}\")\n\n\n3. Metadata Filtering\nfilter() - Custom filters on image properties:\n# Low cloud cover\nclear_images = dry_season.filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))\nprint(f\"Clear images (&lt;10% clouds): {clear_images.size().getInfo()}\")\n\n# Specific satellite\ns2a_only = dry_season.filter(ee.Filter.eq('SPACECRAFT_NAME', 'Sentinel-2A'))\nprint(f\"Sentinel-2A only: {s2a_only.size().getInfo()}\")\n\n# Multiple conditions\nbest_images = (dry_season\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 5))\n    .filter(ee.Filter.eq('SPACECRAFT_NAME', 'Sentinel-2B'))\n)\nprint(f\"Best images (S-2B, &lt;5% clouds): {best_images.size().getInfo()}\")\n\n\n\nChain Filters for Precision\nCombine multiple filters:\n# Define AOI for Bohol\nbohol_aoi = ee.Geometry.Rectangle([123.8, 9.6, 124.6, 10.2])\n\n# Multi-filter pipeline\nbohol_images = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(bohol_aoi)\n    .filterDate('2024-03-01', '2024-05-31')  # Dry season\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n    .select(['B2', 'B3', 'B4', 'B8', 'B11', 'B12', 'QA60']))  # Relevant bands\n\nprint(\"=\" * 50)\nprint(f\"Bohol Image Collection (Dry Season 2024)\")\nprint(\"=\" * 50)\nprint(f\"  Total images: {bohol_images.size().getInfo()}\")\nprint(f\"  Date range: {bohol_images.aggregate_min('system:time_start').getInfo()} to {bohol_images.aggregate_max('system:time_start').getInfo()}\")\nprint(\"=\" * 50)",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "sessions/session4.html#part-5-working-with-sentinel-data-in-gee",
    "href": "sessions/session4.html#part-5-working-with-sentinel-data-in-gee",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 5: Working with Sentinel Data in GEE",
    "text": "Part 5: Working with Sentinel Data in GEE\n\nSentinel-2 Surface Reflectance\nCollection ID: COPERNICUS/S2_SR\nKey information:\n\nLevel: Level-2A (atmospherically corrected surface reflectance)\nBands: 13 spectral bands (B1-B12, plus QA60)\nResolution: 10m (B2-B4, B8), 20m (B5-B7, B8A, B11-B12), 60m (B1, B9, B10)\nRevisit: 5 days (constellation)\nUpdates: Near real-time (within days of acquisition)\n\nBand naming in GEE:\n\n\n\nBand\nName\nWavelength\nResolution\nUse\n\n\n\n\nB2\nBlue\n490 nm\n10m\nAtmospheric, water\n\n\nB3\nGreen\n560 nm\n10m\nVegetation, water\n\n\nB4\nRed\n665 nm\n10m\nVegetation discrimination\n\n\nB8\nNIR\n842 nm\n10m\nBiomass, vegetation\n\n\nB11\nSWIR1\n1610 nm\n20m\nMoisture, soil/vegetation\n\n\nB12\nSWIR2\n2190 nm\n20m\nMoisture, geology\n\n\nQA60\nQuality\n-\n60m\nCloud mask\n\n\n\nLoad and visualize:\n# Load collection\ns2 = ee.ImageCollection('COPERNICUS/S2_SR')\n\n# Filter to area and time\nimage = (s2\n    .filterBounds(ee.Geometry.Point([124.0, 10.0]))  # Bohol\n    .filterDate('2024-03-01', '2024-03-31')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))\n    .first())\n\n# Visualize with geemap (interactive mapping)\nimport geemap\n\nMap = geemap.Map(center=[10.0, 124.0], zoom=9)\nvis_params = {\n    'bands': ['B4', 'B3', 'B2'],\n    'min': 0,\n    'max': 3000,\n    'gamma': 1.4\n}\nMap.addLayer(image, vis_params, 'Sentinel-2 True Color')\nMap\n\n\nSentinel-1 SAR\nCollection ID: COPERNICUS/S1_GRD\nKey information:\n\nLevel: Level-1 Ground Range Detected (GRD)\nPolarization: VV, VH (or HH, HV depending on mode)\nResolution: 10m (IW mode)\nRevisit: 6-12 days\nAdvantages: All-weather, day-night imaging\n\nLoad Sentinel-1:\n# Load Sentinel-1 collection\ns1 = ee.ImageCollection('COPERNICUS/S1_GRD')\n\n# Filter for ascending pass, IW mode, VV+VH polarization\ns1_filtered = (s1\n    .filterBounds(bohol_aoi)\n    .filterDate('2024-06-01', '2024-08-31')  # Wet season\n    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n    .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n    .select(['VV', 'VH']))\n\nprint(f\"Sentinel-1 images: {s1_filtered.size().getInfo()}\")\n\n# Visualize\ns1_image = s1_filtered.median()  # Median composite\nMap = geemap.Map(center=[10.0, 124.0], zoom=9)\nMap.addLayer(s1_image, {'bands': ['VV'], 'min': -25, 'max': 0}, 'S1 VV')\nMap.addLayer(s1_image, {'bands': ['VH'], 'min': -30, 'max': -5}, 'S1 VH')\nMap",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "sessions/session4.html#part-6-cloud-masking-and-preprocessing",
    "href": "sessions/session4.html#part-6-cloud-masking-and-preprocessing",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 6: Cloud Masking and Preprocessing",
    "text": "Part 6: Cloud Masking and Preprocessing\n\nWhy Cloud Masking?\nProblems with clouds:\n\nObscure ground features\nContaminate spectral indices\nReduce classification accuracy\nCreate artifacts in composites\n\nSolution: Use quality assessment (QA) bands to identify and mask clouds.\n\n\nSentinel-2 Cloud Masking\nSentinel-2 includes QA60 band:\n\nBit 10: Opaque clouds\nBit 11: Cirrus clouds\n\nCloud masking function:\ndef mask_s2_clouds(image):\n    \"\"\"Mask clouds using QA60 band.\"\"\"\n    qa = image.select('QA60')\n\n    # Bits 10 and 11 are clouds and cirrus\n    cloud_bit_mask = 1 &lt;&lt; 10\n    cirrus_bit_mask = 1 &lt;&lt; 11\n\n    # Both flags should be zero = clear\n    mask = (qa.bitwiseAnd(cloud_bit_mask).eq(0)\n            .And(qa.bitwiseAnd(cirrus_bit_mask).eq(0)))\n\n    # Return masked image, scaled to reflectance (0-1)\n    return image.updateMask(mask).divide(10000)\n\n# Apply to collection\ns2_masked = bohol_images.map(mask_s2_clouds)\n\nprint(\"Cloud masking applied! ✓\")\n\n\n\n\n\n\nNoteUnderstanding Bitwise Operations\n\n\n\nQA60 band stores flags as bits:\nQA60 = 1024 (binary: 10000000000)\n         Bit 10 is set → Cloud present\n\nBit mask:\ncloud_bit_mask = 1 &lt;&lt; 10 = 1024\nqa.bitwiseAnd(cloud_bit_mask) extracts bit 10\n.eq(0) checks if bit is 0 (no cloud)\nThis efficient encoding allows multiple flags in one band!\n\n\n\n\nAdvanced Cloud Masking with SCL\nScene Classification Layer (SCL) band provides detailed classification:\ndef mask_s2_clouds_scl(image):\n    \"\"\"Advanced cloud masking using SCL band.\"\"\"\n    scl = image.select('SCL')\n\n    # SCL values:\n    # 3 = cloud shadows\n    # 8 = cloud medium probability\n    # 9 = cloud high probability\n    # 10 = thin cirrus\n    # 11 = snow/ice\n\n    # Keep only vegetation (4), bare soil (5), water (6)\n    mask = scl.eq(4).Or(scl.eq(5)).Or(scl.eq(6))\n\n    return image.updateMask(mask).divide(10000)\n\n# Note: Need to load SCL band\ns2_with_scl = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(bohol_aoi)\n    .filterDate('2024-03-01', '2024-05-31')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n    .select(['B2', 'B3', 'B4', 'B8', 'SCL']))\n\ns2_masked_scl = s2_with_scl.map(mask_s2_clouds_scl)",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "sessions/session4.html#part-7-creating-temporal-composites",
    "href": "sessions/session4.html#part-7-creating-temporal-composites",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 7: Creating Temporal Composites",
    "text": "Part 7: Creating Temporal Composites\n\nWhy Composites?\nChallenges in tropical regions (like Philippines):\n\nFrequent cloud cover (&gt;60% annual average)\nDifficult to find single cloud-free scene\nMonsoon seasons worsen problem\n\nSolution: Temporal compositing\nCombine multiple images over time to create cloud-free mosaic.\n\n\nComposite Methods\n\n1. Median Composite\nMost common - robust to outliers:\n# Create median composite (dry season 2024)\ncomposite_median = s2_masked.median()\n\nprint(\"Median composite created! ✓\")\n\n# Visualize\nMap = geemap.Map(center=[10.0, 124.0], zoom=9)\nvis_params = {\n    'bands': ['B4', 'B3', 'B2'],\n    'min': 0,\n    'max': 0.3,\n    'gamma': 1.4\n}\nMap.addLayer(composite_median, vis_params, 'Median Composite (Dry Season)')\nMap\nWhy median?\n\nMiddle value of sorted pixel values over time\nRemoves clouds (typically brightest values)\nRemoves shadows (typically darkest values)\nPreserves realistic surface reflectance\n\n\n\n2. Mean Composite\nAverage of all pixels:\ncomposite_mean = s2_masked.mean()\n\n# Smoother than median, but sensitive to remaining clouds\n\n\n3. Greenest Pixel Composite\nSelect pixel with highest NDVI (most vegetated):\ndef add_ndvi(image):\n    \"\"\"Add NDVI band to image.\"\"\"\n    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n    return image.addBands(ndvi)\n\n# Add NDVI to all images\ns2_with_ndvi = s2_masked.map(add_ndvi)\n\n# Get maximum NDVI composite (greenest pixel)\ncomposite_max_ndvi = s2_with_ndvi.qualityMosaic('NDVI')\n\nMap.addLayer(composite_max_ndvi, vis_params, 'Greenest Pixel Composite')\n\n\n\nMulti-temporal Analysis\nCompare dry vs. wet season:\n# Dry season composite (Jan-May)\ndry_season = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(bohol_aoi)\n    .filterDate('2024-01-01', '2024-05-31')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n    .map(mask_s2_clouds)\n    .median())\n\n# Wet season composite (Jun-Nov)\nwet_season = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(bohol_aoi)\n    .filterDate('2024-06-01', '2024-11-30')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n    .map(mask_s2_clouds)\n    .median())\n\n# Calculate NDVI for both\ndry_ndvi = dry_season.normalizedDifference(['B8', 'B4'])\nwet_ndvi = wet_season.normalizedDifference(['B8', 'B4'])\n\n# NDVI difference (wet - dry)\nndvi_change = wet_ndvi.subtract(dry_ndvi)\n\n# Visualize\nMap = geemap.Map(center=[10.0, 124.0], zoom=9)\nMap.addLayer(dry_season, vis_params, 'Dry Season')\nMap.addLayer(wet_season, vis_params, 'Wet Season')\nMap.addLayer(ndvi_change, {'min': -0.3, 'max': 0.3, 'palette': ['red', 'white', 'green']},\n             'NDVI Change (Wet-Dry)')\nMap\n\nInterpretation for Philippines:\n\nGreen areas (positive change): Rice paddies planted during wet season, increased vegetation vigor\nRed areas (negative change): Areas with less vegetation in wet season (possibly fallow, harvested, or flooded)\nWhite areas (no change): Stable land cover (evergreen forest, urban)",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "sessions/session4.html#part-8-calculating-spectral-indices-at-scale",
    "href": "sessions/session4.html#part-8-calculating-spectral-indices-at-scale",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 8: Calculating Spectral Indices at Scale",
    "text": "Part 8: Calculating Spectral Indices at Scale\n\nNDVI (Vegetation Health)\ndef calculate_ndvi(image):\n    \"\"\"Calculate NDVI for an image.\"\"\"\n    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n    return image.addBands(ndvi)\n\n# Apply to collection\ns2_with_ndvi = s2_masked.map(calculate_ndvi)\n\n# Get NDVI composite\nndvi_composite = s2_with_ndvi.select('NDVI').median()\n\n# Visualize\nndvi_params = {\n    'min': 0,\n    'max': 1,\n    'palette': ['red', 'yellow', 'green']\n}\nMap = geemap.Map(center=[10.0, 124.0], zoom=9)\nMap.addLayer(ndvi_composite, ndvi_params, 'NDVI Composite')\nMap\n\n\nNDWI (Water Bodies)\ndef calculate_ndwi(image):\n    \"\"\"Calculate NDWI for water detection.\"\"\"\n    ndwi = image.normalizedDifference(['B3', 'B8']).rename('NDWI')\n    return image.addBands(ndwi)\n\ns2_with_ndwi = s2_masked.map(calculate_ndwi)\nndwi_composite = s2_with_ndwi.select('NDWI').median()\n\n# Extract water bodies (NDWI &gt; 0.3)\nwater_mask = ndwi_composite.gt(0.3)\n\nMap.addLayer(water_mask.selfMask(), {'palette': 'blue'}, 'Water Bodies')\n\n\nMultiple Indices\ndef add_indices(image):\n    \"\"\"Add multiple spectral indices.\"\"\"\n    # NDVI\n    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n\n    # NDWI\n    ndwi = image.normalizedDifference(['B3', 'B8']).rename('NDWI')\n\n    # NDBI (Built-up)\n    ndbi = image.normalizedDifference(['B11', 'B8']).rename('NDBI')\n\n    # EVI (Enhanced Vegetation Index)\n    evi = image.expression(\n        '2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))',\n        {\n            'NIR': image.select('B8'),\n            'RED': image.select('B4'),\n            'BLUE': image.select('B2')\n        }\n    ).rename('EVI')\n\n    return image.addBands([ndvi, ndwi, ndbi, evi])\n\n# Apply to collection\ns2_with_indices = s2_masked.map(add_indices)\n\n# Create composite with all indices\nmulti_index_composite = s2_with_indices.median()\n\nprint(\"Bands in composite:\", multi_index_composite.bandNames().getInfo())",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "sessions/session4.html#part-9-exporting-data-from-earth-engine",
    "href": "sessions/session4.html#part-9-exporting-data-from-earth-engine",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 9: Exporting Data from Earth Engine",
    "text": "Part 9: Exporting Data from Earth Engine\n\nExport to Google Drive\nWhy export?\n\nUse data outside Earth Engine\nTrain ML models locally or in Colab\nShare processed results\nCreate high-resolution figures\n\nExport image:\n# Define export region (use AOI)\nexport_region = bohol_aoi\n\n# Export composite to Drive\nexport_task = ee.batch.Export.image.toDrive(\n    image=composite_median.select(['B2', 'B3', 'B4', 'B8']),\n    description='Bohol_S2_Median_Dry2024',\n    folder='CopPhil_Training',\n    fileNamePrefix='bohol_s2_composite',\n    region=export_region,\n    scale=10,  # Resolution in meters\n    crs='EPSG:32651',  # UTM Zone 51N\n    maxPixels=1e9,\n    fileFormat='GeoTIFF'\n)\n\n# Start export task\nexport_task.start()\n\nprint(\"Export task started! ✓\")\nprint(\"Check status at: https://code.earthengine.google.com/tasks\")\nMonitor export:\nimport time\n\n# Check task status\ntask_id = export_task.id\nprint(f\"Task ID: {task_id}\")\n\n# Poll until complete (check every 30 seconds)\nwhile export_task.active():\n    print(f\"  Status: {export_task.status()['state']} ...\")\n    time.sleep(30)\n\nprint(f\"✓ Export complete! Status: {export_task.status()['state']}\")\n\n\nExport Options\n1. Export to Google Drive (easiest):\nee.batch.Export.image.toDrive()\n2. Export to Cloud Storage:\nee.batch.Export.image.toCloudStorage(\n    image=image,\n    bucket='your-gcs-bucket',\n    fileNamePrefix='path/to/file',\n    ...\n)\n3. Export to Asset (for reuse in GEE):\nee.batch.Export.image.toAsset(\n    image=image,\n    description='MyAsset',\n    assetId='users/your-username/your-asset-name',\n    ...\n)\n\n\nExport FeatureCollection (Vector)\nExport classification results or statistics:\n# Create sample points with NDVI values\nsample_points = ndvi_composite.sample(\n    region=bohol_aoi,\n    scale=100,\n    numPixels=1000,\n    geometries=True\n)\n\n# Export to Drive\nexport_vector = ee.batch.Export.table.toDrive(\n    collection=sample_points,\n    description='Bohol_NDVI_Samples',\n    folder='CopPhil_Training',\n    fileFormat='CSV'\n)\n\nexport_vector.start()\nprint(\"Vector export started! ✓\")",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "sessions/session4.html#part-10-best-practices-and-limitations",
    "href": "sessions/session4.html#part-10-best-practices-and-limitations",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 10: Best Practices and Limitations",
    "text": "Part 10: Best Practices and Limitations\n\nBest Practices\n\n\n\n\n\n\nTipGEE Workflow Tips\n\n\n\n\nFilter aggressively: Reduce collection size before processing\nUse Cloud masking: Always mask clouds for optical data\nScale matters: Choose appropriate scale for export (don’t over-sample)\nTest on small areas: Prototype with small AOI before scaling up\nMonitor quotas: Be aware of computation and storage limits\nReproducibility: Save scripts, document parameters\nLeverage built-in functions: Don’t reinvent the wheel\n\n\n\n\n\nComputational Limits\nEarth Engine has quotas (free tier):\n\nSimultaneous requests: Limited concurrent computations\nExport size: Max 100,000 pixels per dimension\nAsset storage: Limited space for uploaded/exported assets\nProcessing time: Long-running tasks may timeout\n\nSolutions:\n\nBreak large exports into tiles\nUse reduce() operations instead of getInfo() for large data\nExport to Asset for intermediate results\nConsider upgrading to commercial tier for production workflows\n\n\n\nLimitations for AI/ML\nWhat GEE does well:\n\nData access and pre-processing\nLarge-scale feature extraction\nRandom Forest / CART classification\nPixel-based analysis\n\nWhat GEE struggles with:\n\nTraining deep learning models (CNNs, U-Net, LSTMs)\nCustom loss functions and optimizers\nGPU-accelerated training\nComplex model architectures requiring TensorFlow/PyTorch\n\n\n\n\n\n\n\nNoteRecommended Workflow for Deep Learning\n\n\n\n\nUse GEE for: Data discovery, filtering, cloud masking, compositing, exporting training patches\nUse Python (Colab/local) for: Training CNNs/U-Nets with TensorFlow or PyTorch\nReturn to GEE for: Applying trained model at scale (if feasible) or export tiles for prediction",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "sessions/session4.html#part-11-complete-example-philippine-land-cover-composite",
    "href": "sessions/session4.html#part-11-complete-example-philippine-land-cover-composite",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 11: Complete Example: Philippine Land Cover Composite",
    "text": "Part 11: Complete Example: Philippine Land Cover Composite\nScenario: Create cloud-free RGB and NDVI composites for entire Palawan province.\n# 1. Define AOI (Palawan province)\npalawan = ee.Geometry.Rectangle([117.5, 8.0, 119.5, 12.0])\n\n# 2. Load and filter Sentinel-2\ns2_palawan = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(palawan)\n    .filterDate('2024-01-01', '2024-05-31')  # Dry season\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n    .select(['B2', 'B3', 'B4', 'B8', 'QA60']))\n\nprint(f\"Images in collection: {s2_palawan.size().getInfo()}\")\n\n# 3. Apply cloud masking\ndef mask_clouds(image):\n    qa = image.select('QA60')\n    cloud_mask = qa.bitwiseAnd(1 &lt;&lt; 10).eq(0).And(qa.bitwiseAnd(1 &lt;&lt; 11).eq(0))\n    return image.updateMask(cloud_mask).divide(10000)\n\ns2_masked = s2_palawan.map(mask_clouds)\n\n# 4. Create median composite\ncomposite = s2_masked.median()\n\n# 5. Calculate NDVI\nndvi = composite.normalizedDifference(['B8', 'B4']).rename('NDVI')\ncomposite_with_ndvi = composite.addBands(ndvi)\n\n# 6. Visualize\nMap = geemap.Map(center=[10.0, 118.5], zoom=8)\n\nrgb_vis = {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3, 'gamma': 1.4}\nndvi_vis = {'bands': ['NDVI'], 'min': 0, 'max': 1, 'palette': ['brown', 'yellow', 'green', 'darkgreen']}\n\nMap.addLayer(composite, rgb_vis, 'Palawan True Color')\nMap.addLayer(composite_with_ndvi, ndvi_vis, 'Palawan NDVI')\nMap\n\n# 7. Export to Drive\nexport_rgb = ee.batch.Export.image.toDrive(\n    image=composite.select(['B4', 'B3', 'B2']),\n    description='Palawan_RGB_DryS2024',\n    folder='CopPhil_Training',\n    region=palawan,\n    scale=10,\n    maxPixels=1e10,\n    fileFormat='GeoTIFF'\n)\n\nexport_ndvi = ee.batch.Export.image.toDrive(\n    image=ndvi,\n    description='Palawan_NDVI_DryS2024',\n    folder='CopPhil_Training',\n    region=palawan,\n    scale=10,\n    maxPixels=1e10,\n    fileFormat='GeoTIFF'\n)\n\nexport_rgb.start()\nexport_ndvi.start()\n\nprint(\"=\" * 60)\nprint(\"PALAWAN LAND COVER COMPOSITE - EXPORT STARTED\")\nprint(\"=\" * 60)\nprint(\"RGB Composite: Check Google Drive in ~10-30 minutes\")\nprint(\"NDVI Layer: Check Google Drive in ~10-30 minutes\")\nprint(\"Monitor at: https://code.earthengine.google.com/tasks\")\nprint(\"=\" * 60)",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "sessions/session4.html#key-takeaways",
    "href": "sessions/session4.html#key-takeaways",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\n\n\n\n\nImportantSession 4 Summary\n\n\n\nGoogle Earth Engine: - Cloud platform with petabytes of EO data - No downloading required - process in cloud - Free for research/education\nCore Concepts: - Image / ImageCollection for rasters - Feature / FeatureCollection for vectors - Geometry for locations and AOIs\nKey Operations: - Filter by bounds, date, metadata - Cloud masking using QA bands - Temporal composites (median, mean) - Spectral indices (NDVI, NDWI) - Export to Drive/Cloud Storage\nWorkflow: 1. Define AOI 2. Filter collection 3. Mask clouds 4. Create composite 5. Calculate indices 6. Visualize 7. Export\nBest for: Pre-processing, data access, Random Forest, large-area mapping\nUse Python/TensorFlow for: Deep learning model training\nNext steps: Apply these skills in Days 2-4 for land cover classification, flood mapping, and time series analysis!",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "sessions/session4.html#practice-exercises",
    "href": "sessions/session4.html#practice-exercises",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Practice Exercises",
    "text": "Practice Exercises\n\n\n\n\n\n\nTipTry These Challenges\n\n\n\nExercise 1: Your Province Composite\nCreate a cloud-free Sentinel-2 composite for your home province using the complete workflow above.\nExercise 2: Multi-temporal NDVI Analysis\nCalculate NDVI composites for each month of 2024 over an agricultural area. Create a time series chart.\nExercise 3: Water Body Extraction\nUse NDWI to extract all water bodies in a coastal province. Export as vector (polygons).\nExercise 4: SAR Flood Detection\nCompare Sentinel-1 VV polarization before and after a typhoon to detect flooded areas.\nExercise 5: Export Training Data\nCreate stratified random samples of different land cover classes and export as CSV for ML training.\nBonus: Integrate with Session 3\nExport a GEE composite, then load it in Python (Session 3 techniques) to perform additional analysis with Rasterio.",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "sessions/session4.html#further-reading",
    "href": "sessions/session4.html#further-reading",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Further Reading",
    "text": "Further Reading\n\nOfficial Documentation\n\nEarth Engine Guides\nPython API Introduction\nSentinel-2 in GEE\nSentinel-1 Algorithms\n\n\n\nTutorials\n\nEnd-to-End GEE Course\nGBIF GEE Python Primer\nGEE Community Tutorials\n\n\n\nTools\n\ngeemap Library - Interactive mapping in Python\neemont - Extended functionality for GEE Python\nAwesome Earth Engine - Curated resources",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "sessions/session4.html#jupyter-notebook",
    "href": "sessions/session4.html#jupyter-notebook",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Jupyter Notebook",
    "text": "Jupyter Notebook\n\n\n\n\n\n\nNoteAccess the Interactive Notebook\n\n\n\nA complete Jupyter notebook with all Google Earth Engine examples from this session is available:\nOpen Notebook 2: Google Earth Engine →\nThis notebook includes: - All code examples ready to run - Philippine case studies - Additional exercises - Export workflows - Troubleshooting guide\n\n\n\n\n\n\n← Previous\n\n\nSession 3: Python for Geospatial Data\n\n\n\n\nDay 1 Complete!\n\n\nBack to Home →",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "sessions/session2.html",
    "href": "sessions/session2.html",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "",
    "text": "Duration: 2 hours | Format: Lecture + Conceptual Exercises | Platform: Presentation",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "sessions/session2.html#session-overview",
    "href": "sessions/session2.html#session-overview",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Session Overview",
    "text": "Session Overview\nThis session provides a comprehensive introduction to Artificial Intelligence and Machine Learning concepts specifically tailored for Earth Observation applications. You’ll learn the complete AI/ML workflow, understand different learning paradigms, explore neural network fundamentals, and discover why data quality matters more than model complexity in 2025’s data-centric AI paradigm.\n\nLearning Objectives\nBy the end of this session, you will be able to:\n\nDefine AI and ML in the context of Earth Observation\nDescribe the complete AI/ML workflow from problem definition to deployment\nDistinguish between supervised and unsupervised learning with EO examples\nExplain classification vs. regression tasks in satellite data analysis\nUnderstand neural network architecture fundamentals\nIdentify key components: neurons, layers, activation functions, loss functions, optimizers\nArticulate the data-centric AI paradigm and its importance for EO\nApply best practices for data quality, quantity, diversity, and annotation",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "sessions/session2.html#part-1-what-is-aiml",
    "href": "sessions/session2.html#part-1-what-is-aiml",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Part 1: What is AI/ML?",
    "text": "Part 1: What is AI/ML?\n\nDefining the Terms\nArtificial Intelligence (AI):\n\nBroad field focused on creating intelligent machines\nSystems that can perceive, reason, learn, and act\nIncludes everything from rule-based systems to machine learning\n\nMachine Learning (ML):\n\nSubset of AI focused on learning from data\nAlgorithms that improve performance through experience\nKey distinction: No explicit programming of rules\n\n\n\n\n\n\n\nNoteThe ML Difference\n\n\n\nTraditional Programming:\nRules + Data → Output\nMachine Learning:\nData + Desired Output → Rules (Model)\nIn EO: Instead of coding “if NIR &gt; 0.6 and Red &lt; 0.3, then forest”, ML learns the pattern from labeled examples.\n\n\n\n\nWhy ML for Earth Observation?\nChallenges that ML addresses:\n\nScale: Petabytes of satellite data - impossible to manually analyze\nComplexity: Multispectral, temporal, spatial patterns humans can’t easily detect\nConsistency: Automated processing ensures reproducible results\nSpeed: Real-time disaster mapping requires immediate analysis\n\nTraditional vs. ML approaches:\n\n\n\n\n\n\n\n\nTask\nTraditional\nML Approach\n\n\n\n\nWater detection\nManual NDWI threshold\nLearn optimal threshold + texture from examples\n\n\nLand cover\nRule-based classification\nRandom Forest or CNN with training samples\n\n\nFlood mapping\nExpert visual interpretation\nU-Net segmentation trained on labeled floods\n\n\nCrop monitoring\nFixed vegetation index thresholds\nLSTM time series model learning phenology",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "sessions/session2.html#part-2-the-aiml-workflow-for-earth-observation",
    "href": "sessions/session2.html#part-2-the-aiml-workflow-for-earth-observation",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Part 2: The AI/ML Workflow for Earth Observation",
    "text": "Part 2: The AI/ML Workflow for Earth Observation\nUnderstanding the complete workflow is essential for successful EO projects. Each step matters.\n\nStep 1: Problem Definition\nDefine clearly what you want to achieve:\n\nWhat question are you answering? (e.g., “Where are mangroves declining?”)\nWhat output do you need? (map, time series, alert system?)\nWhat accuracy is acceptable?\nWhat constraints exist? (time, computational resources, data availability)\n\n\nPhilippine Example:\nProblem: Map rice paddies in Central Luzon to estimate harvest timing for food security\nClear definition: - Binary classification: rice vs. non-rice - 20m spatial resolution acceptable (Sentinel-2 bands) - Temporal: wet and dry season separately - Accuracy target: &gt;85% for operational use\n\n\n\nStep 2: Data Acquisition\nGather all necessary data:\n\nSatellite imagery: Sentinel-1/2, Landsat, commercial VHR\nGround truth: Field surveys, high-res imagery interpretation, existing maps\nAncillary data: DEM, climate, administrative boundaries\n\nData sources for Philippines:\n\nCopernicus Data Space Ecosystem (Sentinel-1/2)\nPhilSA SIYASAT (NovaSAR-1)\nNAMRIA Geoportal (land cover basemaps)\nPAGASA (climate data)\n\n\n\nStep 3: Data Pre-processing\nCritical step - “Garbage in, garbage out”\nFor satellite imagery:\n\nAtmospheric correction: Convert to surface reflectance (use Level-2A)\nCloud masking: Remove or mask cloudy pixels\nGeometric correction: Ensure proper alignment\nRadiometric calibration: Consistent values across scenes\nTemporal compositing: Reduce clouds via median/mean composites\n\nFor training labels:\n\nQuality control: Verify label accuracy\nCoordinate alignment: Ensure labels match imagery timing and location\nClass balancing: Ensure adequate samples per class\nFormat standardization: Convert to ML-ready format\n\n\n\n\n\n\n\nWarningPre-processing Pitfalls\n\n\n\nCommon errors that degrade model performance:\n\nUsing Top-of-Atmosphere instead of surface reflectance\nTemporal mismatch: 2020 imagery with 2018 labels\nIncomplete cloud masking leaving cloud shadows\nMixed pixels at boundaries (especially for validation)\nInconsistent band ordering across scenes\n\n\n\n\n\nStep 4: Feature Engineering\nDeriving informative variables from raw data\nFor traditional ML (Random Forest, SVM):\n\nSpectral indices: NDVI, NDWI, NDBI, EVI, SAVI\nTextural features: GLCM metrics (contrast, entropy)\nTemporal features: Mean, std dev, phenology metrics\nTopographic features: Elevation, slope, aspect (from DEM)\nContextual features: Distance to roads, water bodies\n\nExample: Forest classification features\n# Spectral indices\nNDVI = (NIR - Red) / (NIR + Red)\nNDWI = (Green - NIR) / (Green + NIR)\n\n# Texture (from GLCM)\nContrast = ...  # measure of local variation\nHomogeneity = ...  # measure of uniformity\n\n# Topographic\nElevation, Slope\n\n# Result: Input feature vector per pixel\nX = [Red, Green, Blue, NIR, SWIR1, SWIR2, NDVI, NDWI, Contrast, Elevation, Slope]\nFor deep learning (CNNs):\n\nLess manual feature engineering needed\nNetworks automatically learn features from raw pixels\nStill benefit from good input data (cloud-free, calibrated)\n\n\n\nStep 5: Model Selection and Training\nChoose appropriate algorithm:\nConsider:\n\nTask type (classification, regression, segmentation)\nData size (deep learning needs more data)\nInterpretability requirements\nComputational resources\nDeployment constraints\n\nCommon EO algorithms:\n\n\n\n\n\n\n\n\n\nAlgorithm\nType\nBest For\nData Needs\n\n\n\n\nRandom Forest\nEnsemble\nClassification, feature importance\nMedium\n\n\nSVM\nKernel\nBinary classification, small data\nSmall-Medium\n\n\nCNN\nDeep Learning\nImage classification, automatic features\nLarge\n\n\nU-Net\nDeep Learning\nSemantic segmentation (pixel-wise)\nLarge\n\n\nLSTM\nDeep Learning\nTime series prediction\nLarge\n\n\n\nTraining process:\n\nSplit data: training (70%), validation (15%), testing (15%)\nFeed training data to algorithm\nAlgorithm adjusts parameters to minimize error\nMonitor performance on validation set\nIterate: adjust hyperparameters if needed\n\n\n\nStep 6: Validation and Evaluation\nRigorous testing on independent data\n\n\n\n\n\n\nImportantNever Test on Training Data!\n\n\n\nTesting on data the model has seen gives falsely optimistic results. Always use held-out test data.\n\n\nClassification metrics:\n\nOverall Accuracy: Percentage of correctly classified pixels\nConfusion Matrix: Shows which classes are confused\nProducer’s Accuracy: How many ground truth samples were correctly classified\nUser’s Accuracy: How many predicted samples are actually correct\nKappa Coefficient: Agreement accounting for chance\nF1-Score: Harmonic mean of precision and recall\n\nRegression metrics:\n\nRMSE (Root Mean Squared Error): Average prediction error\nMAE (Mean Absolute Error): Average absolute deviation\nR² (Coefficient of Determination): Proportion of variance explained\n\nPhilippine Example: Flood mapping evaluation\nConfusion Matrix:\n                Predicted\n              | Flood | No Flood |\nActual Flood  |  450  |   50     |  Producer's Acc: 90%\nActual No Flood|  30   |  1470    |  Producer's Acc: 98%\n\nUser's Accuracy: 93.8%   96.7%\nOverall Accuracy: 96%\n\n\nStep 7: Deployment and Operationalization\nMaking the model operational:\nDeployment strategies:\n\nBatch processing: Apply model to large archives\nNear real-time: Process new satellite acquisitions automatically\nOn-demand: User-triggered analysis\nEdge processing: On-board satellite AI (ESA Φsat-2)\n\nOperational considerations:\n\nScalability: Can it handle regional/national scale?\nAutomation: Minimize manual intervention\nMonitoring: Track performance over time\nRetraining: Update model as conditions change\nIntegration: Connect to decision support systems\n\nPhilippine context:\n\nDOST-ASTI AIPI platform for model deployment\nDIMER repository for model sharing\nIntegration with LGU disaster response protocols\nDelivery via PhilSA Digital Space Campus",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "sessions/session2.html#part-3-types-of-machine-learning",
    "href": "sessions/session2.html#part-3-types-of-machine-learning",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Part 3: Types of Machine Learning",
    "text": "Part 3: Types of Machine Learning\n\nSupervised Learning\nLearning from labeled data\nThe algorithm is given: - Input: Satellite image or features - Output: Known label (class or value) - Goal: Learn mapping from input to output\n\nClassification Tasks\nPredicting categorical labels\nEO Examples:\n\nLand Cover Classification\n\nInput: Sentinel-2 pixel values\nOutput: Forest, Water, Urban, Agriculture, Bare soil\nAlgorithm: Random Forest, CNN\n\nCloud Detection\n\nInput: Multi-band imagery\nOutput: Cloud vs. Clear\nAlgorithm: Threshold or ML classifier\n\nCrop Type Mapping\n\nInput: Multi-temporal NDVI\nOutput: Rice, Corn, Sugarcane, Coconut\nAlgorithm: Random Forest or LSTM\n\n\n\nPhilippine Case Study: Mangrove Mapping\nTask: Classify pixels as mangrove or non-mangrove in coastal areas\nData: - Sentinel-2 multi-temporal imagery (dry and wet season) - Field-validated mangrove polygons - NAMRIA coastal land cover baseline\nApproach: - Extract spectral values and indices (NDVI, NDWI) - Train Random Forest classifier - Validate against independent field data - Deploy via DOST-ASTI AIPI\nResult: 92% accuracy mangrove extent map for Palawan coastline\n\n\n\nRegression Tasks\nPredicting continuous values\nEO Examples:\n\nBiomass Estimation\n\nInput: Sentinel-1 SAR backscatter, Sentinel-2 vegetation indices\nOutput: Forest biomass (tons per hectare)\nAlgorithm: Random Forest Regression\n\nSoil Moisture Prediction\n\nInput: Sentinel-1 VV/VH polarization, temperature\nOutput: Volumetric soil moisture (%)\nAlgorithm: Neural network regression\n\nCrop Yield Forecasting\n\nInput: NDVI time series, rainfall, temperature\nOutput: Expected yield (tons per hectare)\nAlgorithm: LSTM regression\n\n\nKey difference from classification: - Output is a number on a continuous scale - Loss functions measure distance from true value (MSE, RMSE) - Evaluation uses regression metrics (R², RMSE)\n\n\n\nUnsupervised Learning\nFinding patterns in unlabeled data\nThe algorithm receives: - Input: Satellite imagery or features - No labels provided - Goal: Discover inherent structure or groupings\n\nClustering\nGrouping similar pixels/regions together\nCommon algorithm: k-means\n\nSpecify number of clusters (k)\nAlgorithm iteratively groups pixels with similar spectral characteristics\nResult: Image segmented into k clusters\nHuman interpretation needed: “Cluster 1 looks like water, Cluster 2 like forest…”\n\nEO Applications:\n\nExploratory analysis: “How many distinct spectral classes in this region?”\nChange detection: Cluster before/after images to find anomalies\nImage segmentation: Group similar pixels for object-based analysis\n\n\n\n\n\n\n\nTipWhen to Use Unsupervised Learning\n\n\n\nAdvantages: - No need for expensive labeled data - Can discover unexpected patterns - Good for initial data exploration\nLimitations: - Results need interpretation - No guarantee clusters match desired classes - Often less accurate than supervised methods for specific tasks - Difficult to evaluate objectively\n\n\nComparison Example:\nSupervised (Land Cover Classification): - Provide 1000 labeled samples: forest, water, urban - Train Random Forest - Result: Every pixel assigned forest/water/urban - Evaluation: 90% accuracy against test labels\nUnsupervised (k-means Clustering): - No labels provided - Run k-means with k=3 - Result: Three clusters emerge - Interpretation: Cluster A=water, B=vegetation, C=mixed urban/bare - Evaluation: Subjective or requires labels anyway",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "sessions/session2.html#part-4-introduction-to-deep-learning",
    "href": "sessions/session2.html#part-4-introduction-to-deep-learning",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Part 4: Introduction to Deep Learning",
    "text": "Part 4: Introduction to Deep Learning\n\nWhat is Deep Learning?\nDeep Learning = Neural Networks with Many Layers\n\nSubset of machine learning\nInspired by biological neurons\nMultiple processing layers extract progressively abstract features\nDominant approach for image analysis since ~2012\n\nWhy “deep”? - Refers to depth: many hidden layers - Modern networks: 10s to 100s of layers - Enables learning complex, hierarchical representations\n\n\nNeural Network Fundamentals\n\nThe Artificial Neuron\nBuilding block of neural networks:\nInputs (x1, x2, x3) → [Weighted Sum + Bias] → Activation Function → Output\nMathematical operation:\n\nWeighted sum: z = w1*x1 + w2*x2 + w3*x3 + b\nActivation function: output = activation(z)\n\nExample: Detecting bright pixels\nInputs: [Red=0.8, Green=0.7, NIR=0.9]\nWeights: [w1=1.0, w2=1.0, w3=1.0]\nBias: b = -2.0\n\nz = 1.0*0.8 + 1.0*0.7 + 1.0*0.9 - 2.0 = 0.4\noutput = ReLU(0.4) = 0.4  (indicates moderately bright)\n\n\nNetwork Architecture\nLayers of neurons:\n\nInput Layer: Receives raw data (e.g., pixel values)\nHidden Layers: Process and transform data\nOutput Layer: Produces final prediction\n\nFor a simple image classification:\nInput Layer (256 neurons = 16x16 image)\n   ↓\nHidden Layer 1 (128 neurons with ReLU)\n   ↓\nHidden Layer 2 (64 neurons with ReLU)\n   ↓\nOutput Layer (5 neurons = 5 classes, softmax activation)\nEach connection has a weight - the network learns optimal weights through training.\n\n\nActivation Functions\nIntroduce non-linearity - crucial for learning complex patterns\nCommon activation functions:\n\n\n\nFunction\nEquation\nUse Case\n\n\n\n\nReLU\nmax(0, x)\nHidden layers (most common)\n\n\nSigmoid\n1 / (1 + e^-x)\nBinary classification output\n\n\nSoftmax\ne^xi / Σe^xj\nMulti-class classification output\n\n\nTanh\n(e^x - e^-x) / (e^x + e^-x)\nHidden layers (older)\n\n\n\nWhy activation functions matter:\nWithout non-linearity, multiple layers would collapse to a single linear transformation - no benefit from depth!\n\n\n\n\n\n\nNoteReLU: The Default Choice\n\n\n\nReLU (Rectified Linear Unit) has become standard for hidden layers because:\n\nSimple: f(x) = max(0, x)\nComputationally efficient\nAvoids vanishing gradient problem\nEmpirically performs very well\n\n\n\n\n\nLoss Functions\nMeasure how wrong the model’s predictions are\nThe model’s objective: minimize the loss function\nFor classification:\nCategorical Cross-Entropy:\nLoss = -Σ(y_true * log(y_pred))\n\nPenalizes confident wrong predictions heavily\nEncourages high probability for correct class\n\nExample:\nTrue class: Forest (encoded as [1, 0, 0, 0, 0])\nPrediction: [0.7, 0.1, 0.1, 0.05, 0.05]  ← Good, 70% on forest\nLoss = -1*log(0.7) = 0.36\n\nPrediction: [0.2, 0.3, 0.4, 0.05, 0.05]  ← Bad, only 20% on forest\nLoss = -1*log(0.2) = 1.61  (much higher penalty)\nFor regression:\nMean Squared Error (MSE):\nLoss = (1/n) * Σ(y_true - y_pred)²\nExample: Biomass prediction:\nTrue: 150 tons/ha\nPrediction: 140 tons/ha\nError: 10 tons/ha\nSquared Error: 100\n\n\nOptimizers\nAlgorithms that adjust weights to minimize loss\nThe process:\n\nCalculate loss on current batch of data\nCompute gradients (via backpropagation): how should each weight change?\nUpdate weights in direction that reduces loss\nRepeat thousands/millions of times\n\nCommon optimizers:\n\n\n\n\n\n\n\n\nOptimizer\nDescription\nWhen to Use\n\n\n\n\nSGD\nStochastic Gradient Descent\nSimple, well-understood\n\n\nAdam\nAdaptive learning rate\nDefault choice, usually works well\n\n\nRMSprop\nRoot Mean Square Propagation\nGood for RNNs\n\n\nAdaGrad\nAdaptive Gradient\nWhen features vary in frequency\n\n\n\nAdam is most popular because: - Adapts learning rate per parameter - Combines benefits of momentum and adaptive learning - Requires minimal tuning - Works well across diverse problems\n\n\n\n\n\n\nTipTraining Terminology\n\n\n\nEpoch: One complete pass through the entire training dataset\nBatch: Subset of training data processed together before updating weights\nIteration: One weight update (one batch processed)\nExample: - Training data: 10,000 samples - Batch size: 100 - 1 epoch = 100 iterations (10,000 / 100) - Training for 50 epochs = 5,000 iterations\n\n\n\n\nThe Training Process\nIterative improvement:\n1. Initialize weights randomly\n2. For each epoch:\n    For each batch:\n        a. Forward pass: Compute predictions\n        b. Calculate loss\n        c. Backward pass: Compute gradients (backpropagation)\n        d. Update weights using optimizer\n    e. Evaluate on validation set\n3. Stop when validation performance plateaus\nMonitoring training:\n\nTraining loss should decrease - model learning patterns\nValidation loss should decrease - model generalizing\nIf validation loss increases while training loss decreases: Overfitting!\n\n\n\n\nDeep Learning for Earth Observation\nWhy CNNs excel at EO:\nTraditional ML: - Manual feature engineering needed - Limited ability to capture spatial patterns - Each pixel treated somewhat independently\nCNNs: - Automatic feature extraction from raw pixels - Spatial awareness through convolutional filters - Hierarchical learning: edges → textures → objects → scenes - Translation invariance: Detects patterns anywhere in image\nCommon EO architectures:\n\nCNNs: Image classification, object detection\nU-Net: Semantic segmentation (flood mapping, building extraction)\nResNet: Very deep networks for complex classification\nLSTMs: Time series analysis (crop monitoring, drought prediction)\n\nWe’ll explore these in depth on Days 2-4!",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "sessions/session2.html#part-5-data-centric-ai-in-earth-observation",
    "href": "sessions/session2.html#part-5-data-centric-ai-in-earth-observation",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Part 5: Data-Centric AI in Earth Observation",
    "text": "Part 5: Data-Centric AI in Earth Observation\n\nThe Paradigm Shift (2025)\n\n\n\n\n\n\nImportantData &gt; Models\n\n\n\nOld paradigm (Model-Centric AI): - Focus on developing better algorithms - Keep data fixed, iterate on model architecture - “Our new model achieves 92% accuracy!”\nNew paradigm (Data-Centric AI): - Focus on improving data quality and curation - Keep model fixed (use proven architectures), iterate on data - “Better data improved our model from 85% to 95% accuracy!”\n\n\nWhy the shift?\n\nModel architectures have matured: ResNet, U-Net, LSTM are well-established\nBiggest gains now come from data: Most underperforming models suffer from data issues\nReal-world deployment: Data quality determines operational success\n\n\n\nPillar 1: Data Quality\nHigh-quality data is accurate, consistent, and properly processed\nFor satellite imagery:\nQuality issues to address:\n\nCloud contamination: Use Level-2A with SCL cloud masks\nAtmospheric effects: Always use atmospherically corrected data\nSensor artifacts: Check for striping, banding, saturation\nGeometric accuracy: Ensure sub-pixel registration\nRadiometric consistency: Calibrate across sensors and times\n\n\nPhilippine Challenge: Cloud Cover\nPhilippines has one of highest cloud cover frequencies globally (&gt;60% during monsoon).\nData quality solutions: - Multi-temporal compositing (median over 3 months) - Combine optical (Sentinel-2) + SAR (Sentinel-1) which penetrates clouds - Use aggressive cloud masking (accept fewer images for higher quality) - Leverage dry season (Dec-May) for optical data\n\nFor training labels:\nQuality issues:\n\nPositional error: GPS drift, georeferencing mismatch\nTemporal mismatch: 2018 labels with 2020 imagery\nClass ambiguity: Unclear definitions (shrub vs. sparse forest?)\nMixed pixels: Polygon boundaries include multiple classes\nLabeling inconsistency: Different interpreters, different criteria\n\nBest practices:\n\nClear class definitions: Document what each class includes/excludes\nConsistent methodology: Same interpreter, same time of year, same imagery\nQuality control: Multiple reviewers, consensus protocols\nTemporal alignment: Labels contemporary with imagery (within months)\nPositional accuracy: Use high-resolution reference imagery\n\n\n\nPillar 2: Data Quantity\nMore data (usually) improves performance\nBut quantity alone isn’t enough - quality matters more!\nHow much data do you need?\n\n\n\nAlgorithm\nTypical Requirements\n\n\n\n\nRandom Forest\n100s - 1000s samples per class\n\n\nSimple CNN\n1000s - 10,000s samples\n\n\nDeep CNN (ResNet)\n10,000s - 100,000s samples\n\n\nFoundation Models\nMillions - billions samples\n\n\n\nStrategies when labeled data is limited:\n\nData Augmentation\n\nRotation, flipping, cropping\nColor jittering (adjust brightness, contrast)\nAdding noise\nCaution: Ensure augmentations are realistic for EO\n\nTransfer Learning\n\nUse model pre-trained on large dataset (ImageNet, SatMAE)\nFine-tune on your small dataset\nLeverages learned features from similar tasks\n\nActive Learning\n\nIteratively: train model → find uncertain predictions → label those → retrain\nEfficiently focuses labeling effort where it matters most\n\nSynthetic Data\n\nGenerate training data via simulation\nExample: Simulated SAR scenes for flood detection\n\n\n\n\n\n\n\n\nNote2025 Research: Data Efficiency\n\n\n\nRecent studies show:\n\nSome EO tasks reach optimal accuracy with &lt;20% of temporal instances\nSingle band from single sensor can be sufficient for specific tasks\nImplication: Smart data selection &gt; brute force data collection\n\nSource: “Data-Centric Machine Learning for Earth Observation: Necessary and Sufficient Features” (arXiv 2024)\n\n\n\n\nPillar 3: Data Diversity\nRepresentative data covers the full range of scenarios the model will encounter\nDimensions of diversity:\n\nGeographic diversity\n\nDifferent regions (Luzon, Visayas, Mindanao)\nDifferent ecosystems (lowland, highland, coastal)\nDifferent climate zones\n\nTemporal diversity\n\nDifferent seasons (wet, dry)\nDifferent years (inter-annual variability)\nDifferent phenological stages (planting, growing, harvest)\n\nClass diversity\n\nMultiple examples per class\nEdge cases and rare types\nTransitional zones\n\nSensor diversity\n\nDifferent satellites (Sentinel-2A, 2B, 2C)\nDifferent atmospheric conditions\nDifferent viewing angles\n\n\nExample: Urban classification\nPoor diversity: All training samples from Metro Manila CBD\nResult: Model fails on: - Small provincial towns (different building density) - Informal settlements (different materials) - Peri-urban areas (mixed land cover)\nGood diversity: Samples from: - Large cities (Manila, Cebu, Davao) - Medium towns (Baguio, Iloilo, Cagayan de Oro) - Small municipalities - Different building materials (concrete, metal roofing, nipa huts) - Different periods (to capture growth)\nResult: Model generalizes well across Philippines\n\n\nPillar 4: Annotation Strategy\nHow you label data profoundly impacts model performance\nAnnotation approaches:\n\nPoint sampling: Fast, but limited context\nPolygon delineation: More information, more time-consuming\nPixel-level labeling: Maximum detail, required for segmentation\nImage-level labels: Easiest, suitable for scene classification\n\nBest practices:\n1. Expert involvement - Use domain experts for complex classes (forest types, crop stages) - Train labelers thoroughly on class definitions - Regular calibration sessions\n2. Quality over quantity - 500 high-quality labels &gt; 5000 noisy labels - Invest in review and correction - Document difficult cases\n3. Class balance - Ensure adequate representation of minority classes - Stratified sampling by class - Consider class weights in training if imbalanced\n4. Consensus protocols - Multiple labelers per sample - Majority vote or adjudication for disagreements - Measure inter-annotator agreement\n5. Iterative refinement - Use model predictions to find label errors - Retrain after improving labels - Focus effort on low-confidence predictions\n\nPhilippine Solution: ALaM Project\nDOST-ASTI’s Automated Labeling Machine (ALaM) addresses annotation bottleneck:\n\nCombines automated labeling with crowdsourcing\nHuman-in-the-loop quality control\nIntegration with DIMER model repository\nReduces labeling time and cost significantly\n\n\n\n\n2025 Examples: Data-Centric Success Stories\n\nNASA-IBM Geospatial Foundation Model\nOpen-source model trained on massive HLS dataset (Harmonized Landsat-Sentinel-2)\nData-centric approach: - Millions of satellite images - Self-supervised pre-training (no labels needed) - Fine-tuned for specific tasks with small labeled datasets\nResult: - State-of-the-art performance on multiple EO tasks - Reduces labeled data requirements by 10-100x - Democratizes access to powerful EO AI\n\n\nESA Φsat-2 On-Board AI\nLaunched 2025: 22cm CubeSat with on-board AI processing\nData-centric innovation: - Processes imagery directly on satellite - Only transmits actionable information (not raw data) - Reduces bandwidth requirements - Enables real-time event detection (fires, ships, clouds)\nImplication: Data quality selection happens in space!\n\n\nEarthDaily Constellation\n10-satellite constellation for daily global coverage\nFocus on AI-ready data: - Scientific-grade calibration - Consistent, reliable acquisitions - Optimized spectral bands for ML - Emphasis on data quality for algorithm performance",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "sessions/session2.html#key-takeaways",
    "href": "sessions/session2.html#key-takeaways",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\n\n\n\n\nImportantSession 2 Summary\n\n\n\n\nAI/ML learns patterns from data rather than explicit programming\nThe EO workflow spans problem definition → data → preprocessing → features → training → validation → deployment\nSupervised learning (classification & regression) is dominant for EO because we need specific outputs\nUnsupervised learning (clustering) is useful for exploration but requires interpretation\nNeural networks are composed of layers of neurons using activation functions, optimized via loss minimization\nDeep learning automatically extracts hierarchical features - dominant for image analysis\nData-centric AI (2025 paradigm): Improving data quality, quantity, diversity, and annotation beats tweaking models\nFor Philippine EO: Leverage DOST-ASTI tools (DIMER, AIPI, ALaM) to operationalize data-centric approaches\n\nNext steps: Hands-on Python for geospatial data (Session 3) and Google Earth Engine (Session 4) to put these concepts into practice!",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "sessions/session2.html#discussion-questions",
    "href": "sessions/session2.html#discussion-questions",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Discussion Questions",
    "text": "Discussion Questions\n\n\n\n\n\n\nTipReflect & Discuss\n\n\n\n\nWhat EO problem in your work could benefit from ML? Is it classification or regression?\nWhat data quality issues have you encountered with Philippine satellite data?\nHow would you ensure diversity in training data for a national-scale land cover map?\nWhich Philippine datasets (PhilSA, NAMRIA, PAGASA) could complement satellite imagery for your ML project?\nHow might DOST-ASTI’s DIMER and AIPI platforms reduce barriers to deploying ML in your organization?",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "sessions/session2.html#further-reading",
    "href": "sessions/session2.html#further-reading",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Further Reading",
    "text": "Further Reading\n\nFoundational Concepts\n\nNASA ARSET: Fundamentals of Machine Learning for Earth Science\nData-Centric AI: Better, Not Just More\n\n\n\nNeural Networks\n\nDeep Learning Book (Goodfellow et al.) - Free online\nNeural Networks and Deep Learning (Nielsen) - Interactive tutorial\n\n\n\nEO-Specific ML\n\nEO College: Introduction to Machine Learning for Earth Observation\nML4Earth Resources\nClimate Change AI: Earth Observation & Monitoring\n\n\n\nPhilippine AI Initiatives\n\nDOST-ASTI SkAI-Pinas\nDIMER Model Hub\n\n\n\n\n\n← Previous\n\n\nSession 1: Copernicus & Philippine EO\n\n\n\n\nNext Session\n\n\nSession 3: Python for Geospatial Data →",
    "crumbs": [
      "Home",
      "Training Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "notebooks/notebook1.html",
    "href": "notebooks/notebook1.html",
    "title": "Notebook 1: Python for Geospatial Data",
    "section": "",
    "text": "This notebook accompanies Session 3: Python for Geospatial Data. You’ll learn to work with vector data using GeoPandas and raster data using Rasterio.\n\n\nBy completing this notebook, you will:\n\nLoad and visualize vector data with GeoPandas\nWork with different vector formats (Shapefiles, GeoJSON, GeoPackage)\nPerform coordinate reference system transformations\nRead and process raster data with Rasterio\nExtract band information and metadata\nVisualize multi-band imagery\nApply the concepts to Philippine data"
  },
  {
    "objectID": "notebooks/notebook1.html#session-3-hands-on-notebook",
    "href": "notebooks/notebook1.html#session-3-hands-on-notebook",
    "title": "Notebook 1: Python for Geospatial Data",
    "section": "",
    "text": "This notebook accompanies Session 3: Python for Geospatial Data. You’ll learn to work with vector data using GeoPandas and raster data using Rasterio.\n\n\nBy completing this notebook, you will:\n\nLoad and visualize vector data with GeoPandas\nWork with different vector formats (Shapefiles, GeoJSON, GeoPackage)\nPerform coordinate reference system transformations\nRead and process raster data with Rasterio\nExtract band information and metadata\nVisualize multi-band imagery\nApply the concepts to Philippine data"
  },
  {
    "objectID": "notebooks/notebook1.html#getting-started",
    "href": "notebooks/notebook1.html#getting-started",
    "title": "Notebook 1: Python for Geospatial Data",
    "section": "Getting Started",
    "text": "Getting Started\n\nOption 1: Open in Google Colab (Recommended)\nClick the button below to open this notebook in Google Colab:\n\n\n\n\nOpen In Colab\n\n\n\nAdvantages: - No installation required - Free GPU access - Auto-saves to Google Drive - Pre-configured environment\n\n\nOption 2: Download Notebook\nDownload the Jupyter notebook to run locally or upload to your own Colab:\n\nDownload .ipynb File\nRequirements for local use:\npip install numpy pandas matplotlib geopandas rasterio\n\n\n\nOption 3: View Online\nYou can also view the notebook content below without running any code."
  },
  {
    "objectID": "notebooks/notebook1.html#notebook-preview",
    "href": "notebooks/notebook1.html#notebook-preview",
    "title": "Notebook 1: Python for Geospatial Data",
    "section": "Notebook Preview",
    "text": "Notebook Preview\n\n\n\n\n\n\nNoteInteractive Execution Required\n\n\n\nThis is a hands-on exercise notebook. For the best learning experience, open it in Google Colab or Jupyter to run the code cells interactively.\n\n\n\nTopics Covered\n\nIntroduction to GeoPandas\n\nReading vector data\nExploring GeoDataFrames\nCoordinate reference systems\nSpatial operations\n\nWorking with Philippine Data\n\nLoading administrative boundaries\nFiltering regions and provinces\nCalculating areas and centroids\nCreating maps\n\nIntroduction to Rasterio\n\nReading raster data\nUnderstanding raster metadata\nExtracting bands\nVisualizing imagery\n\nPalawan Case Study\n\nSentinel-2 imagery analysis\nLand cover visualization\nSpectral band combinations\nNDVI calculation"
  },
  {
    "objectID": "notebooks/notebook1.html#prerequisites",
    "href": "notebooks/notebook1.html#prerequisites",
    "title": "Notebook 1: Python for Geospatial Data",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore starting this notebook, ensure you have:\n\nCompleted the Setup Guide\nGoogle account (for Colab)\nBasic Python knowledge\nUnderstanding of Session 3 concepts"
  },
  {
    "objectID": "notebooks/notebook1.html#notebook-contents",
    "href": "notebooks/notebook1.html#notebook-contents",
    "title": "Notebook 1: Python for Geospatial Data",
    "section": "Notebook Contents",
    "text": "Notebook Contents\nThe full interactive notebook includes:\n\n15+ code cells with detailed explanations\n10+ visualizations of vector and raster data\nExercises to test your understanding\nPhilippine case studies using real data\nTroubleshooting tips for common issues"
  },
  {
    "objectID": "notebooks/notebook1.html#support",
    "href": "notebooks/notebook1.html#support",
    "title": "Notebook 1: Python for Geospatial Data",
    "section": "Support",
    "text": "Support\n\nDuring the Training\n\nAsk questions in the live session\nConsult teaching assistants\nWork through exercises at your own pace\n\n\n\nAfter the Training\n\nReview the Cheat Sheets\nCheck the FAQ\nAccess the Glossary"
  },
  {
    "objectID": "notebooks/notebook1.html#related-resources",
    "href": "notebooks/notebook1.html#related-resources",
    "title": "Notebook 1: Python for Geospatial Data",
    "section": "Related Resources",
    "text": "Related Resources\n\nSession Materials: - Session 3: Python for Geospatial Data - Session 3 Presentation Slides\nQuick References: - GeoPandas Cheat Sheet - Rasterio Cheat Sheet\nDocumentation: - GeoPandas Documentation - Rasterio Documentation"
  },
  {
    "objectID": "notebooks/notebook1.html#next-steps",
    "href": "notebooks/notebook1.html#next-steps",
    "title": "Notebook 1: Python for Geospatial Data",
    "section": "Next Steps",
    "text": "Next Steps\nAfter completing this notebook:\n\n✅ Practice with your own Philippine data\n✅ Move on to Session 4: Google Earth Engine\n✅ Try Notebook 2: Google Earth Engine\n\n\n\n\n\nBack\n\n\n← Session 3 Overview\n\n\n\n\nNext\n\n\nNotebook 2: Google Earth Engine →\n\n\n\n\nReady to code? Open the notebook in Colab and start learning!"
  },
  {
    "objectID": "notebooks/Day1_Session4_Google_Earth_Engine.html",
    "href": "notebooks/Day1_Session4_Google_Earth_Engine.html",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "",
    "text": "Open In Colab"
  },
  {
    "objectID": "notebooks/Day1_Session4_Google_Earth_Engine.html#copphil-4-day-advanced-online-training-on-aiml-for-earth-observation",
    "href": "notebooks/Day1_Session4_Google_Earth_Engine.html#copphil-4-day-advanced-online-training-on-aiml-for-earth-observation",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "",
    "text": "Open In Colab"
  },
  {
    "objectID": "notebooks/Day1_Session4_Google_Earth_Engine.html#learning-objectives",
    "href": "notebooks/Day1_Session4_Google_Earth_Engine.html#learning-objectives",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this session, you will be able to:\n\nAuthenticate and initialize Google Earth Engine (GEE) in Python\nUnderstand GEE core concepts (Image, ImageCollection, Feature, FeatureCollection)\nAccess and filter Sentinel-1 and Sentinel-2 data collections\nApply cloud masking and create temporal composites\nCalculate spectral indices (NDVI) at scale\nExport processed data for use in AI/ML workflows\nApply GEE best practices for computational efficiency"
  },
  {
    "objectID": "notebooks/Day1_Session4_Google_Earth_Engine.html#why-google-earth-engine",
    "href": "notebooks/Day1_Session4_Google_Earth_Engine.html#why-google-earth-engine",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "Why Google Earth Engine?",
    "text": "Why Google Earth Engine?\nGoogle Earth Engine provides:\n\nPetabyte-scale data catalog: Sentinel-1, Sentinel-2, Landsat, MODIS, and more\nCloud computing: Process data without downloading\nPlanetary-scale analysis: Analyze entire countries or continents\nFree access: For research and education\n\nPerfect for preparing training data for AI/ML models!"
  },
  {
    "objectID": "notebooks/Day1_Session4_Google_Earth_Engine.html#prerequisites",
    "href": "notebooks/Day1_Session4_Google_Earth_Engine.html#prerequisites",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nCompletion of Session 3 (Python geospatial basics)\nGoogle Earth Engine account (sign up at https://earthengine.google.com/)\nBasic understanding of Sentinel-1 and Sentinel-2 missions"
  },
  {
    "objectID": "notebooks/Day1_Session4_Google_Earth_Engine.html#setup-and-authentication",
    "href": "notebooks/Day1_Session4_Google_Earth_Engine.html#setup-and-authentication",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "1. Setup and Authentication",
    "text": "1. Setup and Authentication\n\n1.1 Install Earth Engine API\n\n# Install Earth Engine Python API\n!pip install earthengine-api -q\n\nprint(\"Earth Engine API installed successfully!\")\n\n\n\n1.2 Authentication\nImportant: You need to authenticate once per environment. Follow the instructions that appear:\n\nClick the authentication link\nSelect your Google account\nGrant permissions\nCopy the authorization code\nPaste it back into the notebook\n\n\nimport ee\n\n# Authenticate (only needed once per environment)\ntry:\n    ee.Authenticate()\n    print(\"Authentication successful!\")\nexcept Exception as e:\n    print(f\"Authentication note: {e}\")\n    print(\"If already authenticated, you can proceed to initialization.\")\n\n\n\n1.3 Initialize Earth Engine\n\n# Initialize Earth Engine\nee.Initialize()\n\nprint(\"Earth Engine initialized successfully!\")\nprint(f\"Earth Engine Python API version: {ee.__version__}\")\n\n\n\n1.4 Import Additional Libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display\nimport datetime\n\n# Set visualization defaults\nplt.rcParams['figure.figsize'] = (12, 8)\n\nprint(\"All libraries imported successfully!\")\n\n\n\nTroubleshooting Authentication\nIf you encounter issues:\n\nNot signed up? Register at https://earthengine.google.com/\nPermission errors? Ensure you’re using the correct Google account\nAlready authenticated? Skip authentication and go directly to ee.Initialize()"
  },
  {
    "objectID": "notebooks/Day1_Session4_Google_Earth_Engine.html#gee-core-concepts",
    "href": "notebooks/Day1_Session4_Google_Earth_Engine.html#gee-core-concepts",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "2. GEE Core Concepts",
    "text": "2. GEE Core Concepts\n\n2.1 Geometry Objects\nEarth Engine uses geometries to define areas of interest (AOI).\n\n# Create a Point geometry (Metro Manila)\nmanila_point = ee.Geometry.Point([121.0, 14.6])\nprint(\"Manila Point:\", manila_point.getInfo())\n\n# Create a Rectangle (Palawan)\npalawan_bbox = ee.Geometry.Rectangle([117.5, 8.5, 119.5, 11.5])\nprint(\"\\nPalawan Bounding Box:\", palawan_bbox.getInfo())\n\n# Create a Polygon (custom AOI)\ncustom_polygon = ee.Geometry.Polygon([\n    [[120.8, 14.4], [121.2, 14.4], [121.2, 14.8], [120.8, 14.8], [120.8, 14.4]]\n])\nprint(\"\\nCustom Polygon:\", custom_polygon.getInfo())\n\n# Buffer around point (10 km)\nmanila_buffer = manila_point.buffer(10000)  # meters\nprint(\"\\nManila 10km Buffer area (km²):\", manila_buffer.area().divide(1e6).getInfo())\n\n\n\n2.2 Image and ImageCollection\n\nImage: A single raster image (one scene)\nImageCollection: A stack/time-series of images\n\n\n# Access Sentinel-2 Surface Reflectance collection\ns2_collection = ee.ImageCollection('COPERNICUS/S2_SR')\n\n# Get collection size (total images available - this will be very large!)\n# Note: This queries the entire global archive, so we'll filter first\n\n# Filter to Manila area for 2024\nmanila_s2 = s2_collection.filterBounds(manila_point) \\\n                         .filterDate('2024-01-01', '2024-12-31')\n\nprint(f\"Sentinel-2 images over Manila in 2024: {manila_s2.size().getInfo()}\")\n\n# Get the first image\nfirst_image = manila_s2.first()\nprint(f\"\\nFirst image ID: {first_image.get('system:id').getInfo()}\")\nprint(f\"Acquisition date: {ee.Date(first_image.get('system:time_start')).format('YYYY-MM-dd').getInfo()}\")\n\n\n\n2.3 Feature and FeatureCollection\nFeatures are vector data (points, lines, polygons with attributes).\n\n# Create a Feature (point with properties)\nmanila_feature = ee.Feature(\n    manila_point,\n    {'name': 'Metro Manila', 'population': 13000000, 'type': 'capital'}\n)\n\nprint(\"Manila Feature properties:\", manila_feature.getInfo()['properties'])\n\n# Create a FeatureCollection\ncities = ee.FeatureCollection([\n    ee.Feature(ee.Geometry.Point([121.0, 14.6]), {'name': 'Manila', 'pop': 13000000}),\n    ee.Feature(ee.Geometry.Point([125.6, 7.1]), {'name': 'Davao', 'pop': 1800000}),\n    ee.Feature(ee.Geometry.Point([123.9, 10.3]), {'name': 'Cebu', 'pop': 3000000})\n])\n\nprint(f\"\\nNumber of cities: {cities.size().getInfo()}\")\n\n\n\n2.4 Filters and Reducers\nFilters select subsets of collections. Reducers aggregate or summarize data.\n\n# Filtering examples\nfiltered = s2_collection \\\n    .filterBounds(palawan_bbox) \\\n    .filterDate('2024-06-01', '2024-08-31') \\\n    .filterMetadata('CLOUDY_PIXEL_PERCENTAGE', 'less_than', 20)\n\nprint(f\"Filtered images (Palawan, Jun-Aug 2024, &lt;20% clouds): {filtered.size().getInfo()}\")\n\n# Reducer examples\n# Create a median composite (reduces time dimension)\nmedian_composite = filtered.median()\n\n# Calculate mean NDVI over region (reduces spatial dimension)\n# We'll do this in detail later\nprint(\"\\nReducers allow you to:\")\nprint(\"  - Temporal: mean(), median(), max(), min() across time\")\nprint(\"  - Spatial: reduceRegion() for statistics over an area\")"
  },
  {
    "objectID": "notebooks/Day1_Session4_Google_Earth_Engine.html#working-with-sentinel-2",
    "href": "notebooks/Day1_Session4_Google_Earth_Engine.html#working-with-sentinel-2",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "3. Working with Sentinel-2",
    "text": "3. Working with Sentinel-2\n\n3.1 Define Area of Interest (AOI)\nWe’ll focus on Palawan Province - important for Natural Resource Management.\n\n# Define Palawan AOI\naoi = ee.Geometry.Rectangle([117.8, 9.0, 119.2, 10.8])\n\n# Calculate AOI area\naoi_area_km2 = aoi.area().divide(1e6).getInfo()\nprint(f\"AOI Area: {aoi_area_km2:.2f} km²\")\n\n# Visualize AOI bounds\nbounds = aoi.bounds().getInfo()['coordinates'][0]\nprint(f\"AOI Bounds: {bounds}\")\n\n\n\n3.2 Access Sentinel-2 Collection\n\n# Define date range (2024 dry season - less clouds)\nstart_date = '2024-01-01'\nend_date = '2024-03-31'\n\n# Access Sentinel-2 Surface Reflectance\ns2 = ee.ImageCollection('COPERNICUS/S2_SR') \\\n    .filterBounds(aoi) \\\n    .filterDate(start_date, end_date) \\\n    .filterMetadata('CLOUDY_PIXEL_PERCENTAGE', 'less_than', 30)\n\nprint(f\"Sentinel-2 images found: {s2.size().getInfo()}\")\n\n# List image dates and cloud cover\ndef get_image_info(image):\n    date = ee.Date(image.get('system:time_start')).format('YYYY-MM-dd')\n    clouds = image.get('CLOUDY_PIXEL_PERCENTAGE')\n    return ee.Feature(None, {'date': date, 'clouds': clouds})\n\nimage_info = s2.map(get_image_info).getInfo()['features']\n\nprint(\"\\nAvailable images:\")\nprint(f\"{'Date':&lt;15} {'Cloud %':&gt;10}\")\nprint(\"-\" * 25)\nfor info in image_info[:10]:  # Show first 10\n    props = info['properties']\n    print(f\"{props['date']:&lt;15} {props['clouds']:&gt;10.1f}\")\n\nif len(image_info) &gt; 10:\n    print(f\"... and {len(image_info) - 10} more images\")\n\n\n\n3.3 Cloud Masking Function\nSentinel-2 Level-2A includes quality bands for cloud masking.\n\ndef maskS2clouds(image):\n    \"\"\"\n    Mask clouds and cirrus in Sentinel-2 imagery using QA60 band.\n    \n    QA60 is a bitmask band:\n    - Bit 10: Opaque clouds\n    - Bit 11: Cirrus clouds\n    \n    Parameters:\n    -----------\n    image : ee.Image\n        Sentinel-2 Level-2A image\n    \n    Returns:\n    --------\n    ee.Image : Cloud-masked image\n    \"\"\"\n    qa = image.select('QA60')\n    \n    # Bits 10 and 11 are clouds and cirrus, respectively\n    cloudBitMask = 1 &lt;&lt; 10\n    cirrusBitMask = 1 &lt;&lt; 11\n    \n    # Both flags should be set to zero, indicating clear conditions\n    mask = qa.bitwiseAnd(cloudBitMask).eq(0).And(\n           qa.bitwiseAnd(cirrusBitMask).eq(0))\n    \n    return image.updateMask(mask).copyProperties(image, ['system:time_start'])\n\nprint(\"Cloud masking function defined!\")\nprint(\"This function will:\")\nprint(\"  1. Read the QA60 quality band\")\nprint(\"  2. Check bits 10 (clouds) and 11 (cirrus)\")\nprint(\"  3. Mask pixels where either bit is set\")\nprint(\"  4. Preserve image metadata\")\n\n\n\n3.4 Apply Cloud Masking and Create Composite\n\n# Apply cloud mask to all images\ns2_masked = s2.map(maskS2clouds)\n\nprint(f\"Cloud masking applied to {s2_masked.size().getInfo()} images\")\n\n# Create median composite\ncomposite = s2_masked.median().clip(aoi)\n\nprint(\"\\nMedian composite created!\")\nprint(\"Why median?\")\nprint(\"  - Robust to outliers (remaining clouds, shadows)\")\nprint(\"  - Better than mean for temporal composites\")\nprint(\"  - Produces clean, cloud-free images\")\n\n\n\n3.5 Visualize with Thumbnail\nEarth Engine can generate quick preview images.\n\n# Define visualization parameters for True Color (RGB)\nvis_params_rgb = {\n    'bands': ['B4', 'B3', 'B2'],  # Red, Green, Blue\n    'min': 0,\n    'max': 3000,\n    'gamma': 1.4  # Enhance contrast\n}\n\n# Get thumbnail URL\nthumbnail_url = composite.getThumbURL({\n    'region': aoi,\n    'dimensions': 512,\n    **vis_params_rgb\n})\n\nprint(\"True Color Composite (Sentinel-2 RGB):\")\ndisplay(Image(url=thumbnail_url))\n\n\n# False Color Composite (NIR, Red, Green) - highlights vegetation\nvis_params_false = {\n    'bands': ['B8', 'B4', 'B3'],  # NIR, Red, Green\n    'min': 0,\n    'max': 4000,\n    'gamma': 1.4\n}\n\nthumbnail_url_false = composite.getThumbURL({\n    'region': aoi,\n    'dimensions': 512,\n    **vis_params_false\n})\n\nprint(\"False Color Composite (NIR-R-G) - Vegetation appears RED:\")\ndisplay(Image(url=thumbnail_url_false))\n\n\n\n3.6 Calculate NDVI\nNDVI = (NIR - Red) / (NIR + Red)\n\n# Calculate NDVI using normalized difference\nndvi = composite.normalizedDifference(['B8', 'B4']).rename('NDVI')\n\nprint(\"NDVI calculated!\")\n\n# Get NDVI statistics over AOI\nndvi_stats = ndvi.reduceRegion(\n    reducer=ee.Reducer.mean().combine(\n        reducer2=ee.Reducer.minMax(),\n        sharedInputs=True\n    ),\n    geometry=aoi,\n    scale=10,  # 10m resolution\n    maxPixels=1e9\n).getInfo()\n\nprint(\"\\nNDVI Statistics:\")\nprint(f\"  Mean: {ndvi_stats['NDVI_mean']:.3f}\")\nprint(f\"  Min:  {ndvi_stats['NDVI_min']:.3f}\")\nprint(f\"  Max:  {ndvi_stats['NDVI_max']:.3f}\")\n\n\n# Visualize NDVI\nvis_params_ndvi = {\n    'bands': ['NDVI'],\n    'min': -0.2,\n    'max': 0.8,\n    'palette': ['blue', 'white', 'yellow', 'green', 'darkgreen']\n}\n\nthumbnail_url_ndvi = ndvi.getThumbURL({\n    'region': aoi,\n    'dimensions': 512,\n    **vis_params_ndvi\n})\n\nprint(\"NDVI (Normalized Difference Vegetation Index):\")\nprint(\"Blue/White: Water/Bare soil\")\nprint(\"Yellow: Sparse vegetation\")\nprint(\"Green: Moderate vegetation\")\nprint(\"Dark Green: Dense vegetation\\n\")\ndisplay(Image(url=thumbnail_url_ndvi))\n\n\n\nExercise 1: Change Location and Dates\nTask: Modify the code to analyze a different Philippine location and time period.\nSuggestions: - Metro Manila: [120.9, 14.4, 121.1, 14.7] - Mindanao (Davao): [125.3, 6.9, 125.7, 7.3] - Cebu: [123.7, 10.2, 124.0, 10.5]\nTry different seasons: - Dry season: January-May - Wet season: June-November\n\n# Your code here\n# Example: Metro Manila during wet season\n\n# Define new AOI\nmanila_aoi = ee.Geometry.Rectangle([120.9, 14.4, 121.1, 14.7])\n\n# New date range (wet season)\nnew_start = '2024-07-01'\nnew_end = '2024-09-30'\n\n# Query Sentinel-2\nmanila_s2 = ee.ImageCollection('COPERNICUS/S2_SR') \\\n    .filterBounds(manila_aoi) \\\n    .filterDate(new_start, new_end) \\\n    .filterMetadata('CLOUDY_PIXEL_PERCENTAGE', 'less_than', 30) \\\n    .map(maskS2clouds)\n\nprint(f\"Images found: {manila_s2.size().getInfo()}\")\n\n# Create composite\nmanila_composite = manila_s2.median().clip(manila_aoi)\n\n# Visualize\nmanila_thumb = manila_composite.getThumbURL({\n    'region': manila_aoi,\n    'dimensions': 512,\n    **vis_params_rgb\n})\n\nprint(\"\\nMetro Manila True Color Composite:\")\ndisplay(Image(url=manila_thumb))"
  },
  {
    "objectID": "notebooks/Day1_Session4_Google_Earth_Engine.html#working-with-sentinel-1-sar",
    "href": "notebooks/Day1_Session4_Google_Earth_Engine.html#working-with-sentinel-1-sar",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "4. Working with Sentinel-1 SAR",
    "text": "4. Working with Sentinel-1 SAR\nSentinel-1 provides all-weather, day-night radar imagery - essential for the Philippines’ cloudy tropical climate!\n\n4.1 Access Sentinel-1 Collection\n\n# Define parameters\nsar_aoi = palawan_bbox\nsar_start = '2024-01-01'\nsar_end = '2024-03-31'\n\n# Access Sentinel-1 GRD (Ground Range Detected)\ns1 = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n    .filterBounds(sar_aoi) \\\n    .filterDate(sar_start, sar_end) \\\n    .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n    .filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING')) \\\n    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n\nprint(f\"Sentinel-1 images found: {s1.size().getInfo()}\")\n\nprint(\"\\nFilters applied:\")\nprint(\"  - Instrument Mode: IW (Interferometric Wide swath)\")\nprint(\"  - Orbit: Descending (evening pass)\")\nprint(\"  - Polarization: VV and VH (dual-pol)\")\nprint(\"\\nWhy these filters?\")\nprint(\"  - IW mode: Standard for land monitoring (250km swath)\")\nprint(\"  - Descending: Consistent geometry\")\nprint(\"  - VV/VH: Sensitive to different surface properties\")\n\n\n\n4.2 Create SAR Composite\nFor SAR, we use mean to reduce speckle noise.\n\n# Select VV and VH bands\ns1_composite = s1.select(['VV', 'VH']).mean().clip(sar_aoi)\n\nprint(\"SAR composite created using mean (reduces speckle)\")\n\n# Calculate VV/VH ratio (useful for land cover)\nvv_vh_ratio = s1_composite.select('VV').divide(s1_composite.select('VH')).rename('VV_VH_ratio')\n\nprint(\"VV/VH ratio calculated (useful for classification)\")\n\n\n\n4.3 Visualize SAR Data\n\n# VV polarization visualization\nvis_params_vv = {\n    'bands': ['VV'],\n    'min': -25,\n    'max': 0\n}\n\nsar_thumb_vv = s1_composite.getThumbURL({\n    'region': sar_aoi,\n    'dimensions': 512,\n    **vis_params_vv\n})\n\nprint(\"Sentinel-1 VV Polarization (dB):\")\nprint(\"Dark areas: Water, calm surfaces (low backscatter)\")\nprint(\"Bright areas: Urban, rough surfaces (high backscatter)\\n\")\ndisplay(Image(url=sar_thumb_vv))\n\n\n# False color SAR (VV, VH, VV/VH ratio)\nsar_false_color = ee.Image.cat([\n    s1_composite.select('VV'),\n    s1_composite.select('VH'),\n    vv_vh_ratio\n])\n\nvis_params_sar_false = {\n    'min': [-25, -25, 0],\n    'max': [0, 0, 2]\n}\n\nsar_thumb_false = sar_false_color.getThumbURL({\n    'region': sar_aoi,\n    'dimensions': 512,\n    **vis_params_sar_false\n})\n\nprint(\"Sentinel-1 False Color (VV-VH-Ratio):\")\ndisplay(Image(url=sar_thumb_false))\n\n\n\nExercise 2: Compare VV and VH Polarizations\nTask: Create side-by-side visualizations of VV and VH polarizations.\nHint: VH is often more sensitive to vegetation volume.\n\n# Your code here\n# Solution:\n\n# VH polarization\nvis_params_vh = {\n    'bands': ['VH'],\n    'min': -25,\n    'max': 0\n}\n\nsar_thumb_vh = s1_composite.getThumbURL({\n    'region': sar_aoi,\n    'dimensions': 512,\n    **vis_params_vh\n})\n\nprint(\"VV Polarization:\")\ndisplay(Image(url=sar_thumb_vv))\n\nprint(\"\\nVH Polarization (more sensitive to vegetation structure):\")\ndisplay(Image(url=sar_thumb_vh))\n\nprint(\"\\nKey Differences:\")\nprint(\"  - VV: Better for water detection, urban areas\")\nprint(\"  - VH: Better for vegetation, forest structure\")\nprint(\"  - Using both improves classification accuracy!\")"
  },
  {
    "objectID": "notebooks/Day1_Session4_Google_Earth_Engine.html#data-export-for-aiml-workflows",
    "href": "notebooks/Day1_Session4_Google_Earth_Engine.html#data-export-for-aiml-workflows",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "5. Data Export for AI/ML Workflows",
    "text": "5. Data Export for AI/ML Workflows\nTo train custom ML models, we need to export data from Earth Engine.\n\n5.1 Export Image to Google Drive\n\n# Export Sentinel-2 composite\nexport_task_s2 = ee.batch.Export.image.toDrive(\n    image=composite.select(['B2', 'B3', 'B4', 'B8']),  # Select bands to export\n    description='Palawan_S2_Composite_Q1_2024',\n    folder='EarthEngine_Exports',\n    fileNamePrefix='palawan_s2_composite',\n    region=aoi,\n    scale=10,  # 10m resolution\n    crs='EPSG:4326',\n    maxPixels=1e9\n)\n\n# Start the export task\nexport_task_s2.start()\n\nprint(\"Export task started!\")\nprint(f\"Task ID: {export_task_s2.id}\")\nprint(\"\\nExport details:\")\nprint(f\"  - Destination: Google Drive/EarthEngine_Exports/\")\nprint(f\"  - Filename: palawan_s2_composite.tif\")\nprint(f\"  - Bands: B2, B3, B4, B8 (Blue, Green, Red, NIR)\")\nprint(f\"  - Resolution: 10m\")\nprint(f\"  - Format: GeoTIFF\")\nprint(\"\\nMonitor status at: https://code.earthengine.google.com/tasks\")\n\n\n\n5.2 Check Export Status\n\n# Check task status\ntask_status = export_task_s2.status()\nprint(f\"Task Status: {task_status['state']}\")\n\nif task_status['state'] == 'RUNNING':\n    print(\"Task is running... Check back in a few minutes.\")\nelif task_status['state'] == 'COMPLETED':\n    print(\"Task completed! Check your Google Drive.\")\nelif task_status['state'] == 'FAILED':\n    print(f\"Task failed: {task_status.get('error_message', 'Unknown error')}\")\nelse:\n    print(f\"Task state: {task_status['state']}\")\n\n\n\n5.3 Export NDVI\n\n# Export NDVI layer\nexport_task_ndvi = ee.batch.Export.image.toDrive(\n    image=ndvi,\n    description='Palawan_NDVI_Q1_2024',\n    folder='EarthEngine_Exports',\n    fileNamePrefix='palawan_ndvi',\n    region=aoi,\n    scale=10,\n    crs='EPSG:4326',\n    maxPixels=1e9\n)\n\nexport_task_ndvi.start()\n\nprint(f\"NDVI export started!\")\nprint(f\"Task ID: {export_task_ndvi.id}\")\n\n\n\n5.4 Export Training Samples (for ML)\nFor ML model training, we often need to export training samples as vectors.\n\n# Create sample points for different land cover types\n# In practice, you would digitize these in GEE Code Editor or use existing data\n\n# Example: Random sample points\nsample_points = composite.sample(\n    region=aoi,\n    scale=30,  # Sample every 30m\n    numPixels=1000,  # Number of samples\n    seed=42  # For reproducibility\n)\n\nprint(f\"Generated {sample_points.size().getInfo()} sample points\")\n\n# Export samples to Drive as CSV\nexport_task_samples = ee.batch.Export.table.toDrive(\n    collection=sample_points,\n    description='Palawan_Training_Samples',\n    folder='EarthEngine_Exports',\n    fileNamePrefix='palawan_samples',\n    fileFormat='CSV'\n)\n\nexport_task_samples.start()\n\nprint(f\"\\nSample points export started!\")\nprint(f\"Task ID: {export_task_samples.id}\")\nprint(\"\\nThese samples can be used for:\")\nprint(\"  - Training ML classifiers\")\nprint(\"  - Validating model predictions\")\nprint(\"  - Feature engineering\")\n\n\n\nExercise 3: Export Custom AOI Composite\nTask: Export a composite for your chosen location from Exercise 1.\nRequirements: - Use your custom AOI - Export RGB bands (B2, B3, B4) - 10m resolution - Give it a meaningful filename\n\n# Your code here\n# Solution template:\n\nmy_export_task = ee.batch.Export.image.toDrive(\n    image=manila_composite.select(['B2', 'B3', 'B4']),\n    description='My_Custom_Export',\n    folder='EarthEngine_Exports',\n    fileNamePrefix='my_custom_composite',\n    region=manila_aoi,\n    scale=10,\n    crs='EPSG:4326',\n    maxPixels=1e9\n)\n\nmy_export_task.start()\nprint(f\"Custom export started! Task ID: {my_export_task.id}\")"
  },
  {
    "objectID": "notebooks/Day1_Session4_Google_Earth_Engine.html#integration-with-aiml-workflows",
    "href": "notebooks/Day1_Session4_Google_Earth_Engine.html#integration-with-aiml-workflows",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "6. Integration with AI/ML Workflows",
    "text": "6. Integration with AI/ML Workflows\n\n6.1 Preparing Training Data\nEarth Engine excels at preparing analysis-ready data for ML.\n\n# Create a multi-band image stack for ML\nml_stack = composite.select(['B2', 'B3', 'B4', 'B8', 'B11', 'B12']) \\\n                    .addBands(ndvi) \\\n                    .addBands(s1_composite.select(['VV', 'VH']))\n\nprint(\"ML-ready image stack created!\")\nprint(\"\\nBands included:\")\nband_names = ml_stack.bandNames().getInfo()\nfor i, band in enumerate(band_names, 1):\n    print(f\"  {i}. {band}\")\n\nprint(\"\\nWhy this combination?\")\nprint(\"  - Optical bands (B2-B12): Spectral information\")\nprint(\"  - NDVI: Vegetation index\")\nprint(\"  - SAR (VV, VH): All-weather information\")\nprint(\"  - Multi-sensor fusion improves classification!\")\n\n\n\n6.2 Sampling for Training\nExtract feature vectors for ML model training.\n\n# Define training regions (in practice, digitize or load from shapefile)\n# For demonstration, we'll create simple point collections\n\n# Forest training points\nforest_points = ee.FeatureCollection([\n    ee.Feature(ee.Geometry.Point([118.5, 10.2]), {'landcover': 0, 'class_name': 'Forest'}),\n    ee.Feature(ee.Geometry.Point([118.6, 10.3]), {'landcover': 0, 'class_name': 'Forest'}),\n    ee.Feature(ee.Geometry.Point([118.4, 10.1]), {'landcover': 0, 'class_name': 'Forest'})\n])\n\n# Water training points\nwater_points = ee.FeatureCollection([\n    ee.Feature(ee.Geometry.Point([118.2, 9.5]), {'landcover': 1, 'class_name': 'Water'}),\n    ee.Feature(ee.Geometry.Point([118.3, 9.6]), {'landcover': 1, 'class_name': 'Water'})\n])\n\n# Merge training points\ntraining_points = forest_points.merge(water_points)\n\n# Sample image at training points\ntraining_data = ml_stack.sampleRegions(\n    collection=training_points,\n    properties=['landcover', 'class_name'],\n    scale=10\n)\n\nprint(f\"Training samples created: {training_data.size().getInfo()}\")\nprint(\"\\nSample features:\")\nsample = training_data.first().getInfo()\nprint(f\"Properties: {list(sample['properties'].keys())}\")\n\nprint(\"\\nNext steps (Day 2):\")\nprint(\"  1. Export training data\")\nprint(\"  2. Train Random Forest classifier\")\nprint(\"  3. Apply classifier to image\")\nprint(\"  4. Validate results\")\n\n\n\n6.3 Earth Engine Built-in ML (Preview)\nGEE has built-in classifiers for quick prototyping.\n\n# Train a simple classifier (Random Forest)\n# Note: This is a preview - we'll cover this in detail on Day 2\n\nclassifier = ee.Classifier.smileRandomForest(\n    numberOfTrees=10\n).train(\n    features=training_data,\n    classProperty='landcover',\n    inputProperties=ml_stack.bandNames()\n)\n\n# Classify the image\nclassified = ml_stack.classify(classifier)\n\nprint(\"Simple classification performed!\")\nprint(\"\\nNote: This is a minimal example.\")\nprint(\"On Day 2, we'll learn:\")\nprint(\"  - Proper training data collection\")\nprint(\"  - Feature selection\")\nprint(\"  - Model validation\")\nprint(\"  - Accuracy assessment\")\n\n# Visualize classification\nvis_params_class = {\n    'min': 0,\n    'max': 1,\n    'palette': ['green', 'blue']  # Forest, Water\n}\n\nclass_thumb = classified.getThumbURL({\n    'region': aoi,\n    'dimensions': 512,\n    **vis_params_class\n})\n\nprint(\"\\nSimple Classification Result:\")\ndisplay(Image(url=class_thumb))"
  },
  {
    "objectID": "notebooks/Day1_Session4_Google_Earth_Engine.html#best-practices-and-tips",
    "href": "notebooks/Day1_Session4_Google_Earth_Engine.html#best-practices-and-tips",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "7. Best Practices and Tips",
    "text": "7. Best Practices and Tips\n\n7.1 Memory Management\n\n# Best Practices:\nprint(\"Earth Engine Best Practices:\")\nprint(\"\\n1. MEMORY MANAGEMENT:\")\nprint(\"   - Avoid .getInfo() on large objects (use for small metadata only)\")\nprint(\"   - Use .limit() to restrict collection size during testing\")\nprint(\"   - Export large results instead of downloading\")\n\n# Example: Limit collection size\nlimited_collection = s2.limit(5)\nprint(f\"\\n   Limited collection size: {limited_collection.size().getInfo()}\")\n\nprint(\"\\n2. COMPUTATIONAL QUOTAS:\")\nprint(\"   - Free tier: 250GB Cloud Storage, 10k+ compute hours/month\")\nprint(\"   - Set maxPixels appropriately (default: 1e8)\")\nprint(\"   - Use appropriate scale (don't oversample)\")\n\nprint(\"\\n3. EFFICIENCY:\")\nprint(\"   - Filter early: bounds → date → metadata\")\nprint(\"   - Select only needed bands\")\nprint(\"   - Clip to AOI before intensive operations\")\n\n\n\n7.2 When to Use GEE vs Local Processing\n\nprint(\"USE GOOGLE EARTH ENGINE FOR:\")\nprint(\"  ✓ Data access and pre-processing\")\nprint(\"  ✓ Large-scale spatial analysis\")\nprint(\"  ✓ Time series analysis\")\nprint(\"  ✓ Cloud masking and compositing\")\nprint(\"  ✓ Simple ML (Random Forest, CART)\")\nprint(\"  ✓ Zonal statistics\")\nprint(\"  ✓ Rapid prototyping\")\n\nprint(\"\\nUSE LOCAL PROCESSING (Python/Colab) FOR:\")\nprint(\"  ✓ Deep learning (CNN, U-Net, LSTM)\")\nprint(\"  ✓ Custom model architectures\")\nprint(\"  ✓ Fine-grained control over training\")\nprint(\"  ✓ Integration with TensorFlow/PyTorch\")\nprint(\"  ✓ Advanced data augmentation\")\nprint(\"  ✓ Transfer learning\")\n\nprint(\"\\nBEST WORKFLOW:\")\nprint(\"  1. Use GEE for data preparation\")\nprint(\"  2. Export training data\")\nprint(\"  3. Train models locally (Colab GPU)\")\nprint(\"  4. Deploy models on new data\")\n\n\n\n7.3 Troubleshooting Common Errors\n\nprint(\"COMMON ERRORS AND SOLUTIONS:\\n\")\n\nprint(\"1. 'User memory limit exceeded'\")\nprint(\"   → Reduce AOI size or increase scale\")\nprint(\"   → Use .limit() on collections\")\nprint(\"   → Export instead of .getInfo()\")\n\nprint(\"\\n2. 'Computation timed out'\")\nprint(\"   → Simplify operations\")\nprint(\"   → Filter collections more aggressively\")\nprint(\"   → Break into smaller exports\")\n\nprint(\"\\n3. 'EEException: Collection.first: No matching elements'\")\nprint(\"   → Check date range (no images available)\")\nprint(\"   → Verify AOI (outside coverage?)\")\nprint(\"   → Relax filters (clouds, etc.)\")\n\nprint(\"\\n4. Export task fails\")\nprint(\"   → Check maxPixels limit\")\nprint(\"   → Verify Google Drive space\")\nprint(\"   → Check region coordinates\")\n\nprint(\"\\n5. 'Image.select: Pattern X did not match any bands'\")\nprint(\"   → Check band names: .bandNames().getInfo()\")\nprint(\"   → Verify dataset (S2 vs S2_SR bands differ)\")"
  },
  {
    "objectID": "notebooks/Day1_Session4_Google_Earth_Engine.html#key-takeaways",
    "href": "notebooks/Day1_Session4_Google_Earth_Engine.html#key-takeaways",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "8. Key Takeaways",
    "text": "8. Key Takeaways\nWhat You’ve Learned:\n\nGEE Authentication & Setup\n\nOne-time authentication process\nInitialize for each session\nPython API basics\n\nCore GEE Concepts\n\nGeometry: Points, Rectangles, Polygons\nImage & ImageCollection: Raster data\nFeature & FeatureCollection: Vector data\nFilters: Subset data by space, time, metadata\nReducers: Aggregate/summarize data\n\nSentinel-2 Workflows\n\nAccess surface reflectance data\nCloud masking with QA60\nCreate median composites\nCalculate NDVI\nVisualize with thumbnails\n\nSentinel-1 SAR\n\nAll-weather imaging capability\nVV and VH polarizations\nSpeckle reduction with mean\nComplementary to optical data\n\nData Export\n\nExport to Google Drive\nImages (GeoTIFF) and tables (CSV)\nMonitor tasks\nPrepare data for ML\n\nML Integration\n\nPrepare multi-band stacks\nSample training data\nBuilt-in classifiers (preview)\nGEE ↔︎ Local workflow"
  },
  {
    "objectID": "notebooks/Day1_Session4_Google_Earth_Engine.html#next-steps",
    "href": "notebooks/Day1_Session4_Google_Earth_Engine.html#next-steps",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "9. Next Steps",
    "text": "9. Next Steps\nDay 2: We’ll apply these skills to build Machine Learning classification models:\n\nRandom Forest for land cover classification\nFeature engineering and selection\nTraining data collection strategies\nModel validation and accuracy assessment\nPhilippine case study: Palawan land cover mapping\n\nDay 3-4: Advanced deep learning: - CNNs for image classification - U-Net for semantic segmentation - Object detection - Time series analysis with LSTMs"
  },
  {
    "objectID": "notebooks/Day1_Session4_Google_Earth_Engine.html#additional-resources",
    "href": "notebooks/Day1_Session4_Google_Earth_Engine.html#additional-resources",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "10. Additional Resources",
    "text": "10. Additional Resources\n\nOfficial Documentation\n\nEarth Engine Guide: https://developers.google.com/earth-engine/\nPython API Intro: https://developers.google.com/earth-engine/tutorials/community/intro-to-python-api\nData Catalog: https://developers.google.com/earth-engine/datasets/\n\n\n\nTutorials\n\nEnd-to-End GEE Course: https://courses.spatialthoughts.com/end-to-end-gee.html\nGEE Community Tutorials: https://github.com/google/earthengine-community\nAwesome Earth Engine: https://github.com/giswqs/Awesome-GEE\n\n\n\nPhilippine Context\n\nPhilSA: https://philsa.gov.ph/\nCopPhil Mirror Site: (Coming 2025)\nDOST-ASTI: https://asti.dost.gov.ph/\n\n\n\nBooks\n\nCloud-Based Remote Sensing with Google Earth Engine (Cardille et al.)\nEarth Observation Using Python (Parente & Pepe)"
  },
  {
    "objectID": "notebooks/Day1_Session4_Google_Earth_Engine.html#practice-exercises",
    "href": "notebooks/Day1_Session4_Google_Earth_Engine.html#practice-exercises",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "11. Practice Exercises",
    "text": "11. Practice Exercises\nTo reinforce your learning, try these exercises:\n\nExercise A: Multi-temporal Analysis\nCreate composites for different seasons (dry vs wet) and compare NDVI changes.\n\n\nExercise B: Multi-location Comparison\nCompare NDVI between different Philippine regions (urban vs forest vs agriculture).\n\n\nExercise C: Sentinel-1 Flood Detection\nUse SAR data to identify potential flood areas (low VV backscatter).\n\n\nExercise D: Data Fusion\nCombine Sentinel-1 and Sentinel-2 to create a comprehensive dataset for classification.\n\n\nExercise E: Time Series\nPlot NDVI time series for a specific location over an entire year."
  },
  {
    "objectID": "notebooks/Day1_Session4_Google_Earth_Engine.html#congratulations",
    "href": "notebooks/Day1_Session4_Google_Earth_Engine.html#congratulations",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "Congratulations!",
    "text": "Congratulations!\nYou’ve completed Day 1 of the CopPhil AI/ML Training!\nYou now have: - ✓ Python geospatial skills (GeoPandas, Rasterio) - ✓ Google Earth Engine proficiency - ✓ Access to petabytes of satellite data - ✓ Ability to prepare data for AI/ML\nTomorrow: We build our first machine learning models!\n\nGenerated with Claude Code for CopPhil Digital Space Campus\nEU-Philippines Copernicus Capacity Support Programme\nData-Centric AI for Earth Observation"
  }
]