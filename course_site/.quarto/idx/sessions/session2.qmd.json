{"title":"Session 2: Core Concepts of AI/ML for Earth Observation","markdown":{"yaml":{"title":"Session 2: Core Concepts of AI/ML for Earth Observation","subtitle":"Understanding the fundamentals of machine learning for satellite data analysis","date":"last-modified"},"headingText":"Session Overview","containsRefs":false,"markdown":"\n\n::: {.session-info}\n**Duration:** 2 hours | **Format:** Lecture + Conceptual Exercises | **Platform:** Presentation\n:::\n\n\nThis session provides a comprehensive introduction to Artificial Intelligence and Machine Learning concepts specifically tailored for Earth Observation applications. You'll learn the complete AI/ML workflow, understand different learning paradigms, explore neural network fundamentals, and discover why data quality matters more than model complexity in 2025's data-centric AI paradigm.\n\n::: {.learning-objectives}\n### Learning Objectives\n\nBy the end of this session, you will be able to:\n\n- **Define** AI and ML in the context of Earth Observation\n- **Describe** the complete AI/ML workflow from problem definition to deployment\n- **Distinguish** between supervised and unsupervised learning with EO examples\n- **Explain** classification vs. regression tasks in satellite data analysis\n- **Understand** neural network architecture fundamentals\n- **Identify** key components: neurons, layers, activation functions, loss functions, optimizers\n- **Articulate** the data-centric AI paradigm and its importance for EO\n- **Apply** best practices for data quality, quantity, diversity, and annotation\n:::\n\n---\n\n## Part 1: What is AI/ML?\n\n### Defining the Terms\n\n**Artificial Intelligence (AI):**\n\n- Broad field focused on creating intelligent machines\n- Systems that can perceive, reason, learn, and act\n- Includes everything from rule-based systems to machine learning\n\n**Machine Learning (ML):**\n\n- Subset of AI focused on learning from data\n- Algorithms that improve performance through experience\n- **Key distinction:** No explicit programming of rules\n\n::: {.callout-note}\n## The ML Difference\n\n**Traditional Programming:**\n\n```\nRules + Data → Output\n```\n\n**Machine Learning:**\n\n```\nData + Desired Output → Rules (Model)\n```\n\nIn EO: Instead of coding \"if NIR > 0.6 and Red < 0.3, then forest\", ML learns the pattern from labeled examples.\n:::\n\n### Why ML for Earth Observation?\n\n**Challenges that ML addresses:**\n\n1. **Scale:** Petabytes of satellite data - impossible to manually analyze\n2. **Complexity:** Multispectral, temporal, spatial patterns humans can't easily detect\n3. **Consistency:** Automated processing ensures reproducible results\n4. **Speed:** Real-time disaster mapping requires immediate analysis\n\n**Traditional vs. ML approaches:**\n\n| Task | Traditional | ML Approach |\n|------|-------------|-------------|\n| **Water detection** | Manual NDWI threshold | Learn optimal threshold + texture from examples |\n| **Land cover** | Rule-based classification | Random Forest or CNN with training samples |\n| **Flood mapping** | Expert visual interpretation | U-Net segmentation trained on labeled floods |\n| **Crop monitoring** | Fixed vegetation index thresholds | LSTM time series model learning phenology |\n\n---\n\n## Part 2: The AI/ML Workflow for Earth Observation\n\nUnderstanding the complete workflow is essential for successful EO projects. Each step matters.\n\n### Step 1: Problem Definition\n\n**Define clearly what you want to achieve:**\n\n- What question are you answering? (e.g., \"Where are mangroves declining?\")\n- What output do you need? (map, time series, alert system?)\n- What accuracy is acceptable?\n- What constraints exist? (time, computational resources, data availability)\n\n::: {.philippine-context}\n**Philippine Example:**\n\n**Problem:** Map rice paddies in Central Luzon to estimate harvest timing for food security\n\n**Clear definition:**\n- Binary classification: rice vs. non-rice\n- 20m spatial resolution acceptable (Sentinel-2 bands)\n- Temporal: wet and dry season separately\n- Accuracy target: >85% for operational use\n:::\n\n### Step 2: Data Acquisition\n\n**Gather all necessary data:**\n\n- **Satellite imagery:** Sentinel-1/2, Landsat, commercial VHR\n- **Ground truth:** Field surveys, high-res imagery interpretation, existing maps\n- **Ancillary data:** DEM, climate, administrative boundaries\n\n**Data sources for Philippines:**\n\n- Copernicus Data Space Ecosystem (Sentinel-1/2)\n- PhilSA SIYASAT (NovaSAR-1)\n- NAMRIA Geoportal (land cover basemaps)\n- PAGASA (climate data)\n\n### Step 3: Data Pre-processing\n\n**Critical step - \"Garbage in, garbage out\"**\n\n**For satellite imagery:**\n\n- **Atmospheric correction:** Convert to surface reflectance (use Level-2A)\n- **Cloud masking:** Remove or mask cloudy pixels\n- **Geometric correction:** Ensure proper alignment\n- **Radiometric calibration:** Consistent values across scenes\n- **Temporal compositing:** Reduce clouds via median/mean composites\n\n**For training labels:**\n\n- **Quality control:** Verify label accuracy\n- **Coordinate alignment:** Ensure labels match imagery timing and location\n- **Class balancing:** Ensure adequate samples per class\n- **Format standardization:** Convert to ML-ready format\n\n::: {.callout-warning}\n## Pre-processing Pitfalls\n\n**Common errors that degrade model performance:**\n\n- Using Top-of-Atmosphere instead of surface reflectance\n- Temporal mismatch: 2020 imagery with 2018 labels\n- Incomplete cloud masking leaving cloud shadows\n- Mixed pixels at boundaries (especially for validation)\n- Inconsistent band ordering across scenes\n:::\n\n### Step 4: Feature Engineering\n\n**Deriving informative variables from raw data**\n\n**For traditional ML (Random Forest, SVM):**\n\n- **Spectral indices:** NDVI, NDWI, NDBI, EVI, SAVI\n- **Textural features:** GLCM metrics (contrast, entropy)\n- **Temporal features:** Mean, std dev, phenology metrics\n- **Topographic features:** Elevation, slope, aspect (from DEM)\n- **Contextual features:** Distance to roads, water bodies\n\n**Example: Forest classification features**\n\n```python\n# Spectral indices\nNDVI = (NIR - Red) / (NIR + Red)\nNDWI = (Green - NIR) / (Green + NIR)\n\n# Texture (from GLCM)\nContrast = ...  # measure of local variation\nHomogeneity = ...  # measure of uniformity\n\n# Topographic\nElevation, Slope\n\n# Result: Input feature vector per pixel\nX = [Red, Green, Blue, NIR, SWIR1, SWIR2, NDVI, NDWI, Contrast, Elevation, Slope]\n```\n\n**For deep learning (CNNs):**\n\n- Less manual feature engineering needed\n- Networks automatically learn features from raw pixels\n- Still benefit from good input data (cloud-free, calibrated)\n\n### Step 5: Model Selection and Training\n\n**Choose appropriate algorithm:**\n\n**Consider:**\n\n- Task type (classification, regression, segmentation)\n- Data size (deep learning needs more data)\n- Interpretability requirements\n- Computational resources\n- Deployment constraints\n\n**Common EO algorithms:**\n\n| Algorithm | Type | Best For | Data Needs |\n|-----------|------|----------|------------|\n| **Random Forest** | Ensemble | Classification, feature importance | Medium |\n| **SVM** | Kernel | Binary classification, small data | Small-Medium |\n| **CNN** | Deep Learning | Image classification, automatic features | Large |\n| **U-Net** | Deep Learning | Semantic segmentation (pixel-wise) | Large |\n| **LSTM** | Deep Learning | Time series prediction | Large |\n\n**Training process:**\n\n1. Split data: training (70%), validation (15%), testing (15%)\n2. Feed training data to algorithm\n3. Algorithm adjusts parameters to minimize error\n4. Monitor performance on validation set\n5. Iterate: adjust hyperparameters if needed\n\n### Step 6: Validation and Evaluation\n\n**Rigorous testing on independent data**\n\n::: {.callout-important}\n## Never Test on Training Data!\n\nTesting on data the model has seen gives falsely optimistic results. Always use held-out test data.\n:::\n\n**Classification metrics:**\n\n- **Overall Accuracy:** Percentage of correctly classified pixels\n- **Confusion Matrix:** Shows which classes are confused\n- **Producer's Accuracy:** How many ground truth samples were correctly classified\n- **User's Accuracy:** How many predicted samples are actually correct\n- **Kappa Coefficient:** Agreement accounting for chance\n- **F1-Score:** Harmonic mean of precision and recall\n\n**Regression metrics:**\n\n- **RMSE (Root Mean Squared Error):** Average prediction error\n- **MAE (Mean Absolute Error):** Average absolute deviation\n- **R² (Coefficient of Determination):** Proportion of variance explained\n\n**Philippine Example: Flood mapping evaluation**\n\n```\nConfusion Matrix:\n                Predicted\n              | Flood | No Flood |\nActual Flood  |  450  |   50     |  Producer's Acc: 90%\nActual No Flood|  30   |  1470    |  Producer's Acc: 98%\n\nUser's Accuracy: 93.8%   96.7%\nOverall Accuracy: 96%\n```\n\n### Step 7: Deployment and Operationalization\n\n**Making the model operational:**\n\n**Deployment strategies:**\n\n1. **Batch processing:** Apply model to large archives\n2. **Near real-time:** Process new satellite acquisitions automatically\n3. **On-demand:** User-triggered analysis\n4. **Edge processing:** On-board satellite AI (ESA Φsat-2)\n\n**Operational considerations:**\n\n- **Scalability:** Can it handle regional/national scale?\n- **Automation:** Minimize manual intervention\n- **Monitoring:** Track performance over time\n- **Retraining:** Update model as conditions change\n- **Integration:** Connect to decision support systems\n\n**Philippine context:**\n\n- DOST-ASTI AIPI platform for model deployment\n- DIMER repository for model sharing\n- Integration with LGU disaster response protocols\n- Delivery via PhilSA Digital Space Campus\n\n---\n\n## Part 3: Types of Machine Learning\n\n### Supervised Learning\n\n**Learning from labeled data**\n\nThe algorithm is given:\n- **Input:** Satellite image or features\n- **Output:** Known label (class or value)\n- **Goal:** Learn mapping from input to output\n\n#### Classification Tasks\n\n**Predicting categorical labels**\n\n**EO Examples:**\n\n1. **Land Cover Classification**\n   - Input: Sentinel-2 pixel values\n   - Output: Forest, Water, Urban, Agriculture, Bare soil\n   - Algorithm: Random Forest, CNN\n\n2. **Cloud Detection**\n   - Input: Multi-band imagery\n   - Output: Cloud vs. Clear\n   - Algorithm: Threshold or ML classifier\n\n3. **Crop Type Mapping**\n   - Input: Multi-temporal NDVI\n   - Output: Rice, Corn, Sugarcane, Coconut\n   - Algorithm: Random Forest or LSTM\n\n::: {.philippine-context}\n**Philippine Case Study: Mangrove Mapping**\n\n**Task:** Classify pixels as mangrove or non-mangrove in coastal areas\n\n**Data:**\n- Sentinel-2 multi-temporal imagery (dry and wet season)\n- Field-validated mangrove polygons\n- NAMRIA coastal land cover baseline\n\n**Approach:**\n- Extract spectral values and indices (NDVI, NDWI)\n- Train Random Forest classifier\n- Validate against independent field data\n- Deploy via DOST-ASTI AIPI\n\n**Result:** 92% accuracy mangrove extent map for Palawan coastline\n:::\n\n#### Regression Tasks\n\n**Predicting continuous values**\n\n**EO Examples:**\n\n1. **Biomass Estimation**\n   - Input: Sentinel-1 SAR backscatter, Sentinel-2 vegetation indices\n   - Output: Forest biomass (tons per hectare)\n   - Algorithm: Random Forest Regression\n\n2. **Soil Moisture Prediction**\n   - Input: Sentinel-1 VV/VH polarization, temperature\n   - Output: Volumetric soil moisture (%)\n   - Algorithm: Neural network regression\n\n3. **Crop Yield Forecasting**\n   - Input: NDVI time series, rainfall, temperature\n   - Output: Expected yield (tons per hectare)\n   - Algorithm: LSTM regression\n\n**Key difference from classification:**\n- Output is a number on a continuous scale\n- Loss functions measure distance from true value (MSE, RMSE)\n- Evaluation uses regression metrics (R², RMSE)\n\n### Unsupervised Learning\n\n**Finding patterns in unlabeled data**\n\nThe algorithm receives:\n- **Input:** Satellite imagery or features\n- **No labels provided**\n- **Goal:** Discover inherent structure or groupings\n\n#### Clustering\n\n**Grouping similar pixels/regions together**\n\n**Common algorithm: k-means**\n\n1. Specify number of clusters (k)\n2. Algorithm iteratively groups pixels with similar spectral characteristics\n3. Result: Image segmented into k clusters\n4. **Human interpretation needed:** \"Cluster 1 looks like water, Cluster 2 like forest...\"\n\n**EO Applications:**\n\n- **Exploratory analysis:** \"How many distinct spectral classes in this region?\"\n- **Change detection:** Cluster before/after images to find anomalies\n- **Image segmentation:** Group similar pixels for object-based analysis\n\n::: {.callout-tip}\n## When to Use Unsupervised Learning\n\n**Advantages:**\n- No need for expensive labeled data\n- Can discover unexpected patterns\n- Good for initial data exploration\n\n**Limitations:**\n- Results need interpretation\n- No guarantee clusters match desired classes\n- Often less accurate than supervised methods for specific tasks\n- Difficult to evaluate objectively\n:::\n\n**Comparison Example:**\n\n**Supervised (Land Cover Classification):**\n- Provide 1000 labeled samples: forest, water, urban\n- Train Random Forest\n- Result: Every pixel assigned forest/water/urban\n- Evaluation: 90% accuracy against test labels\n\n**Unsupervised (k-means Clustering):**\n- No labels provided\n- Run k-means with k=3\n- Result: Three clusters emerge\n- Interpretation: Cluster A=water, B=vegetation, C=mixed urban/bare\n- Evaluation: Subjective or requires labels anyway\n\n---\n\n## Part 4: Introduction to Deep Learning\n\n### What is Deep Learning?\n\n**Deep Learning = Neural Networks with Many Layers**\n\n- Subset of machine learning\n- Inspired by biological neurons\n- Multiple processing layers extract progressively abstract features\n- Dominant approach for image analysis since ~2012\n\n**Why \"deep\"?**\n- Refers to depth: many hidden layers\n- Modern networks: 10s to 100s of layers\n- Enables learning complex, hierarchical representations\n\n### Neural Network Fundamentals\n\n#### The Artificial Neuron\n\n**Building block of neural networks:**\n\n```\nInputs (x1, x2, x3) → [Weighted Sum + Bias] → Activation Function → Output\n```\n\n**Mathematical operation:**\n\n1. **Weighted sum:** `z = w1*x1 + w2*x2 + w3*x3 + b`\n2. **Activation function:** `output = activation(z)`\n\n**Example: Detecting bright pixels**\n\n```\nInputs: [Red=0.8, Green=0.7, NIR=0.9]\nWeights: [w1=1.0, w2=1.0, w3=1.0]\nBias: b = -2.0\n\nz = 1.0*0.8 + 1.0*0.7 + 1.0*0.9 - 2.0 = 0.4\noutput = ReLU(0.4) = 0.4  (indicates moderately bright)\n```\n\n#### Network Architecture\n\n**Layers of neurons:**\n\n1. **Input Layer:** Receives raw data (e.g., pixel values)\n2. **Hidden Layers:** Process and transform data\n3. **Output Layer:** Produces final prediction\n\n**For a simple image classification:**\n\n```\nInput Layer (256 neurons = 16x16 image)\n   ↓\nHidden Layer 1 (128 neurons with ReLU)\n   ↓\nHidden Layer 2 (64 neurons with ReLU)\n   ↓\nOutput Layer (5 neurons = 5 classes, softmax activation)\n```\n\nEach connection has a **weight** - the network learns optimal weights through training.\n\n#### Activation Functions\n\n**Introduce non-linearity - crucial for learning complex patterns**\n\n**Common activation functions:**\n\n| Function | Equation | Use Case |\n|----------|----------|----------|\n| **ReLU** | `max(0, x)` | Hidden layers (most common) |\n| **Sigmoid** | `1 / (1 + e^-x)` | Binary classification output |\n| **Softmax** | `e^xi / Σe^xj` | Multi-class classification output |\n| **Tanh** | `(e^x - e^-x) / (e^x + e^-x)` | Hidden layers (older) |\n\n**Why activation functions matter:**\n\nWithout non-linearity, multiple layers would collapse to a single linear transformation - no benefit from depth!\n\n::: {.callout-note}\n## ReLU: The Default Choice\n\n**ReLU (Rectified Linear Unit)** has become standard for hidden layers because:\n\n- Simple: `f(x) = max(0, x)`\n- Computationally efficient\n- Avoids vanishing gradient problem\n- Empirically performs very well\n:::\n\n#### Loss Functions\n\n**Measure how wrong the model's predictions are**\n\nThe model's objective: **minimize the loss function**\n\n**For classification:**\n\n**Categorical Cross-Entropy:**\n\n```\nLoss = -Σ(y_true * log(y_pred))\n```\n\n- Penalizes confident wrong predictions heavily\n- Encourages high probability for correct class\n\n**Example:**\n```\nTrue class: Forest (encoded as [1, 0, 0, 0, 0])\nPrediction: [0.7, 0.1, 0.1, 0.05, 0.05]  ← Good, 70% on forest\nLoss = -1*log(0.7) = 0.36\n\nPrediction: [0.2, 0.3, 0.4, 0.05, 0.05]  ← Bad, only 20% on forest\nLoss = -1*log(0.2) = 1.61  (much higher penalty)\n```\n\n**For regression:**\n\n**Mean Squared Error (MSE):**\n\n```\nLoss = (1/n) * Σ(y_true - y_pred)²\n```\n\n**Example: Biomass prediction:**\n```\nTrue: 150 tons/ha\nPrediction: 140 tons/ha\nError: 10 tons/ha\nSquared Error: 100\n```\n\n#### Optimizers\n\n**Algorithms that adjust weights to minimize loss**\n\n**The process:**\n\n1. Calculate loss on current batch of data\n2. Compute gradients (via backpropagation): how should each weight change?\n3. Update weights in direction that reduces loss\n4. Repeat thousands/millions of times\n\n**Common optimizers:**\n\n| Optimizer | Description | When to Use |\n|-----------|-------------|-------------|\n| **SGD** | Stochastic Gradient Descent | Simple, well-understood |\n| **Adam** | Adaptive learning rate | Default choice, usually works well |\n| **RMSprop** | Root Mean Square Propagation | Good for RNNs |\n| **AdaGrad** | Adaptive Gradient | When features vary in frequency |\n\n**Adam is most popular** because:\n- Adapts learning rate per parameter\n- Combines benefits of momentum and adaptive learning\n- Requires minimal tuning\n- Works well across diverse problems\n\n::: {.callout-tip}\n## Training Terminology\n\n**Epoch:** One complete pass through the entire training dataset\n\n**Batch:** Subset of training data processed together before updating weights\n\n**Iteration:** One weight update (one batch processed)\n\n**Example:**\n- Training data: 10,000 samples\n- Batch size: 100\n- 1 epoch = 100 iterations (10,000 / 100)\n- Training for 50 epochs = 5,000 iterations\n:::\n\n#### The Training Process\n\n**Iterative improvement:**\n\n```\n1. Initialize weights randomly\n2. For each epoch:\n    For each batch:\n        a. Forward pass: Compute predictions\n        b. Calculate loss\n        c. Backward pass: Compute gradients (backpropagation)\n        d. Update weights using optimizer\n    e. Evaluate on validation set\n3. Stop when validation performance plateaus\n```\n\n**Monitoring training:**\n\n- **Training loss should decrease** - model learning patterns\n- **Validation loss should decrease** - model generalizing\n- **If validation loss increases while training loss decreases:** Overfitting!\n\n### Deep Learning for Earth Observation\n\n**Why CNNs excel at EO:**\n\nTraditional ML:\n- Manual feature engineering needed\n- Limited ability to capture spatial patterns\n- Each pixel treated somewhat independently\n\nCNNs:\n- **Automatic feature extraction** from raw pixels\n- **Spatial awareness** through convolutional filters\n- **Hierarchical learning:** edges → textures → objects → scenes\n- **Translation invariance:** Detects patterns anywhere in image\n\n**Common EO architectures:**\n\n1. **CNNs:** Image classification, object detection\n2. **U-Net:** Semantic segmentation (flood mapping, building extraction)\n3. **ResNet:** Very deep networks for complex classification\n4. **LSTMs:** Time series analysis (crop monitoring, drought prediction)\n\nWe'll explore these in depth on Days 2-4!\n\n---\n\n## Part 5: Data-Centric AI in Earth Observation\n\n### The Paradigm Shift (2025)\n\n::: {.callout-important}\n## Data > Models\n\n**Old paradigm (Model-Centric AI):**\n- Focus on developing better algorithms\n- Keep data fixed, iterate on model architecture\n- \"Our new model achieves 92% accuracy!\"\n\n**New paradigm (Data-Centric AI):**\n- Focus on improving data quality and curation\n- Keep model fixed (use proven architectures), iterate on data\n- \"Better data improved our model from 85% to 95% accuracy!\"\n:::\n\n**Why the shift?**\n\n1. **Model architectures have matured:** ResNet, U-Net, LSTM are well-established\n2. **Biggest gains now come from data:** Most underperforming models suffer from data issues\n3. **Real-world deployment:** Data quality determines operational success\n\n### Pillar 1: Data Quality\n\n**High-quality data is accurate, consistent, and properly processed**\n\n**For satellite imagery:**\n\n**Quality issues to address:**\n\n- **Cloud contamination:** Use Level-2A with SCL cloud masks\n- **Atmospheric effects:** Always use atmospherically corrected data\n- **Sensor artifacts:** Check for striping, banding, saturation\n- **Geometric accuracy:** Ensure sub-pixel registration\n- **Radiometric consistency:** Calibrate across sensors and times\n\n::: {.philippine-context}\n**Philippine Challenge: Cloud Cover**\n\nPhilippines has one of highest cloud cover frequencies globally (>60% during monsoon).\n\n**Data quality solutions:**\n- Multi-temporal compositing (median over 3 months)\n- Combine optical (Sentinel-2) + SAR (Sentinel-1) which penetrates clouds\n- Use aggressive cloud masking (accept fewer images for higher quality)\n- Leverage dry season (Dec-May) for optical data\n:::\n\n**For training labels:**\n\n**Quality issues:**\n\n- **Positional error:** GPS drift, georeferencing mismatch\n- **Temporal mismatch:** 2018 labels with 2020 imagery\n- **Class ambiguity:** Unclear definitions (shrub vs. sparse forest?)\n- **Mixed pixels:** Polygon boundaries include multiple classes\n- **Labeling inconsistency:** Different interpreters, different criteria\n\n**Best practices:**\n\n1. **Clear class definitions:** Document what each class includes/excludes\n2. **Consistent methodology:** Same interpreter, same time of year, same imagery\n3. **Quality control:** Multiple reviewers, consensus protocols\n4. **Temporal alignment:** Labels contemporary with imagery (within months)\n5. **Positional accuracy:** Use high-resolution reference imagery\n\n### Pillar 2: Data Quantity\n\n**More data (usually) improves performance**\n\n**But quantity alone isn't enough - quality matters more!**\n\n**How much data do you need?**\n\n| Algorithm | Typical Requirements |\n|-----------|---------------------|\n| Random Forest | 100s - 1000s samples per class |\n| Simple CNN | 1000s - 10,000s samples |\n| Deep CNN (ResNet) | 10,000s - 100,000s samples |\n| Foundation Models | Millions - billions samples |\n\n**Strategies when labeled data is limited:**\n\n1. **Data Augmentation**\n   - Rotation, flipping, cropping\n   - Color jittering (adjust brightness, contrast)\n   - Adding noise\n   - **Caution:** Ensure augmentations are realistic for EO\n\n2. **Transfer Learning**\n   - Use model pre-trained on large dataset (ImageNet, SatMAE)\n   - Fine-tune on your small dataset\n   - Leverages learned features from similar tasks\n\n3. **Active Learning**\n   - Iteratively: train model → find uncertain predictions → label those → retrain\n   - Efficiently focuses labeling effort where it matters most\n\n4. **Synthetic Data**\n   - Generate training data via simulation\n   - Example: Simulated SAR scenes for flood detection\n\n::: {.callout-note}\n## 2025 Research: Data Efficiency\n\nRecent studies show:\n\n- Some EO tasks reach optimal accuracy with **<20% of temporal instances**\n- **Single band** from single sensor can be sufficient for specific tasks\n- **Implication:** Smart data selection > brute force data collection\n\n**Source:** \"Data-Centric Machine Learning for Earth Observation: Necessary and Sufficient Features\" (arXiv 2024)\n:::\n\n### Pillar 3: Data Diversity\n\n**Representative data covers the full range of scenarios the model will encounter**\n\n**Dimensions of diversity:**\n\n1. **Geographic diversity**\n   - Different regions (Luzon, Visayas, Mindanao)\n   - Different ecosystems (lowland, highland, coastal)\n   - Different climate zones\n\n2. **Temporal diversity**\n   - Different seasons (wet, dry)\n   - Different years (inter-annual variability)\n   - Different phenological stages (planting, growing, harvest)\n\n3. **Class diversity**\n   - Multiple examples per class\n   - Edge cases and rare types\n   - Transitional zones\n\n4. **Sensor diversity**\n   - Different satellites (Sentinel-2A, 2B, 2C)\n   - Different atmospheric conditions\n   - Different viewing angles\n\n**Example: Urban classification**\n\n**Poor diversity:** All training samples from Metro Manila CBD\n\n**Result:** Model fails on:\n- Small provincial towns (different building density)\n- Informal settlements (different materials)\n- Peri-urban areas (mixed land cover)\n\n**Good diversity:** Samples from:\n- Large cities (Manila, Cebu, Davao)\n- Medium towns (Baguio, Iloilo, Cagayan de Oro)\n- Small municipalities\n- Different building materials (concrete, metal roofing, nipa huts)\n- Different periods (to capture growth)\n\n**Result:** Model generalizes well across Philippines\n\n### Pillar 4: Annotation Strategy\n\n**How you label data profoundly impacts model performance**\n\n**Annotation approaches:**\n\n1. **Point sampling:** Fast, but limited context\n2. **Polygon delineation:** More information, more time-consuming\n3. **Pixel-level labeling:** Maximum detail, required for segmentation\n4. **Image-level labels:** Easiest, suitable for scene classification\n\n**Best practices:**\n\n**1. Expert involvement**\n- Use domain experts for complex classes (forest types, crop stages)\n- Train labelers thoroughly on class definitions\n- Regular calibration sessions\n\n**2. Quality over quantity**\n- 500 high-quality labels > 5000 noisy labels\n- Invest in review and correction\n- Document difficult cases\n\n**3. Class balance**\n- Ensure adequate representation of minority classes\n- Stratified sampling by class\n- Consider class weights in training if imbalanced\n\n**4. Consensus protocols**\n- Multiple labelers per sample\n- Majority vote or adjudication for disagreements\n- Measure inter-annotator agreement\n\n**5. Iterative refinement**\n- Use model predictions to find label errors\n- Retrain after improving labels\n- Focus effort on low-confidence predictions\n\n::: {.philippine-context}\n**Philippine Solution: ALaM Project**\n\nDOST-ASTI's **Automated Labeling Machine (ALaM)** addresses annotation bottleneck:\n\n- Combines automated labeling with crowdsourcing\n- Human-in-the-loop quality control\n- Integration with DIMER model repository\n- Reduces labeling time and cost significantly\n:::\n\n### 2025 Examples: Data-Centric Success Stories\n\n#### NASA-IBM Geospatial Foundation Model\n\n**Open-source model trained on massive HLS dataset (Harmonized Landsat-Sentinel-2)**\n\n**Data-centric approach:**\n- Millions of satellite images\n- Self-supervised pre-training (no labels needed)\n- Fine-tuned for specific tasks with small labeled datasets\n\n**Result:**\n- State-of-the-art performance on multiple EO tasks\n- Reduces labeled data requirements by 10-100x\n- Democratizes access to powerful EO AI\n\n#### ESA Φsat-2 On-Board AI\n\n**Launched 2025: 22cm CubeSat with on-board AI processing**\n\n**Data-centric innovation:**\n- Processes imagery directly on satellite\n- Only transmits actionable information (not raw data)\n- Reduces bandwidth requirements\n- Enables real-time event detection (fires, ships, clouds)\n\n**Implication:** Data quality selection happens in space!\n\n#### EarthDaily Constellation\n\n**10-satellite constellation for daily global coverage**\n\n**Focus on AI-ready data:**\n- Scientific-grade calibration\n- Consistent, reliable acquisitions\n- Optimized spectral bands for ML\n- Emphasis on data quality for algorithm performance\n\n---\n\n## Key Takeaways\n\n::: {.callout-important}\n## Session 2 Summary\n\n1. **AI/ML learns patterns from data** rather than explicit programming\n2. **The EO workflow** spans problem definition → data → preprocessing → features → training → validation → deployment\n3. **Supervised learning** (classification & regression) is dominant for EO because we need specific outputs\n4. **Unsupervised learning** (clustering) is useful for exploration but requires interpretation\n5. **Neural networks** are composed of layers of neurons using activation functions, optimized via loss minimization\n6. **Deep learning** automatically extracts hierarchical features - dominant for image analysis\n7. **Data-centric AI (2025 paradigm):** Improving data quality, quantity, diversity, and annotation beats tweaking models\n8. **For Philippine EO:** Leverage DOST-ASTI tools (DIMER, AIPI, ALaM) to operationalize data-centric approaches\n\n**Next steps:** Hands-on Python for geospatial data (Session 3) and Google Earth Engine (Session 4) to put these concepts into practice!\n:::\n\n---\n\n## Discussion Questions\n\n::: {.callout-tip}\n## Reflect & Discuss\n\n1. **What EO problem in your work** could benefit from ML? Is it classification or regression?\n\n2. **What data quality issues** have you encountered with Philippine satellite data?\n\n3. **How would you ensure diversity** in training data for a national-scale land cover map?\n\n4. **Which Philippine datasets** (PhilSA, NAMRIA, PAGASA) could complement satellite imagery for your ML project?\n\n5. **How might DOST-ASTI's DIMER and AIPI platforms** reduce barriers to deploying ML in your organization?\n:::\n\n---\n\n## Further Reading\n\n### Foundational Concepts\n- [NASA ARSET: Fundamentals of Machine Learning for Earth Science](https://appliedsciences.nasa.gov/get-involved/training/english/arset-fundamentals-machine-learning-earth-science)\n- [Data-Centric AI: Better, Not Just More](https://arxiv.org/abs/2312.05327)\n\n### Neural Networks\n- [Deep Learning Book (Goodfellow et al.)](https://www.deeplearningbook.org/) - Free online\n- [Neural Networks and Deep Learning (Nielsen)](http://neuralnetworksanddeeplearning.com/) - Interactive tutorial\n\n### EO-Specific ML\n- [EO College: Introduction to Machine Learning for Earth Observation](https://eo-college.org/courses/introduction-to-machine-learning-for-earth-observation/)\n- [ML4Earth Resources](https://ml4earth.de/)\n- [Climate Change AI: Earth Observation & Monitoring](https://www.climatechange.ai/subject_areas/earth_observation_monitoring)\n\n### Philippine AI Initiatives\n- [DOST-ASTI SkAI-Pinas](https://asti.dost.gov.ph/)\n- [DIMER Model Hub](https://asti.dost.gov.ph/news-articles/asti-leads-ph-ai-revo-with-dimer-model-hub/)\n\n---\n\n::: {.session-nav}\n::: {.session-nav-link href=\"session1.qmd\"}\n::: {.session-nav-label}\n← Previous\n:::\n::: {.session-nav-title}\nSession 1: Copernicus & Philippine EO\n:::\n:::\n::: {.session-nav-link href=\"session3.qmd\"}\n::: {.session-nav-label}\nNext Session\n:::\n::: {.session-nav-title}\nSession 3: Python for Geospatial Data →\n:::\n:::\n:::\n","srcMarkdownNoYaml":"\n\n::: {.session-info}\n**Duration:** 2 hours | **Format:** Lecture + Conceptual Exercises | **Platform:** Presentation\n:::\n\n## Session Overview\n\nThis session provides a comprehensive introduction to Artificial Intelligence and Machine Learning concepts specifically tailored for Earth Observation applications. You'll learn the complete AI/ML workflow, understand different learning paradigms, explore neural network fundamentals, and discover why data quality matters more than model complexity in 2025's data-centric AI paradigm.\n\n::: {.learning-objectives}\n### Learning Objectives\n\nBy the end of this session, you will be able to:\n\n- **Define** AI and ML in the context of Earth Observation\n- **Describe** the complete AI/ML workflow from problem definition to deployment\n- **Distinguish** between supervised and unsupervised learning with EO examples\n- **Explain** classification vs. regression tasks in satellite data analysis\n- **Understand** neural network architecture fundamentals\n- **Identify** key components: neurons, layers, activation functions, loss functions, optimizers\n- **Articulate** the data-centric AI paradigm and its importance for EO\n- **Apply** best practices for data quality, quantity, diversity, and annotation\n:::\n\n---\n\n## Part 1: What is AI/ML?\n\n### Defining the Terms\n\n**Artificial Intelligence (AI):**\n\n- Broad field focused on creating intelligent machines\n- Systems that can perceive, reason, learn, and act\n- Includes everything from rule-based systems to machine learning\n\n**Machine Learning (ML):**\n\n- Subset of AI focused on learning from data\n- Algorithms that improve performance through experience\n- **Key distinction:** No explicit programming of rules\n\n::: {.callout-note}\n## The ML Difference\n\n**Traditional Programming:**\n\n```\nRules + Data → Output\n```\n\n**Machine Learning:**\n\n```\nData + Desired Output → Rules (Model)\n```\n\nIn EO: Instead of coding \"if NIR > 0.6 and Red < 0.3, then forest\", ML learns the pattern from labeled examples.\n:::\n\n### Why ML for Earth Observation?\n\n**Challenges that ML addresses:**\n\n1. **Scale:** Petabytes of satellite data - impossible to manually analyze\n2. **Complexity:** Multispectral, temporal, spatial patterns humans can't easily detect\n3. **Consistency:** Automated processing ensures reproducible results\n4. **Speed:** Real-time disaster mapping requires immediate analysis\n\n**Traditional vs. ML approaches:**\n\n| Task | Traditional | ML Approach |\n|------|-------------|-------------|\n| **Water detection** | Manual NDWI threshold | Learn optimal threshold + texture from examples |\n| **Land cover** | Rule-based classification | Random Forest or CNN with training samples |\n| **Flood mapping** | Expert visual interpretation | U-Net segmentation trained on labeled floods |\n| **Crop monitoring** | Fixed vegetation index thresholds | LSTM time series model learning phenology |\n\n---\n\n## Part 2: The AI/ML Workflow for Earth Observation\n\nUnderstanding the complete workflow is essential for successful EO projects. Each step matters.\n\n### Step 1: Problem Definition\n\n**Define clearly what you want to achieve:**\n\n- What question are you answering? (e.g., \"Where are mangroves declining?\")\n- What output do you need? (map, time series, alert system?)\n- What accuracy is acceptable?\n- What constraints exist? (time, computational resources, data availability)\n\n::: {.philippine-context}\n**Philippine Example:**\n\n**Problem:** Map rice paddies in Central Luzon to estimate harvest timing for food security\n\n**Clear definition:**\n- Binary classification: rice vs. non-rice\n- 20m spatial resolution acceptable (Sentinel-2 bands)\n- Temporal: wet and dry season separately\n- Accuracy target: >85% for operational use\n:::\n\n### Step 2: Data Acquisition\n\n**Gather all necessary data:**\n\n- **Satellite imagery:** Sentinel-1/2, Landsat, commercial VHR\n- **Ground truth:** Field surveys, high-res imagery interpretation, existing maps\n- **Ancillary data:** DEM, climate, administrative boundaries\n\n**Data sources for Philippines:**\n\n- Copernicus Data Space Ecosystem (Sentinel-1/2)\n- PhilSA SIYASAT (NovaSAR-1)\n- NAMRIA Geoportal (land cover basemaps)\n- PAGASA (climate data)\n\n### Step 3: Data Pre-processing\n\n**Critical step - \"Garbage in, garbage out\"**\n\n**For satellite imagery:**\n\n- **Atmospheric correction:** Convert to surface reflectance (use Level-2A)\n- **Cloud masking:** Remove or mask cloudy pixels\n- **Geometric correction:** Ensure proper alignment\n- **Radiometric calibration:** Consistent values across scenes\n- **Temporal compositing:** Reduce clouds via median/mean composites\n\n**For training labels:**\n\n- **Quality control:** Verify label accuracy\n- **Coordinate alignment:** Ensure labels match imagery timing and location\n- **Class balancing:** Ensure adequate samples per class\n- **Format standardization:** Convert to ML-ready format\n\n::: {.callout-warning}\n## Pre-processing Pitfalls\n\n**Common errors that degrade model performance:**\n\n- Using Top-of-Atmosphere instead of surface reflectance\n- Temporal mismatch: 2020 imagery with 2018 labels\n- Incomplete cloud masking leaving cloud shadows\n- Mixed pixels at boundaries (especially for validation)\n- Inconsistent band ordering across scenes\n:::\n\n### Step 4: Feature Engineering\n\n**Deriving informative variables from raw data**\n\n**For traditional ML (Random Forest, SVM):**\n\n- **Spectral indices:** NDVI, NDWI, NDBI, EVI, SAVI\n- **Textural features:** GLCM metrics (contrast, entropy)\n- **Temporal features:** Mean, std dev, phenology metrics\n- **Topographic features:** Elevation, slope, aspect (from DEM)\n- **Contextual features:** Distance to roads, water bodies\n\n**Example: Forest classification features**\n\n```python\n# Spectral indices\nNDVI = (NIR - Red) / (NIR + Red)\nNDWI = (Green - NIR) / (Green + NIR)\n\n# Texture (from GLCM)\nContrast = ...  # measure of local variation\nHomogeneity = ...  # measure of uniformity\n\n# Topographic\nElevation, Slope\n\n# Result: Input feature vector per pixel\nX = [Red, Green, Blue, NIR, SWIR1, SWIR2, NDVI, NDWI, Contrast, Elevation, Slope]\n```\n\n**For deep learning (CNNs):**\n\n- Less manual feature engineering needed\n- Networks automatically learn features from raw pixels\n- Still benefit from good input data (cloud-free, calibrated)\n\n### Step 5: Model Selection and Training\n\n**Choose appropriate algorithm:**\n\n**Consider:**\n\n- Task type (classification, regression, segmentation)\n- Data size (deep learning needs more data)\n- Interpretability requirements\n- Computational resources\n- Deployment constraints\n\n**Common EO algorithms:**\n\n| Algorithm | Type | Best For | Data Needs |\n|-----------|------|----------|------------|\n| **Random Forest** | Ensemble | Classification, feature importance | Medium |\n| **SVM** | Kernel | Binary classification, small data | Small-Medium |\n| **CNN** | Deep Learning | Image classification, automatic features | Large |\n| **U-Net** | Deep Learning | Semantic segmentation (pixel-wise) | Large |\n| **LSTM** | Deep Learning | Time series prediction | Large |\n\n**Training process:**\n\n1. Split data: training (70%), validation (15%), testing (15%)\n2. Feed training data to algorithm\n3. Algorithm adjusts parameters to minimize error\n4. Monitor performance on validation set\n5. Iterate: adjust hyperparameters if needed\n\n### Step 6: Validation and Evaluation\n\n**Rigorous testing on independent data**\n\n::: {.callout-important}\n## Never Test on Training Data!\n\nTesting on data the model has seen gives falsely optimistic results. Always use held-out test data.\n:::\n\n**Classification metrics:**\n\n- **Overall Accuracy:** Percentage of correctly classified pixels\n- **Confusion Matrix:** Shows which classes are confused\n- **Producer's Accuracy:** How many ground truth samples were correctly classified\n- **User's Accuracy:** How many predicted samples are actually correct\n- **Kappa Coefficient:** Agreement accounting for chance\n- **F1-Score:** Harmonic mean of precision and recall\n\n**Regression metrics:**\n\n- **RMSE (Root Mean Squared Error):** Average prediction error\n- **MAE (Mean Absolute Error):** Average absolute deviation\n- **R² (Coefficient of Determination):** Proportion of variance explained\n\n**Philippine Example: Flood mapping evaluation**\n\n```\nConfusion Matrix:\n                Predicted\n              | Flood | No Flood |\nActual Flood  |  450  |   50     |  Producer's Acc: 90%\nActual No Flood|  30   |  1470    |  Producer's Acc: 98%\n\nUser's Accuracy: 93.8%   96.7%\nOverall Accuracy: 96%\n```\n\n### Step 7: Deployment and Operationalization\n\n**Making the model operational:**\n\n**Deployment strategies:**\n\n1. **Batch processing:** Apply model to large archives\n2. **Near real-time:** Process new satellite acquisitions automatically\n3. **On-demand:** User-triggered analysis\n4. **Edge processing:** On-board satellite AI (ESA Φsat-2)\n\n**Operational considerations:**\n\n- **Scalability:** Can it handle regional/national scale?\n- **Automation:** Minimize manual intervention\n- **Monitoring:** Track performance over time\n- **Retraining:** Update model as conditions change\n- **Integration:** Connect to decision support systems\n\n**Philippine context:**\n\n- DOST-ASTI AIPI platform for model deployment\n- DIMER repository for model sharing\n- Integration with LGU disaster response protocols\n- Delivery via PhilSA Digital Space Campus\n\n---\n\n## Part 3: Types of Machine Learning\n\n### Supervised Learning\n\n**Learning from labeled data**\n\nThe algorithm is given:\n- **Input:** Satellite image or features\n- **Output:** Known label (class or value)\n- **Goal:** Learn mapping from input to output\n\n#### Classification Tasks\n\n**Predicting categorical labels**\n\n**EO Examples:**\n\n1. **Land Cover Classification**\n   - Input: Sentinel-2 pixel values\n   - Output: Forest, Water, Urban, Agriculture, Bare soil\n   - Algorithm: Random Forest, CNN\n\n2. **Cloud Detection**\n   - Input: Multi-band imagery\n   - Output: Cloud vs. Clear\n   - Algorithm: Threshold or ML classifier\n\n3. **Crop Type Mapping**\n   - Input: Multi-temporal NDVI\n   - Output: Rice, Corn, Sugarcane, Coconut\n   - Algorithm: Random Forest or LSTM\n\n::: {.philippine-context}\n**Philippine Case Study: Mangrove Mapping**\n\n**Task:** Classify pixels as mangrove or non-mangrove in coastal areas\n\n**Data:**\n- Sentinel-2 multi-temporal imagery (dry and wet season)\n- Field-validated mangrove polygons\n- NAMRIA coastal land cover baseline\n\n**Approach:**\n- Extract spectral values and indices (NDVI, NDWI)\n- Train Random Forest classifier\n- Validate against independent field data\n- Deploy via DOST-ASTI AIPI\n\n**Result:** 92% accuracy mangrove extent map for Palawan coastline\n:::\n\n#### Regression Tasks\n\n**Predicting continuous values**\n\n**EO Examples:**\n\n1. **Biomass Estimation**\n   - Input: Sentinel-1 SAR backscatter, Sentinel-2 vegetation indices\n   - Output: Forest biomass (tons per hectare)\n   - Algorithm: Random Forest Regression\n\n2. **Soil Moisture Prediction**\n   - Input: Sentinel-1 VV/VH polarization, temperature\n   - Output: Volumetric soil moisture (%)\n   - Algorithm: Neural network regression\n\n3. **Crop Yield Forecasting**\n   - Input: NDVI time series, rainfall, temperature\n   - Output: Expected yield (tons per hectare)\n   - Algorithm: LSTM regression\n\n**Key difference from classification:**\n- Output is a number on a continuous scale\n- Loss functions measure distance from true value (MSE, RMSE)\n- Evaluation uses regression metrics (R², RMSE)\n\n### Unsupervised Learning\n\n**Finding patterns in unlabeled data**\n\nThe algorithm receives:\n- **Input:** Satellite imagery or features\n- **No labels provided**\n- **Goal:** Discover inherent structure or groupings\n\n#### Clustering\n\n**Grouping similar pixels/regions together**\n\n**Common algorithm: k-means**\n\n1. Specify number of clusters (k)\n2. Algorithm iteratively groups pixels with similar spectral characteristics\n3. Result: Image segmented into k clusters\n4. **Human interpretation needed:** \"Cluster 1 looks like water, Cluster 2 like forest...\"\n\n**EO Applications:**\n\n- **Exploratory analysis:** \"How many distinct spectral classes in this region?\"\n- **Change detection:** Cluster before/after images to find anomalies\n- **Image segmentation:** Group similar pixels for object-based analysis\n\n::: {.callout-tip}\n## When to Use Unsupervised Learning\n\n**Advantages:**\n- No need for expensive labeled data\n- Can discover unexpected patterns\n- Good for initial data exploration\n\n**Limitations:**\n- Results need interpretation\n- No guarantee clusters match desired classes\n- Often less accurate than supervised methods for specific tasks\n- Difficult to evaluate objectively\n:::\n\n**Comparison Example:**\n\n**Supervised (Land Cover Classification):**\n- Provide 1000 labeled samples: forest, water, urban\n- Train Random Forest\n- Result: Every pixel assigned forest/water/urban\n- Evaluation: 90% accuracy against test labels\n\n**Unsupervised (k-means Clustering):**\n- No labels provided\n- Run k-means with k=3\n- Result: Three clusters emerge\n- Interpretation: Cluster A=water, B=vegetation, C=mixed urban/bare\n- Evaluation: Subjective or requires labels anyway\n\n---\n\n## Part 4: Introduction to Deep Learning\n\n### What is Deep Learning?\n\n**Deep Learning = Neural Networks with Many Layers**\n\n- Subset of machine learning\n- Inspired by biological neurons\n- Multiple processing layers extract progressively abstract features\n- Dominant approach for image analysis since ~2012\n\n**Why \"deep\"?**\n- Refers to depth: many hidden layers\n- Modern networks: 10s to 100s of layers\n- Enables learning complex, hierarchical representations\n\n### Neural Network Fundamentals\n\n#### The Artificial Neuron\n\n**Building block of neural networks:**\n\n```\nInputs (x1, x2, x3) → [Weighted Sum + Bias] → Activation Function → Output\n```\n\n**Mathematical operation:**\n\n1. **Weighted sum:** `z = w1*x1 + w2*x2 + w3*x3 + b`\n2. **Activation function:** `output = activation(z)`\n\n**Example: Detecting bright pixels**\n\n```\nInputs: [Red=0.8, Green=0.7, NIR=0.9]\nWeights: [w1=1.0, w2=1.0, w3=1.0]\nBias: b = -2.0\n\nz = 1.0*0.8 + 1.0*0.7 + 1.0*0.9 - 2.0 = 0.4\noutput = ReLU(0.4) = 0.4  (indicates moderately bright)\n```\n\n#### Network Architecture\n\n**Layers of neurons:**\n\n1. **Input Layer:** Receives raw data (e.g., pixel values)\n2. **Hidden Layers:** Process and transform data\n3. **Output Layer:** Produces final prediction\n\n**For a simple image classification:**\n\n```\nInput Layer (256 neurons = 16x16 image)\n   ↓\nHidden Layer 1 (128 neurons with ReLU)\n   ↓\nHidden Layer 2 (64 neurons with ReLU)\n   ↓\nOutput Layer (5 neurons = 5 classes, softmax activation)\n```\n\nEach connection has a **weight** - the network learns optimal weights through training.\n\n#### Activation Functions\n\n**Introduce non-linearity - crucial for learning complex patterns**\n\n**Common activation functions:**\n\n| Function | Equation | Use Case |\n|----------|----------|----------|\n| **ReLU** | `max(0, x)` | Hidden layers (most common) |\n| **Sigmoid** | `1 / (1 + e^-x)` | Binary classification output |\n| **Softmax** | `e^xi / Σe^xj` | Multi-class classification output |\n| **Tanh** | `(e^x - e^-x) / (e^x + e^-x)` | Hidden layers (older) |\n\n**Why activation functions matter:**\n\nWithout non-linearity, multiple layers would collapse to a single linear transformation - no benefit from depth!\n\n::: {.callout-note}\n## ReLU: The Default Choice\n\n**ReLU (Rectified Linear Unit)** has become standard for hidden layers because:\n\n- Simple: `f(x) = max(0, x)`\n- Computationally efficient\n- Avoids vanishing gradient problem\n- Empirically performs very well\n:::\n\n#### Loss Functions\n\n**Measure how wrong the model's predictions are**\n\nThe model's objective: **minimize the loss function**\n\n**For classification:**\n\n**Categorical Cross-Entropy:**\n\n```\nLoss = -Σ(y_true * log(y_pred))\n```\n\n- Penalizes confident wrong predictions heavily\n- Encourages high probability for correct class\n\n**Example:**\n```\nTrue class: Forest (encoded as [1, 0, 0, 0, 0])\nPrediction: [0.7, 0.1, 0.1, 0.05, 0.05]  ← Good, 70% on forest\nLoss = -1*log(0.7) = 0.36\n\nPrediction: [0.2, 0.3, 0.4, 0.05, 0.05]  ← Bad, only 20% on forest\nLoss = -1*log(0.2) = 1.61  (much higher penalty)\n```\n\n**For regression:**\n\n**Mean Squared Error (MSE):**\n\n```\nLoss = (1/n) * Σ(y_true - y_pred)²\n```\n\n**Example: Biomass prediction:**\n```\nTrue: 150 tons/ha\nPrediction: 140 tons/ha\nError: 10 tons/ha\nSquared Error: 100\n```\n\n#### Optimizers\n\n**Algorithms that adjust weights to minimize loss**\n\n**The process:**\n\n1. Calculate loss on current batch of data\n2. Compute gradients (via backpropagation): how should each weight change?\n3. Update weights in direction that reduces loss\n4. Repeat thousands/millions of times\n\n**Common optimizers:**\n\n| Optimizer | Description | When to Use |\n|-----------|-------------|-------------|\n| **SGD** | Stochastic Gradient Descent | Simple, well-understood |\n| **Adam** | Adaptive learning rate | Default choice, usually works well |\n| **RMSprop** | Root Mean Square Propagation | Good for RNNs |\n| **AdaGrad** | Adaptive Gradient | When features vary in frequency |\n\n**Adam is most popular** because:\n- Adapts learning rate per parameter\n- Combines benefits of momentum and adaptive learning\n- Requires minimal tuning\n- Works well across diverse problems\n\n::: {.callout-tip}\n## Training Terminology\n\n**Epoch:** One complete pass through the entire training dataset\n\n**Batch:** Subset of training data processed together before updating weights\n\n**Iteration:** One weight update (one batch processed)\n\n**Example:**\n- Training data: 10,000 samples\n- Batch size: 100\n- 1 epoch = 100 iterations (10,000 / 100)\n- Training for 50 epochs = 5,000 iterations\n:::\n\n#### The Training Process\n\n**Iterative improvement:**\n\n```\n1. Initialize weights randomly\n2. For each epoch:\n    For each batch:\n        a. Forward pass: Compute predictions\n        b. Calculate loss\n        c. Backward pass: Compute gradients (backpropagation)\n        d. Update weights using optimizer\n    e. Evaluate on validation set\n3. Stop when validation performance plateaus\n```\n\n**Monitoring training:**\n\n- **Training loss should decrease** - model learning patterns\n- **Validation loss should decrease** - model generalizing\n- **If validation loss increases while training loss decreases:** Overfitting!\n\n### Deep Learning for Earth Observation\n\n**Why CNNs excel at EO:**\n\nTraditional ML:\n- Manual feature engineering needed\n- Limited ability to capture spatial patterns\n- Each pixel treated somewhat independently\n\nCNNs:\n- **Automatic feature extraction** from raw pixels\n- **Spatial awareness** through convolutional filters\n- **Hierarchical learning:** edges → textures → objects → scenes\n- **Translation invariance:** Detects patterns anywhere in image\n\n**Common EO architectures:**\n\n1. **CNNs:** Image classification, object detection\n2. **U-Net:** Semantic segmentation (flood mapping, building extraction)\n3. **ResNet:** Very deep networks for complex classification\n4. **LSTMs:** Time series analysis (crop monitoring, drought prediction)\n\nWe'll explore these in depth on Days 2-4!\n\n---\n\n## Part 5: Data-Centric AI in Earth Observation\n\n### The Paradigm Shift (2025)\n\n::: {.callout-important}\n## Data > Models\n\n**Old paradigm (Model-Centric AI):**\n- Focus on developing better algorithms\n- Keep data fixed, iterate on model architecture\n- \"Our new model achieves 92% accuracy!\"\n\n**New paradigm (Data-Centric AI):**\n- Focus on improving data quality and curation\n- Keep model fixed (use proven architectures), iterate on data\n- \"Better data improved our model from 85% to 95% accuracy!\"\n:::\n\n**Why the shift?**\n\n1. **Model architectures have matured:** ResNet, U-Net, LSTM are well-established\n2. **Biggest gains now come from data:** Most underperforming models suffer from data issues\n3. **Real-world deployment:** Data quality determines operational success\n\n### Pillar 1: Data Quality\n\n**High-quality data is accurate, consistent, and properly processed**\n\n**For satellite imagery:**\n\n**Quality issues to address:**\n\n- **Cloud contamination:** Use Level-2A with SCL cloud masks\n- **Atmospheric effects:** Always use atmospherically corrected data\n- **Sensor artifacts:** Check for striping, banding, saturation\n- **Geometric accuracy:** Ensure sub-pixel registration\n- **Radiometric consistency:** Calibrate across sensors and times\n\n::: {.philippine-context}\n**Philippine Challenge: Cloud Cover**\n\nPhilippines has one of highest cloud cover frequencies globally (>60% during monsoon).\n\n**Data quality solutions:**\n- Multi-temporal compositing (median over 3 months)\n- Combine optical (Sentinel-2) + SAR (Sentinel-1) which penetrates clouds\n- Use aggressive cloud masking (accept fewer images for higher quality)\n- Leverage dry season (Dec-May) for optical data\n:::\n\n**For training labels:**\n\n**Quality issues:**\n\n- **Positional error:** GPS drift, georeferencing mismatch\n- **Temporal mismatch:** 2018 labels with 2020 imagery\n- **Class ambiguity:** Unclear definitions (shrub vs. sparse forest?)\n- **Mixed pixels:** Polygon boundaries include multiple classes\n- **Labeling inconsistency:** Different interpreters, different criteria\n\n**Best practices:**\n\n1. **Clear class definitions:** Document what each class includes/excludes\n2. **Consistent methodology:** Same interpreter, same time of year, same imagery\n3. **Quality control:** Multiple reviewers, consensus protocols\n4. **Temporal alignment:** Labels contemporary with imagery (within months)\n5. **Positional accuracy:** Use high-resolution reference imagery\n\n### Pillar 2: Data Quantity\n\n**More data (usually) improves performance**\n\n**But quantity alone isn't enough - quality matters more!**\n\n**How much data do you need?**\n\n| Algorithm | Typical Requirements |\n|-----------|---------------------|\n| Random Forest | 100s - 1000s samples per class |\n| Simple CNN | 1000s - 10,000s samples |\n| Deep CNN (ResNet) | 10,000s - 100,000s samples |\n| Foundation Models | Millions - billions samples |\n\n**Strategies when labeled data is limited:**\n\n1. **Data Augmentation**\n   - Rotation, flipping, cropping\n   - Color jittering (adjust brightness, contrast)\n   - Adding noise\n   - **Caution:** Ensure augmentations are realistic for EO\n\n2. **Transfer Learning**\n   - Use model pre-trained on large dataset (ImageNet, SatMAE)\n   - Fine-tune on your small dataset\n   - Leverages learned features from similar tasks\n\n3. **Active Learning**\n   - Iteratively: train model → find uncertain predictions → label those → retrain\n   - Efficiently focuses labeling effort where it matters most\n\n4. **Synthetic Data**\n   - Generate training data via simulation\n   - Example: Simulated SAR scenes for flood detection\n\n::: {.callout-note}\n## 2025 Research: Data Efficiency\n\nRecent studies show:\n\n- Some EO tasks reach optimal accuracy with **<20% of temporal instances**\n- **Single band** from single sensor can be sufficient for specific tasks\n- **Implication:** Smart data selection > brute force data collection\n\n**Source:** \"Data-Centric Machine Learning for Earth Observation: Necessary and Sufficient Features\" (arXiv 2024)\n:::\n\n### Pillar 3: Data Diversity\n\n**Representative data covers the full range of scenarios the model will encounter**\n\n**Dimensions of diversity:**\n\n1. **Geographic diversity**\n   - Different regions (Luzon, Visayas, Mindanao)\n   - Different ecosystems (lowland, highland, coastal)\n   - Different climate zones\n\n2. **Temporal diversity**\n   - Different seasons (wet, dry)\n   - Different years (inter-annual variability)\n   - Different phenological stages (planting, growing, harvest)\n\n3. **Class diversity**\n   - Multiple examples per class\n   - Edge cases and rare types\n   - Transitional zones\n\n4. **Sensor diversity**\n   - Different satellites (Sentinel-2A, 2B, 2C)\n   - Different atmospheric conditions\n   - Different viewing angles\n\n**Example: Urban classification**\n\n**Poor diversity:** All training samples from Metro Manila CBD\n\n**Result:** Model fails on:\n- Small provincial towns (different building density)\n- Informal settlements (different materials)\n- Peri-urban areas (mixed land cover)\n\n**Good diversity:** Samples from:\n- Large cities (Manila, Cebu, Davao)\n- Medium towns (Baguio, Iloilo, Cagayan de Oro)\n- Small municipalities\n- Different building materials (concrete, metal roofing, nipa huts)\n- Different periods (to capture growth)\n\n**Result:** Model generalizes well across Philippines\n\n### Pillar 4: Annotation Strategy\n\n**How you label data profoundly impacts model performance**\n\n**Annotation approaches:**\n\n1. **Point sampling:** Fast, but limited context\n2. **Polygon delineation:** More information, more time-consuming\n3. **Pixel-level labeling:** Maximum detail, required for segmentation\n4. **Image-level labels:** Easiest, suitable for scene classification\n\n**Best practices:**\n\n**1. Expert involvement**\n- Use domain experts for complex classes (forest types, crop stages)\n- Train labelers thoroughly on class definitions\n- Regular calibration sessions\n\n**2. Quality over quantity**\n- 500 high-quality labels > 5000 noisy labels\n- Invest in review and correction\n- Document difficult cases\n\n**3. Class balance**\n- Ensure adequate representation of minority classes\n- Stratified sampling by class\n- Consider class weights in training if imbalanced\n\n**4. Consensus protocols**\n- Multiple labelers per sample\n- Majority vote or adjudication for disagreements\n- Measure inter-annotator agreement\n\n**5. Iterative refinement**\n- Use model predictions to find label errors\n- Retrain after improving labels\n- Focus effort on low-confidence predictions\n\n::: {.philippine-context}\n**Philippine Solution: ALaM Project**\n\nDOST-ASTI's **Automated Labeling Machine (ALaM)** addresses annotation bottleneck:\n\n- Combines automated labeling with crowdsourcing\n- Human-in-the-loop quality control\n- Integration with DIMER model repository\n- Reduces labeling time and cost significantly\n:::\n\n### 2025 Examples: Data-Centric Success Stories\n\n#### NASA-IBM Geospatial Foundation Model\n\n**Open-source model trained on massive HLS dataset (Harmonized Landsat-Sentinel-2)**\n\n**Data-centric approach:**\n- Millions of satellite images\n- Self-supervised pre-training (no labels needed)\n- Fine-tuned for specific tasks with small labeled datasets\n\n**Result:**\n- State-of-the-art performance on multiple EO tasks\n- Reduces labeled data requirements by 10-100x\n- Democratizes access to powerful EO AI\n\n#### ESA Φsat-2 On-Board AI\n\n**Launched 2025: 22cm CubeSat with on-board AI processing**\n\n**Data-centric innovation:**\n- Processes imagery directly on satellite\n- Only transmits actionable information (not raw data)\n- Reduces bandwidth requirements\n- Enables real-time event detection (fires, ships, clouds)\n\n**Implication:** Data quality selection happens in space!\n\n#### EarthDaily Constellation\n\n**10-satellite constellation for daily global coverage**\n\n**Focus on AI-ready data:**\n- Scientific-grade calibration\n- Consistent, reliable acquisitions\n- Optimized spectral bands for ML\n- Emphasis on data quality for algorithm performance\n\n---\n\n## Key Takeaways\n\n::: {.callout-important}\n## Session 2 Summary\n\n1. **AI/ML learns patterns from data** rather than explicit programming\n2. **The EO workflow** spans problem definition → data → preprocessing → features → training → validation → deployment\n3. **Supervised learning** (classification & regression) is dominant for EO because we need specific outputs\n4. **Unsupervised learning** (clustering) is useful for exploration but requires interpretation\n5. **Neural networks** are composed of layers of neurons using activation functions, optimized via loss minimization\n6. **Deep learning** automatically extracts hierarchical features - dominant for image analysis\n7. **Data-centric AI (2025 paradigm):** Improving data quality, quantity, diversity, and annotation beats tweaking models\n8. **For Philippine EO:** Leverage DOST-ASTI tools (DIMER, AIPI, ALaM) to operationalize data-centric approaches\n\n**Next steps:** Hands-on Python for geospatial data (Session 3) and Google Earth Engine (Session 4) to put these concepts into practice!\n:::\n\n---\n\n## Discussion Questions\n\n::: {.callout-tip}\n## Reflect & Discuss\n\n1. **What EO problem in your work** could benefit from ML? Is it classification or regression?\n\n2. **What data quality issues** have you encountered with Philippine satellite data?\n\n3. **How would you ensure diversity** in training data for a national-scale land cover map?\n\n4. **Which Philippine datasets** (PhilSA, NAMRIA, PAGASA) could complement satellite imagery for your ML project?\n\n5. **How might DOST-ASTI's DIMER and AIPI platforms** reduce barriers to deploying ML in your organization?\n:::\n\n---\n\n## Further Reading\n\n### Foundational Concepts\n- [NASA ARSET: Fundamentals of Machine Learning for Earth Science](https://appliedsciences.nasa.gov/get-involved/training/english/arset-fundamentals-machine-learning-earth-science)\n- [Data-Centric AI: Better, Not Just More](https://arxiv.org/abs/2312.05327)\n\n### Neural Networks\n- [Deep Learning Book (Goodfellow et al.)](https://www.deeplearningbook.org/) - Free online\n- [Neural Networks and Deep Learning (Nielsen)](http://neuralnetworksanddeeplearning.com/) - Interactive tutorial\n\n### EO-Specific ML\n- [EO College: Introduction to Machine Learning for Earth Observation](https://eo-college.org/courses/introduction-to-machine-learning-for-earth-observation/)\n- [ML4Earth Resources](https://ml4earth.de/)\n- [Climate Change AI: Earth Observation & Monitoring](https://www.climatechange.ai/subject_areas/earth_observation_monitoring)\n\n### Philippine AI Initiatives\n- [DOST-ASTI SkAI-Pinas](https://asti.dost.gov.ph/)\n- [DIMER Model Hub](https://asti.dost.gov.ph/news-articles/asti-leads-ph-ai-revo-with-dimer-model-hub/)\n\n---\n\n::: {.session-nav}\n::: {.session-nav-link href=\"session1.qmd\"}\n::: {.session-nav-label}\n← Previous\n:::\n::: {.session-nav-title}\nSession 1: Copernicus & Philippine EO\n:::\n:::\n::: {.session-nav-link href=\"session3.qmd\"}\n::: {.session-nav-label}\nNext Session\n:::\n::: {.session-nav-title}\nSession 3: Python for Geospatial Data →\n:::\n:::\n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"wrap","code-link":true,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":true,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["include-code-files"],"css":["../styles/custom.css"],"toc":true,"toc-depth":3,"number-sections":false,"output-file":"session2.html"},"language":{"toc-title-document":"Contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Instructor","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Date","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","comments":{"hypothesis":false,"utterances":{"repo":"copphil-training/day1"}},"theme":{"light":["cosmo","../styles/custom.scss"],"dark":["darkly","../styles/custom.scss"]},"toc-expand":2,"toc-title":"On This Page","code-copy":true,"smooth-scroll":true,"anchor-sections":true,"fig-cap-location":"bottom","tbl-cap-location":"top","citations-hover":true,"footnotes-hover":true,"title":"Session 2: Core Concepts of AI/ML for Earth Observation","subtitle":"Understanding the fundamentals of machine learning for satellite data analysis","date":"last-modified"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}