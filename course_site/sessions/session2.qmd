---
title: "Session 2: Core Concepts of AI/ML for Earth Observation"
subtitle: "Understanding the fundamentals of machine learning for satellite data analysis"
date: last-modified
---

::: {.session-info}
**Duration:** 2 hours | **Format:** Lecture + Conceptual Exercises | **Platform:** Presentation
:::

## Session Overview

This session provides a comprehensive introduction to Artificial Intelligence and Machine Learning concepts specifically tailored for Earth Observation applications. You'll learn the complete AI/ML workflow, understand different learning paradigms, explore neural network fundamentals, and discover why data quality matters more than model complexity in 2025's data-centric AI paradigm.

::: {.learning-objectives}
### Learning Objectives

By the end of this session, you will be able to:

- **Define** AI and ML in the context of Earth Observation
- **Describe** the complete AI/ML workflow from problem definition to deployment
- **Distinguish** between supervised and unsupervised learning with EO examples
- **Explain** classification vs. regression tasks in satellite data analysis
- **Understand** neural network architecture fundamentals
- **Identify** key components: neurons, layers, activation functions, loss functions, optimizers
- **Articulate** the data-centric AI paradigm and its importance for EO
- **Apply** best practices for data quality, quantity, diversity, and annotation
:::

---

## Part 1: What is AI/ML?

### Defining the Terms

**Artificial Intelligence (AI):**

- Broad field focused on creating intelligent machines
- Systems that can perceive, reason, learn, and act
- Includes everything from rule-based systems to machine learning

**Machine Learning (ML):**

- Subset of AI focused on learning from data
- Algorithms that improve performance through experience
- **Key distinction:** No explicit programming of rules

::: {.callout-note}
## The ML Difference

**Traditional Programming:**

```
Rules + Data → Output
```

**Machine Learning:**

```
Data + Desired Output → Rules (Model)
```

In EO: Instead of coding "if NIR > 0.6 and Red < 0.3, then forest", ML learns the pattern from labeled examples.
:::

### Why ML for Earth Observation?

**Challenges that ML addresses:**

1. **Scale:** Petabytes of satellite data - impossible to manually analyze
2. **Complexity:** Multispectral, temporal, spatial patterns humans can't easily detect
3. **Consistency:** Automated processing ensures reproducible results
4. **Speed:** Real-time disaster mapping requires immediate analysis

**Traditional vs. ML approaches:**

| Task | Traditional | ML Approach |
|------|-------------|-------------|
| **Water detection** | Manual NDWI threshold | Learn optimal threshold + texture from examples |
| **Land cover** | Rule-based classification | Random Forest or CNN with training samples |
| **Flood mapping** | Expert visual interpretation | U-Net segmentation trained on labeled floods |
| **Crop monitoring** | Fixed vegetation index thresholds | LSTM time series model learning phenology |

---

## Part 2: The AI/ML Workflow for Earth Observation

Understanding the complete workflow is essential for successful EO projects. Each step matters.

### Step 1: Problem Definition

**Define clearly what you want to achieve:**

- What question are you answering? (e.g., "Where are mangroves declining?")
- What output do you need? (map, time series, alert system?)
- What accuracy is acceptable?
- What constraints exist? (time, computational resources, data availability)

::: {.philippine-context}
**Philippine Example:**

**Problem:** Map rice paddies in Central Luzon to estimate harvest timing for food security

**Clear definition:**
- Binary classification: rice vs. non-rice
- 20m spatial resolution acceptable (Sentinel-2 bands)
- Temporal: wet and dry season separately
- Accuracy target: >85% for operational use
:::

### Step 2: Data Acquisition

**Gather all necessary data:**

- **Satellite imagery:** Sentinel-1/2, Landsat, commercial VHR
- **Ground truth:** Field surveys, high-res imagery interpretation, existing maps
- **Ancillary data:** DEM, climate, administrative boundaries

**Data sources for Philippines:**

- Copernicus Data Space Ecosystem (Sentinel-1/2)
- PhilSA SIYASAT (NovaSAR-1)
- NAMRIA Geoportal (land cover basemaps)
- PAGASA (climate data)

### Step 3: Data Pre-processing

**Critical step - "Garbage in, garbage out"**

**For satellite imagery:**

- **Atmospheric correction:** Convert to surface reflectance (use Level-2A)
- **Cloud masking:** Remove or mask cloudy pixels
- **Geometric correction:** Ensure proper alignment
- **Radiometric calibration:** Consistent values across scenes
- **Temporal compositing:** Reduce clouds via median/mean composites

**For training labels:**

- **Quality control:** Verify label accuracy
- **Coordinate alignment:** Ensure labels match imagery timing and location
- **Class balancing:** Ensure adequate samples per class
- **Format standardization:** Convert to ML-ready format

::: {.callout-warning}
## Pre-processing Pitfalls

**Common errors that degrade model performance:**

- Using Top-of-Atmosphere instead of surface reflectance
- Temporal mismatch: 2020 imagery with 2018 labels
- Incomplete cloud masking leaving cloud shadows
- Mixed pixels at boundaries (especially for validation)
- Inconsistent band ordering across scenes
:::

### Step 4: Feature Engineering

**Deriving informative variables from raw data**

**For traditional ML (Random Forest, SVM):**

- **Spectral indices:** NDVI, NDWI, NDBI, EVI, SAVI
- **Textural features:** GLCM metrics (contrast, entropy)
- **Temporal features:** Mean, std dev, phenology metrics
- **Topographic features:** Elevation, slope, aspect (from DEM)
- **Contextual features:** Distance to roads, water bodies

**Example: Forest classification features**

```python
# Spectral indices
NDVI = (NIR - Red) / (NIR + Red)
NDWI = (Green - NIR) / (Green + NIR)

# Texture (from GLCM)
Contrast = ...  # measure of local variation
Homogeneity = ...  # measure of uniformity

# Topographic
Elevation, Slope

# Result: Input feature vector per pixel
X = [Red, Green, Blue, NIR, SWIR1, SWIR2, NDVI, NDWI, Contrast, Elevation, Slope]
```

**For deep learning (CNNs):**

- Less manual feature engineering needed
- Networks automatically learn features from raw pixels
- Still benefit from good input data (cloud-free, calibrated)

### Step 5: Model Selection and Training

**Choose appropriate algorithm:**

**Consider:**

- Task type (classification, regression, segmentation)
- Data size (deep learning needs more data)
- Interpretability requirements
- Computational resources
- Deployment constraints

**Common EO algorithms:**

| Algorithm | Type | Best For | Data Needs |
|-----------|------|----------|------------|
| **Random Forest** | Ensemble | Classification, feature importance | Medium |
| **SVM** | Kernel | Binary classification, small data | Small-Medium |
| **CNN** | Deep Learning | Image classification, automatic features | Large |
| **U-Net** | Deep Learning | Semantic segmentation (pixel-wise) | Large |
| **LSTM** | Deep Learning | Time series prediction | Large |

**Training process:**

1. Split data: training (70%), validation (15%), testing (15%)
2. Feed training data to algorithm
3. Algorithm adjusts parameters to minimize error
4. Monitor performance on validation set
5. Iterate: adjust hyperparameters if needed

### Step 6: Validation and Evaluation

**Rigorous testing on independent data**

::: {.callout-important}
## Never Test on Training Data!

Testing on data the model has seen gives falsely optimistic results. Always use held-out test data.
:::

**Classification metrics:**

- **Overall Accuracy:** Percentage of correctly classified pixels
- **Confusion Matrix:** Shows which classes are confused
- **Producer's Accuracy:** How many ground truth samples were correctly classified
- **User's Accuracy:** How many predicted samples are actually correct
- **Kappa Coefficient:** Agreement accounting for chance
- **F1-Score:** Harmonic mean of precision and recall

**Regression metrics:**

- **RMSE (Root Mean Squared Error):** Average prediction error
- **MAE (Mean Absolute Error):** Average absolute deviation
- **R² (Coefficient of Determination):** Proportion of variance explained

**Philippine Example: Flood mapping evaluation**

```
Confusion Matrix:
                Predicted
              | Flood | No Flood |
Actual Flood  |  450  |   50     |  Producer's Acc: 90%
Actual No Flood|  30   |  1470    |  Producer's Acc: 98%

User's Accuracy: 93.8%   96.7%
Overall Accuracy: 96%
```

### Step 7: Deployment and Operationalization

**Making the model operational:**

**Deployment strategies:**

1. **Batch processing:** Apply model to large archives
2. **Near real-time:** Process new satellite acquisitions automatically
3. **On-demand:** User-triggered analysis
4. **Edge processing:** On-board satellite AI (ESA Φsat-2)

**Operational considerations:**

- **Scalability:** Can it handle regional/national scale?
- **Automation:** Minimize manual intervention
- **Monitoring:** Track performance over time
- **Retraining:** Update model as conditions change
- **Integration:** Connect to decision support systems

**Philippine context:**

- DOST-ASTI AIPI platform for model deployment
- DIMER repository for model sharing
- Integration with LGU disaster response protocols
- Delivery via PhilSA Digital Space Campus

---

## Part 3: Types of Machine Learning

### Supervised Learning

**Learning from labeled data**

The algorithm is given:
- **Input:** Satellite image or features
- **Output:** Known label (class or value)
- **Goal:** Learn mapping from input to output

#### Classification Tasks

**Predicting categorical labels**

**EO Examples:**

1. **Land Cover Classification**
   - Input: Sentinel-2 pixel values
   - Output: Forest, Water, Urban, Agriculture, Bare soil
   - Algorithm: Random Forest, CNN

2. **Cloud Detection**
   - Input: Multi-band imagery
   - Output: Cloud vs. Clear
   - Algorithm: Threshold or ML classifier

3. **Crop Type Mapping**
   - Input: Multi-temporal NDVI
   - Output: Rice, Corn, Sugarcane, Coconut
   - Algorithm: Random Forest or LSTM

::: {.philippine-context}
**Philippine Case Study: Mangrove Mapping**

**Task:** Classify pixels as mangrove or non-mangrove in coastal areas

**Data:**
- Sentinel-2 multi-temporal imagery (dry and wet season)
- Field-validated mangrove polygons
- NAMRIA coastal land cover baseline

**Approach:**
- Extract spectral values and indices (NDVI, NDWI)
- Train Random Forest classifier
- Validate against independent field data
- Deploy via DOST-ASTI AIPI

**Result:** 92% accuracy mangrove extent map for Palawan coastline
:::

#### Regression Tasks

**Predicting continuous values**

**EO Examples:**

1. **Biomass Estimation**
   - Input: Sentinel-1 SAR backscatter, Sentinel-2 vegetation indices
   - Output: Forest biomass (tons per hectare)
   - Algorithm: Random Forest Regression

2. **Soil Moisture Prediction**
   - Input: Sentinel-1 VV/VH polarization, temperature
   - Output: Volumetric soil moisture (%)
   - Algorithm: Neural network regression

3. **Crop Yield Forecasting**
   - Input: NDVI time series, rainfall, temperature
   - Output: Expected yield (tons per hectare)
   - Algorithm: LSTM regression

**Key difference from classification:**
- Output is a number on a continuous scale
- Loss functions measure distance from true value (MSE, RMSE)
- Evaluation uses regression metrics (R², RMSE)

### Unsupervised Learning

**Finding patterns in unlabeled data**

The algorithm receives:
- **Input:** Satellite imagery or features
- **No labels provided**
- **Goal:** Discover inherent structure or groupings

#### Clustering

**Grouping similar pixels/regions together**

**Common algorithm: k-means**

1. Specify number of clusters (k)
2. Algorithm iteratively groups pixels with similar spectral characteristics
3. Result: Image segmented into k clusters
4. **Human interpretation needed:** "Cluster 1 looks like water, Cluster 2 like forest..."

**EO Applications:**

- **Exploratory analysis:** "How many distinct spectral classes in this region?"
- **Change detection:** Cluster before/after images to find anomalies
- **Image segmentation:** Group similar pixels for object-based analysis

::: {.callout-tip}
## When to Use Unsupervised Learning

**Advantages:**
- No need for expensive labeled data
- Can discover unexpected patterns
- Good for initial data exploration

**Limitations:**
- Results need interpretation
- No guarantee clusters match desired classes
- Often less accurate than supervised methods for specific tasks
- Difficult to evaluate objectively
:::

**Comparison Example:**

**Supervised (Land Cover Classification):**
- Provide 1000 labeled samples: forest, water, urban
- Train Random Forest
- Result: Every pixel assigned forest/water/urban
- Evaluation: 90% accuracy against test labels

**Unsupervised (k-means Clustering):**
- No labels provided
- Run k-means with k=3
- Result: Three clusters emerge
- Interpretation: Cluster A=water, B=vegetation, C=mixed urban/bare
- Evaluation: Subjective or requires labels anyway

---

## Part 4: Introduction to Deep Learning

### What is Deep Learning?

**Deep Learning = Neural Networks with Many Layers**

- Subset of machine learning
- Inspired by biological neurons
- Multiple processing layers extract progressively abstract features
- Dominant approach for image analysis since ~2012

**Why "deep"?**
- Refers to depth: many hidden layers
- Modern networks: 10s to 100s of layers
- Enables learning complex, hierarchical representations

### Neural Network Fundamentals

#### The Artificial Neuron

**Building block of neural networks:**

```
Inputs (x1, x2, x3) → [Weighted Sum + Bias] → Activation Function → Output
```

**Mathematical operation:**

1. **Weighted sum:** `z = w1*x1 + w2*x2 + w3*x3 + b`
2. **Activation function:** `output = activation(z)`

**Example: Detecting bright pixels**

```
Inputs: [Red=0.8, Green=0.7, NIR=0.9]
Weights: [w1=1.0, w2=1.0, w3=1.0]
Bias: b = -2.0

z = 1.0*0.8 + 1.0*0.7 + 1.0*0.9 - 2.0 = 0.4
output = ReLU(0.4) = 0.4  (indicates moderately bright)
```

#### Network Architecture

**Layers of neurons:**

1. **Input Layer:** Receives raw data (e.g., pixel values)
2. **Hidden Layers:** Process and transform data
3. **Output Layer:** Produces final prediction

**For a simple image classification:**

```
Input Layer (256 neurons = 16x16 image)
   ↓
Hidden Layer 1 (128 neurons with ReLU)
   ↓
Hidden Layer 2 (64 neurons with ReLU)
   ↓
Output Layer (5 neurons = 5 classes, softmax activation)
```

Each connection has a **weight** - the network learns optimal weights through training.

#### Activation Functions

**Introduce non-linearity - crucial for learning complex patterns**

**Common activation functions:**

| Function | Equation | Use Case |
|----------|----------|----------|
| **ReLU** | `max(0, x)` | Hidden layers (most common) |
| **Sigmoid** | `1 / (1 + e^-x)` | Binary classification output |
| **Softmax** | `e^xi / Σe^xj` | Multi-class classification output |
| **Tanh** | `(e^x - e^-x) / (e^x + e^-x)` | Hidden layers (older) |

**Why activation functions matter:**

Without non-linearity, multiple layers would collapse to a single linear transformation - no benefit from depth!

::: {.callout-note}
## ReLU: The Default Choice

**ReLU (Rectified Linear Unit)** has become standard for hidden layers because:

- Simple: `f(x) = max(0, x)`
- Computationally efficient
- Avoids vanishing gradient problem
- Empirically performs very well
:::

#### Loss Functions

**Measure how wrong the model's predictions are**

The model's objective: **minimize the loss function**

**For classification:**

**Categorical Cross-Entropy:**

```
Loss = -Σ(y_true * log(y_pred))
```

- Penalizes confident wrong predictions heavily
- Encourages high probability for correct class

**Example:**
```
True class: Forest (encoded as [1, 0, 0, 0, 0])
Prediction: [0.7, 0.1, 0.1, 0.05, 0.05]  ← Good, 70% on forest
Loss = -1*log(0.7) = 0.36

Prediction: [0.2, 0.3, 0.4, 0.05, 0.05]  ← Bad, only 20% on forest
Loss = -1*log(0.2) = 1.61  (much higher penalty)
```

**For regression:**

**Mean Squared Error (MSE):**

```
Loss = (1/n) * Σ(y_true - y_pred)²
```

**Example: Biomass prediction:**
```
True: 150 tons/ha
Prediction: 140 tons/ha
Error: 10 tons/ha
Squared Error: 100
```

#### Optimizers

**Algorithms that adjust weights to minimize loss**

**The process:**

1. Calculate loss on current batch of data
2. Compute gradients (via backpropagation): how should each weight change?
3. Update weights in direction that reduces loss
4. Repeat thousands/millions of times

**Common optimizers:**

| Optimizer | Description | When to Use |
|-----------|-------------|-------------|
| **SGD** | Stochastic Gradient Descent | Simple, well-understood |
| **Adam** | Adaptive learning rate | Default choice, usually works well |
| **RMSprop** | Root Mean Square Propagation | Good for RNNs |
| **AdaGrad** | Adaptive Gradient | When features vary in frequency |

**Adam is most popular** because:
- Adapts learning rate per parameter
- Combines benefits of momentum and adaptive learning
- Requires minimal tuning
- Works well across diverse problems

::: {.callout-tip}
## Training Terminology

**Epoch:** One complete pass through the entire training dataset

**Batch:** Subset of training data processed together before updating weights

**Iteration:** One weight update (one batch processed)

**Example:**
- Training data: 10,000 samples
- Batch size: 100
- 1 epoch = 100 iterations (10,000 / 100)
- Training for 50 epochs = 5,000 iterations
:::

#### The Training Process

**Iterative improvement:**

```
1. Initialize weights randomly
2. For each epoch:
    For each batch:
        a. Forward pass: Compute predictions
        b. Calculate loss
        c. Backward pass: Compute gradients (backpropagation)
        d. Update weights using optimizer
    e. Evaluate on validation set
3. Stop when validation performance plateaus
```

**Monitoring training:**

- **Training loss should decrease** - model learning patterns
- **Validation loss should decrease** - model generalizing
- **If validation loss increases while training loss decreases:** Overfitting!

### Deep Learning for Earth Observation

**Why CNNs excel at EO:**

Traditional ML:
- Manual feature engineering needed
- Limited ability to capture spatial patterns
- Each pixel treated somewhat independently

CNNs:
- **Automatic feature extraction** from raw pixels
- **Spatial awareness** through convolutional filters
- **Hierarchical learning:** edges → textures → objects → scenes
- **Translation invariance:** Detects patterns anywhere in image

**Common EO architectures:**

1. **CNNs:** Image classification, object detection
2. **U-Net:** Semantic segmentation (flood mapping, building extraction)
3. **ResNet:** Very deep networks for complex classification
4. **LSTMs:** Time series analysis (crop monitoring, drought prediction)

We'll explore these in depth on Days 2-4!

---

## Part 5: Data-Centric AI in Earth Observation

### The Paradigm Shift (2025)

::: {.callout-important}
## Data > Models

**Old paradigm (Model-Centric AI):**
- Focus on developing better algorithms
- Keep data fixed, iterate on model architecture
- "Our new model achieves 92% accuracy!"

**New paradigm (Data-Centric AI):**
- Focus on improving data quality and curation
- Keep model fixed (use proven architectures), iterate on data
- "Better data improved our model from 85% to 95% accuracy!"
:::

**Why the shift?**

1. **Model architectures have matured:** ResNet, U-Net, LSTM are well-established
2. **Biggest gains now come from data:** Most underperforming models suffer from data issues
3. **Real-world deployment:** Data quality determines operational success

### Pillar 1: Data Quality

**High-quality data is accurate, consistent, and properly processed**

**For satellite imagery:**

**Quality issues to address:**

- **Cloud contamination:** Use Level-2A with SCL cloud masks
- **Atmospheric effects:** Always use atmospherically corrected data
- **Sensor artifacts:** Check for striping, banding, saturation
- **Geometric accuracy:** Ensure sub-pixel registration
- **Radiometric consistency:** Calibrate across sensors and times

::: {.philippine-context}
**Philippine Challenge: Cloud Cover**

Philippines has one of highest cloud cover frequencies globally (>60% during monsoon).

**Data quality solutions:**
- Multi-temporal compositing (median over 3 months)
- Combine optical (Sentinel-2) + SAR (Sentinel-1) which penetrates clouds
- Use aggressive cloud masking (accept fewer images for higher quality)
- Leverage dry season (Dec-May) for optical data
:::

**For training labels:**

**Quality issues:**

- **Positional error:** GPS drift, georeferencing mismatch
- **Temporal mismatch:** 2018 labels with 2020 imagery
- **Class ambiguity:** Unclear definitions (shrub vs. sparse forest?)
- **Mixed pixels:** Polygon boundaries include multiple classes
- **Labeling inconsistency:** Different interpreters, different criteria

**Best practices:**

1. **Clear class definitions:** Document what each class includes/excludes
2. **Consistent methodology:** Same interpreter, same time of year, same imagery
3. **Quality control:** Multiple reviewers, consensus protocols
4. **Temporal alignment:** Labels contemporary with imagery (within months)
5. **Positional accuracy:** Use high-resolution reference imagery

### Pillar 2: Data Quantity

**More data (usually) improves performance**

**But quantity alone isn't enough - quality matters more!**

**How much data do you need?**

| Algorithm | Typical Requirements |
|-----------|---------------------|
| Random Forest | 100s - 1000s samples per class |
| Simple CNN | 1000s - 10,000s samples |
| Deep CNN (ResNet) | 10,000s - 100,000s samples |
| Foundation Models | Millions - billions samples |

**Strategies when labeled data is limited:**

1. **Data Augmentation**
   - Rotation, flipping, cropping
   - Color jittering (adjust brightness, contrast)
   - Adding noise
   - **Caution:** Ensure augmentations are realistic for EO

2. **Transfer Learning**
   - Use model pre-trained on large dataset (ImageNet, SatMAE)
   - Fine-tune on your small dataset
   - Leverages learned features from similar tasks

3. **Active Learning**
   - Iteratively: train model → find uncertain predictions → label those → retrain
   - Efficiently focuses labeling effort where it matters most

4. **Synthetic Data**
   - Generate training data via simulation
   - Example: Simulated SAR scenes for flood detection

::: {.callout-note}
## 2025 Research: Data Efficiency

Recent studies show:

- Some EO tasks reach optimal accuracy with **<20% of temporal instances**
- **Single band** from single sensor can be sufficient for specific tasks
- **Implication:** Smart data selection > brute force data collection

**Source:** "Data-Centric Machine Learning for Earth Observation: Necessary and Sufficient Features" (arXiv 2024)
:::

### Pillar 3: Data Diversity

**Representative data covers the full range of scenarios the model will encounter**

**Dimensions of diversity:**

1. **Geographic diversity**
   - Different regions (Luzon, Visayas, Mindanao)
   - Different ecosystems (lowland, highland, coastal)
   - Different climate zones

2. **Temporal diversity**
   - Different seasons (wet, dry)
   - Different years (inter-annual variability)
   - Different phenological stages (planting, growing, harvest)

3. **Class diversity**
   - Multiple examples per class
   - Edge cases and rare types
   - Transitional zones

4. **Sensor diversity**
   - Different satellites (Sentinel-2A, 2B, 2C)
   - Different atmospheric conditions
   - Different viewing angles

**Example: Urban classification**

**Poor diversity:** All training samples from Metro Manila CBD

**Result:** Model fails on:
- Small provincial towns (different building density)
- Informal settlements (different materials)
- Peri-urban areas (mixed land cover)

**Good diversity:** Samples from:
- Large cities (Manila, Cebu, Davao)
- Medium towns (Baguio, Iloilo, Cagayan de Oro)
- Small municipalities
- Different building materials (concrete, metal roofing, nipa huts)
- Different periods (to capture growth)

**Result:** Model generalizes well across Philippines

### Pillar 4: Annotation Strategy

**How you label data profoundly impacts model performance**

**Annotation approaches:**

1. **Point sampling:** Fast, but limited context
2. **Polygon delineation:** More information, more time-consuming
3. **Pixel-level labeling:** Maximum detail, required for segmentation
4. **Image-level labels:** Easiest, suitable for scene classification

**Best practices:**

**1. Expert involvement**
- Use domain experts for complex classes (forest types, crop stages)
- Train labelers thoroughly on class definitions
- Regular calibration sessions

**2. Quality over quantity**
- 500 high-quality labels > 5000 noisy labels
- Invest in review and correction
- Document difficult cases

**3. Class balance**
- Ensure adequate representation of minority classes
- Stratified sampling by class
- Consider class weights in training if imbalanced

**4. Consensus protocols**
- Multiple labelers per sample
- Majority vote or adjudication for disagreements
- Measure inter-annotator agreement

**5. Iterative refinement**
- Use model predictions to find label errors
- Retrain after improving labels
- Focus effort on low-confidence predictions

::: {.philippine-context}
**Philippine Solution: ALaM Project**

DOST-ASTI's **Automated Labeling Machine (ALaM)** addresses annotation bottleneck:

- Combines automated labeling with crowdsourcing
- Human-in-the-loop quality control
- Integration with DIMER model repository
- Reduces labeling time and cost significantly
:::

### 2025 Examples: Data-Centric Success Stories

#### NASA-IBM Geospatial Foundation Model

**Open-source model trained on massive HLS dataset (Harmonized Landsat-Sentinel-2)**

**Data-centric approach:**
- Millions of satellite images
- Self-supervised pre-training (no labels needed)
- Fine-tuned for specific tasks with small labeled datasets

**Result:**
- State-of-the-art performance on multiple EO tasks
- Reduces labeled data requirements by 10-100x
- Democratizes access to powerful EO AI

#### ESA Φsat-2 On-Board AI

**Launched 2025: 22cm CubeSat with on-board AI processing**

**Data-centric innovation:**
- Processes imagery directly on satellite
- Only transmits actionable information (not raw data)
- Reduces bandwidth requirements
- Enables real-time event detection (fires, ships, clouds)

**Implication:** Data quality selection happens in space!

#### EarthDaily Constellation

**10-satellite constellation for daily global coverage**

**Focus on AI-ready data:**
- Scientific-grade calibration
- Consistent, reliable acquisitions
- Optimized spectral bands for ML
- Emphasis on data quality for algorithm performance

---

## Key Takeaways

::: {.callout-important}
## Session 2 Summary

1. **AI/ML learns patterns from data** rather than explicit programming
2. **The EO workflow** spans problem definition → data → preprocessing → features → training → validation → deployment
3. **Supervised learning** (classification & regression) is dominant for EO because we need specific outputs
4. **Unsupervised learning** (clustering) is useful for exploration but requires interpretation
5. **Neural networks** are composed of layers of neurons using activation functions, optimized via loss minimization
6. **Deep learning** automatically extracts hierarchical features - dominant for image analysis
7. **Data-centric AI (2025 paradigm):** Improving data quality, quantity, diversity, and annotation beats tweaking models
8. **For Philippine EO:** Leverage DOST-ASTI tools (DIMER, AIPI, ALaM) to operationalize data-centric approaches

**Next steps:** Hands-on Python for geospatial data (Session 3) and Google Earth Engine (Session 4) to put these concepts into practice!
:::

---

## Discussion Questions

::: {.callout-tip}
## Reflect & Discuss

1. **What EO problem in your work** could benefit from ML? Is it classification or regression?

2. **What data quality issues** have you encountered with Philippine satellite data?

3. **How would you ensure diversity** in training data for a national-scale land cover map?

4. **Which Philippine datasets** (PhilSA, NAMRIA, PAGASA) could complement satellite imagery for your ML project?

5. **How might DOST-ASTI's DIMER and AIPI platforms** reduce barriers to deploying ML in your organization?
:::

---

## Further Reading

### Foundational Concepts
- [NASA ARSET: Fundamentals of Machine Learning for Earth Science](https://appliedsciences.nasa.gov/get-involved/training/english/arset-fundamentals-machine-learning-earth-science)
- [Data-Centric AI: Better, Not Just More](https://arxiv.org/abs/2312.05327)

### Neural Networks
- [Deep Learning Book (Goodfellow et al.)](https://www.deeplearningbook.org/) - Free online
- [Neural Networks and Deep Learning (Nielsen)](http://neuralnetworksanddeeplearning.com/) - Interactive tutorial

### EO-Specific ML
- [EO College: Introduction to Machine Learning for Earth Observation](https://eo-college.org/courses/introduction-to-machine-learning-for-earth-observation/)
- [ML4Earth Resources](https://ml4earth.de/)
- [Climate Change AI: Earth Observation & Monitoring](https://www.climatechange.ai/subject_areas/earth_observation_monitoring)

### Philippine AI Initiatives
- [DOST-ASTI SkAI-Pinas](https://asti.dost.gov.ph/)
- [DIMER Model Hub](https://asti.dost.gov.ph/news-articles/asti-leads-ph-ai-revo-with-dimer-model-hub/)

---

::: {.session-nav}
::: {.session-nav-link href="session1.qmd"}
::: {.session-nav-label}
← Previous
:::
::: {.session-nav-title}
Session 1: Copernicus & Philippine EO
:::
:::
::: {.session-nav-link href="session3.qmd"}
::: {.session-nav-label}
Next Session
:::
::: {.session-nav-title}
Session 3: Python for Geospatial Data →
:::
:::
:::
